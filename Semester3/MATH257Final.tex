\documentclass[fleqn]{report}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{blindtext}
\usepackage{color}
\usepackage[fontsize=16pt]{fontsize}
\usepackage{lipsum}
\usepackage{pgfplots}
\usepackage{physics}
\usepackage{mathtools}
\usepackage[makeroom]{cancel}
\usepackage{ulem}

\setlength{\columnsep}{1cm}
\addtolength{\jot}{0.1cm}
\def\columnseprulecolor{\color{blue}}
\date{Fall 2024}

\newcommand{\textoverline}[1]{$\overline{\mbox{#1}}$}

\newcommand{\col}{\textrm{col}}

\newcommand{\Nul}{\textrm{Nul}}

\newcommand{\hp}{\hspace{1cm}}

\newcommand{\del}{\partial}

\newcommand{\pdif}[2]{ \frac{\partial #1}{ \partial #2} }

\newcommand{\pderiv}[1]{ \frac{\partial}{ \partial #1} }

\newcommand{\comment}[1]{}

\newcommand{\equations} [1] {
\begin{gather*}
#1
\end{gather*}
}

\newcommand{\twovec}[2]{ 
\begin{pmatrix}
#1 \\ 
#2
\end{pmatrix}
}

\title{MATH 257}
\author{Aiden Sirotkine}

\begin{document}

\pagestyle{fancy}
\maketitle
\tableofcontents
\clearpage

\chapter{MATH 257}
My laptop died and I skipped some lectures to go to a part time 
job fair but I know every basic thing about matrices and vectors 
so I should be fine 

\chapter{Column Vectors and Basis Vectors} 
If you take the columns of a vector, then you get a couple vectors 
that span a space.

Solving a linear system is the same as finding the linear combinations 
that equal a certain result

\section{Matrix Vector Multiplication}
\[
\begin{bmatrix}
    c_1 && c_2 && c_3
\end{bmatrix}
\begin{bmatrix} a \\ b \\ c \end{bmatrix} =
a c_1 + b c_2 + c c_3
\]

\section{Transformations}
You can multiply a vector by a matrix to transform it in a certain 
way

\subsection{Rotation}
\[
\begin{bmatrix}
\cos \theta && - \sin \theta \\
\sin \theta && \cos \theta
\end{bmatrix}
\]

\section{Elementary Matrices}
An elementary matrix is a matrix gotten by doing a single elementary 
row operation on the identity matrix.

To find the inverse of an elementary matrix, you just do the 
opposite of the row operation to an identity matrix.

\section{Invertible Matrices}
Suppose $A$ and $B$ are invertible. Then:
\begin{itemize}
    \item
    $A^{-1}$ is invertible then $(A^{-1})^{-1} = A$
    \item
    $AB$ is invertible if $(AB)^{-1} = A^{-1} B^{-1}$
    \item
    $A^T$ is invertible iff $(A^T)^{-1} = (A^{-1})^{T}$
\end{itemize}


\section{LU Decomposition}
idk what it is but it's probably important

It stands for lower upper decomposition.

You can find a upper and lower triangular matrices $L$ and $U$
such that $A = LU$

You know a matrix can be decomposed if you can put the matrix
in echelon form with just row operations from a higher row to 
a lower row. 

\subsection{How To Steps}
\begin{enumerate}
    \item 
    Row reduce
    \item
    Find elementary matrices $E_1, E_2 \ldots$
    \item 
    L = $E^{-1}_1, E^{-1}_2, \ldots$
    \item 
    U = echelon form of original matrix that you already calculated
\end{enumerate}

\subsection{Solving a thingy}
to solve $Ax = b$, you can solve $Ux = c$ such that $Lc = b$.


\subsection{Inner Product}
\[
v \cdot w = v^{T} w
\]

\subsection{Norm}
\[
||v|| = \sqrt{v \cdot v}
\]

\subsection{Distance}
\[
\textrm{dist}(v, w) = ||v - w|| 
\]

\section{Orthogonality}
if twe vectors are orthogonal or perpendicular to each other, then 
\[
v \cdot w = 0
\]

\subsection{Pairwise Orthogonal}
A set of vectors is pairwise orthogonal if they are 
all orthogonal to each other. 

\subsection{Orthonormal Set}
A set of unit vectors that are all orthogonal to each other. 

\section{Subsets/ Subspaces}
A non-subset $H$ of $\mathbb{R}^n$ is a subspace of $\mathbb R^n$
if it satisfies the two following:
\begin{itemize}
    \item 
    if $u, v \in H$, then $u + v \in H$ 

    (closed under addition)
    \item 
    if $u \in H$ and $c$ is scalar, then $cu \in H$

    (closed under scalar multiplication)
\end{itemize}

subspaces are pretty useful

\subsection{Column Space}
The space created by spanning the columns of a matrix

It contains all the vectors $b$ that can be written as $Ax = b$ for some $x$

If $A$ and $B$ are row equivalent, then $\col(A) = \col(B)$

\subsection{Null Space}
The space created by all the solutions of the equation $Ax = 0$. 

The null space of an $m \times n$ matrix $A$ is a subspace in $\mathbb{R}^n$

Let $w$ and $b$ be vectors such that $Aw = b$. Then $\{ v \in \mathbb{R}^n : Av = b\} = w + \textrm{Nul}(A)$


\chapter{Coordinate Vectors}
If you start with a basis $B = \{ v_1, v_2, \ldots, v_m \}$ and want to go to a basis 
$D = \{ w_1, w_2, \ldots, w_m \}$, then you can use a linear transformation.

to get from $E_n$ which is the standard $\mathbb{R}^n$ basis to $B$ you use the 
matrix $I_{EB}$ such that $v_E = I_{EB} v_B$

if you have a linear transformation from $E$ to $E$, the way to get it from $B$ to $D$ is 

\equations{
	T v = v_T 
	\rightarrow 
	T * I_{EB} v_b = I_{ED} v_D
	\rightarrow 
	(I_{ED}^{-1} * T * I_{EB}) * v_b = v_{TD}
}

\section{Determinants}
For a $2 \times 2$ matrix its just $ad - bc$ but for larger matrices its wackier 

the determinant has a couple special properties 

\begin{itemize}
	\item
		$\det I_N = 1$
	\item
		row replacement does not change the determinant
	\item
		row interchange changes the sign of the determinant
	\item
		scalar multiplication of a row scales the determinant by the same factor 
\end{itemize}

The way to find the determinant of larger matrices is by taking the product of the diagonals of a triangular matrixes. 

Just make sure to use only row replacement to create a triangular matrix and then take the product of the diagonal entries and boom you're golden. 

if A is invertible, then $\det(A^{-1}) = \frac{1}{\det(A)}$

Also, $\det(A^T) = \det(A)$

Probably some other stuff with determinants that I missed

\chapter{Eigen-Stuff}
\[
Ax = \lambda x
\]
That's the whole thing. 

if $\det(A - \lambda I) = 0$ then $\lambda$ is an eigenvalue of $A$

\section{Diagonal Matrices}
if you have a Diagonal matrix $D$ of all the eigenvalues of a matrix and a you have a matrix $P$ of an eigenvector for each eigenvalue then 
\[
A = P D P^{-1}
\]

\subsection{Eigenbases}
an eigenbasis is a basis of $\mathbb{R}^N$ made by all the possible eigenvectors of $A$

if $A$ has an eigenbasis, then $A$ is diagonalizable. \

$A^2 = P D P^{-1} P D P^{-1} = P D^2 P^{-1}$ the you can just do the power of each 
eigenvalue in $D$ for $D^2$

\subsection{multiplicity}
If there are 2 linearly independent eigenvectors of the same eigenvalue, then the geometric multiplicity of that eigenvalue is 2. 

If the eigenvalue appears 2 times in the characteristic polynomial, then its 
algebraic multiplicity is 2.

\section{Markov Matrices}
It's an adjacency matrix except instead of 1 its a probability that 
a node will travel from 1 vertex to another. 

\section{Matrix Exponential}
\equations{
    e^{At} = I + At + \frac{(At)^2}{2!} + \frac{(At)^3}{3!}
}

This is easy for diagonalizeable matrices because those can be 
taken to multiple powers very easily. 

\chapter{Differential Equations}
Trust we're still in lin alg 

Let $A$ be a matrix with an eigenbases $v_1, v_2 \ldots v_n$. and a bunch of eigenvalues $\lambda_n$. if $v$ is in the eigenbasis in the form $v = c_1 v_1 + c_2 v_2 + \ldots$, then the unique solution to the differential equation 
$\frac{du}{dt} = Au$ with initial condition $u(0) = v$ is given by 
\[
e^{At} v = 
c_1 e^{\lambda_1 t} v_1 + 
c_2 e^{\lambda_2 t} v_2 + 
\ldots 
\]

I haven't been paying attention at all but like I think thats the 
big thing 

\chapter{Projections}
They're just kinda the projections that you did in calc 3
\[
proj_w(v) = \frac{v \cdot w}{w \cdot w} \vec w
\]
the projection of $v$ onto $w$

$v - proj_w(v)$ is called the error term 

\section{Least Squares Solution}
given that $Ax = b$ is inconsistent, the least squares solution is a
working vector $\hat{x}$ such that the distance between $A \hat x$ and $b$ equals the minimum distance between $A x$ and $b$

You did least squared solutions on the lab theyre not bad 

if you have $l$ coordinates of x, y and a function $y = c_1 x^2 + c_2 x + c_3$ or 
whatever then you make an $l \times 3$ matrix of each x value as $x^2, x, 1$ 
and then you do some transpose stuff 

\equations{
    y = A C
    \\
    A^T A x = A^T y
}

\section{Gram-Schmidt Process}
\equations{
    b_1 = a_1
    \hp 
    b_2 = a_2 - proj_{span(q_2)}(a_2)
    \\
    b_3 =  a_3 - proj_{span(q_1 * q_2)}(a_3)
}

\section{Midterm 3 Junk}
\subsection{Linear Transformations}
\begin{itemize}
\item
T(0) = 0
\item
distributive 
\item 
scalar multiplication holds
\end{itemize}

go re-find out all the bullshit with coordinate matrices 

\subsection{Determinants}
cofactor expansion 

its kinda like a cross product 

\[
C_{ij} = (-1)^{i + j} \det(A_{ij})
\]

then the determinant is just the sum of that times the cross section of 
whatever row and column you're deleting 

\subsection{Diagonalizability}
The algebraic and geometrix multiplicities of eigenvectors don't 
care about matrix multiplication 

a matrix needs $n$ linearly independent eigenvectors for it to be diagonalizable. 

\section{Least Squared}
\[
A (A^T A)^{-1} A^T
\hp 
A x = 
\]


\section{SVD Decomposition}
let $A$ be an $m \times n$ matrix with rank $r$

find the orthonormal eigenbasis of $A^T A$ with eigenvalues 
$\lambda_1, \ldots, \lambda_n$ and eigenvectors $v_1, \ldots$

Set $\sigma_i = \sqrt{\lambda_i}$ 

let $u_r = \frac{1}{\sigma_r} A v_r$

find $u_{r+1} \to u_m$ such that $u_1 \to u_m$ is an orthonormal basis. 

\[
U = 
\begin{bmatrix}
    u_1 && \ldots && u_m
\end{bmatrix}
\hp 
\Sigma = 
\begin{bmatrix}
    \sigma_1 && && \\
    && \ldots && \\
    && && \sigma_{min(m, n)} \\
\end{bmatrix}
\]
\[
V = 
\begin{bmatrix}
    v_1 && \ldots && v_n
\end{bmatrix}
\]
\[
A = U \Sigma V^t
\]


\end{document}
