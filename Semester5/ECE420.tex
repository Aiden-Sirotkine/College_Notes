\documentclass[fleqn]{report}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{blindtext}
\usepackage{color}
\usepackage[fontsize=16pt]{fontsize}
\usepackage{lipsum}
\usepackage{pgfplots}
\usepackage{physics}
\usepackage{mathtools}
\usepackage[makeroom]{cancel}
\usepackage{ulem}
\usepackage{esint}

\geometry{a4paper, margin=2cm} % Set paper size and margins
\graphicspath{ {../Images/} }
\setlength{\columnsep}{1cm}
\addtolength{\jot}{0.1cm}
\def\columnseprulecolor{\color{blue}}
\date{Fall 2025}

\newcommand{\textoverline}[1]{$\overline{\mbox{#1}}$}

\newcommand{\hp}{\hspace{1cm}}

\newcommand{\const}{\textrm{const}}

\newcommand{\del}{\partial}

\newcommand{\pdif}[2]{ \frac{\partial #1}{ \partial #2} }

\newcommand{\pderiv}[1]{ \frac{\partial}{ \partial #1} }

\newcommand{\comment}[1]{}

\newcommand{\equations} [1] {
\begin{gather*}
#1
\end{gather*}
}

\newcommand{\numequations} [1] {
\begin{gather}
#1
\end{gather}
}

\newcommand{\twovec}[2]{ 
\begin{pmatrix}
#1 \\ 
#2
\end{pmatrix}
}

\title{ECE 420}
\author{Aiden Sirotkine}

\begin{document}

\pagestyle{fancy}
\maketitle
\tableofcontents
\clearpage

\chapter{ECE 310 Overview}
\section{Sampling}
\subsection{Shannon-Nyquist Theorem: }

Proves that discrete samples can perfectly reconstruct a continuous signal. 

Reconstruction 

Up-Down Sampling

z-transform 

CTFT 

DTFT 

DFT (FFT)

Yea its literally all fourier transforms lmao

\subsection{Discrete LTI System}
Convolution 

Impulse Response 

Frequency Response 

\subsection{Filters}
Digital Filters 

FIR vs IIR 

Linear Phase 

\section{310 vs 420}
\begin{itemize}
    \item 
    How do we get the data? 

    ECE 310 = offline (batched)

    ECE 420 = online (stream)

    We use buffering techniques 

    overlapped added 

    windowing

    \item 
    Who Computes?

    ECE 310 = Computer code 

    ECE 420 = Mobile app/phone

    RUN-TIME IS IMPORTANT 

    time-domain processes and FFT'S

\end{itemize}

\section{Skillsets}
Android app dev 

C++ and Java. 

Do not need to know crazy circuit shenanigans

You do not need a fancy UI for the DSP final project. 

The DSP algorithm is more important.

\section{Signals}
IMU signals = 1d signals that are acceleration + Gyro

Audio signal is also 1d in the 20 - 20,000 Hz range 

Image signals are 2d signals with visible light and video.

\section{Android Device}
You can loan a tablet 

Need a specific OS

Needs gradle compiler.

\section{Code}
Need Python and need Android Studio

\section{Basic Practice}
\begin{enumerate}
    \item
Develop and Test DSP algorithms in hihg-level languages (Python)
    \item
Port tested algorithms into Android platform (C++, Java)
\end{enumerate}

\subsection{Attendance Quiz}
There will be a quiz and you answer the question in the quiz and that gives you 
the attendance credit.

\section{Labs}
\begin{enumerate}
    \item 
    Lab Quiz (PrairieTest)
    \item 
    Demo Lab
    \item 
    Office Hours
\end{enumerate}

Prelab (Individual)

Quiz (Individual)
Need laptop and ID 

Lab Demo (Group).
Groups are randomized every week. 

This class should be very chill I'll be so fr I'm very glad I picked this 
class. 

\chapter{Sampling}
Take amplitude at various points in time.

You then reconstruct the continuous signal using just the sampled points. 

\section{CTFT}
\equations{
    X_a(\Omega)
    =
    \int^{\infty}_{\infty}
    x_a(t) e^{j \Omega t} dt
}

Scale by Ts and make periodic every $2 \pi$ to get 

\subsection{DTFT}
\equations{
    X(\omega)
    =
    \sum^{\infty}_{\infty}
    x[n] e^{j \omega n}
}

To avoid overlap,
\equations{
    B 
    <
    \frac{\pi}{T_s}
    \hp 
    f < \frac{1}{2 T_s}
}

Where $T_s$ is the sampling period so $1/T_s$ is the sampling rate,
 and $B$ is the angular speed 

\subsection{Nyquist rate}
\equations{
    F_s 
    =
    \frac{1}{T_s}
    > 2f
}

\subsection{Audio Nyquist Rate}
The band max is 20kHz, so the Nyquist rate is 

\equations{
    \frac{1}{T_s}
    =
    2f = 40 Khz
}

So the sampling rate is 40kHz

\section{Digital Filtering}
In Lab 2, we're going to want to take out certain frequencies from 
our entire noise space.

We can try to use a continuous-time bandstop filter.
We could use an RLC circuit, but that has all sorts of consequences.

Instead, we can use a digital filter.

Digital Filters are equivalent to analog filters IF we have 
Nyquist rate sampling.

\subsection{Digital Filter}
Big silly equation that's in the lecture slides

\equations{
    y[n]
    =
    (b_0 x[n] + b_1 x[n-1] + \ldots + b_K x[n-K])
    -
    \\
    (a_0 y[n] + a_1 y[n-1] + \ldots + a_L y[n-L])
}

FIR, if no feedback (L=0)

IIR, if feedback (L $\neq$ 0)

\subsection{Large N}
\begin{itemize}
    \item 
    CLose to desired response 
    \item 
    Sharper transition 
    \item 
    Less ripples 

    BUT 
    \item 
    More computation/memory 
    \item 
    Longer Delay 
    \item 
    (for IIR) possible worse performance

\end{itemize}

\subsection{FIR and Convolution}
\equations{
    y[n]
    =
    (b_0 x[n] + b_1 x[n-1] + \ldots + b_K x[n-K])
    \\
    y[n]
    =
    \sum^K_{k = 0}
    b_k x[n-k]
}

\subsection{Batch vs Block Process}
h = filter 
\hp 
x = batch samples 
\hp 
h*x = ideal output

makes an N-size output 

We can have multiple convolution functions, and they can cause discontinuities 
if we just add them together.

\subsection{Convolution by Circular Buffer}
Challege 1: Block Processing

Audio samples come as a buffer, 

which means discontinuities between buffers 

Solution: Use a buffer (as a global variable) to store the samples 
from the previous buffer. 

something something more words from the lecture slides.

\equations{
    y[n]
    =
    \sum^K_{k=0} h[k]x[n-k] 
}

Assume $K=2$ and $h[n], y[n] = 0$

Consider a Circular Buffer 0, 1, 2

% \begin{tabular}
%     0 && 1 && 2 \\
%     x[0] && 0 && 0
% \end{tabular}
idk fix later 

\section{OpenSL ES}
Open Sound Library for Embedded Systems

Use default sampling rate (48kHz)

The library gives you weird 8 bit sampling 

Use a bitwise operature to turn the 8 bit sampled data into 
the original 16 bit data.

\chapter{Spectral Analysis}
Lab2 and Quiz 2 on digital filtering and Audio notch filtering.

Spectral analysis give you signal in the time domain. 

You can also see signal in the frequency domain. 

It shows the relative distribution of signal "energy" in a different basis 

The most common choice is the Fourier basis (frequency)

The magnitude/log, phase and other post-processing are possible. 

\section{Fourier Transforms}
Turns time domain into frequency domain or vice versa. 

\subsection{Continuous Time, Continuous Frequency}
You use a CTFT, $X_A(\Omega)$

\equations{
    X_a(\Omega)
    =
    \int^{\infty}_{-\infty}
    x_a(t)
    e^{-j \Omega t}
    dt
    \hp 
    \Omega 
    =
    2 \pi f 
}

\subsection{Discrete Time, Continuous Frequency}
You use a DTFT $X(\omega)$

\equations{
    X(\omega)
    =
    \sum^{\infty}_{-\infty}
    x[n]
    e^{-j \omega n}
    dt
    \hp 
    \Omega 
    =
    2 \pi f 
}
\subsection{Discrete Time and Discrete Frequency}
DFT X[k]

\equations{
    X[k]
    =
    \sum_{n=0}^{N-1}
    x[n]
    e^{-j2 \pi k n / N}
}

\subsection{Continuous Time and Discrete Frequency}
Fourier Series $\{ a_k \}$

\section{CTFT vs DTFT vs DFT}
\begin{itemize}
    \item 
    CTFT use an integral and everything is continuous 
    \item 
    DTFT takes in a discrete input and you use a discrete sum, but 
    you get a continuous output 
    \item 
    DFT's exist because you cant integrate or sum to infinity on a computer.
\end{itemize}

The relation between all of them is 
\equations{
    \frac{f}{F_s}
    =
    \frac{\omega}{2 \pi}
    =
    \frac{k}{N}
}

\subsection{Consequences to DFT Truncation}
In order to use DFT, the length of the input samples must be \textbf{finite}

\section{Time-Windowing}
Duration and bandwidth are inverse. 

\subsection{Rectangular Window}
\equations{
    W_R =
    \begin{cases}
        1: 
        0 < t < T 
        \\
        0:
        \textrm{ otherwise} 
    \end{cases}
}
It's just a step function.

If we have a function of just 
\equations{
    x_a(t)
    =
    A \cos(\omega_1 t)
}

Then when we take our DFT, we get 
\equations{
    X_a(j \Omega)
    =
    A \pi \delta(\Omega - \omega_1)
    +
    A \pi \delta(\Omega + \omega_1)
}

So we take our Discrete Fourier Transform to get 
\equations{
    \tilde{x_a}(t) 
    =
    w_R (t) x_a(t) 
    =
    \frac{1}{2}
    A w_R(t) e^{j \omega_1 t}
    +
    \frac{1}{2}
    A w_R(t) e^{-j \omega_1 t}
    \\
    \tilde{X_a}(\Omega) 
    =
    w_R (t) x_a(t) 
    =
    \frac{1}{2}
    A w_R(t) (\Omega - \omega_1)
    +
    \frac{1}{2}
    A w_R(t) (\Omega + \omega_1)
}


\subsection{Hamming Window}
Instead of a step function, it's more of a bump. 

There are not artifacts on the side, but the main lobe is more wide. 

\section{Zero-Padding}
When the window length $L$ is less than the DFT length $N$, you 
add $N - L$ zeros to the end of the sequence. However, it \textbf{only}
increases the resolution of the DFT, \textbf{not the DTFT}.

If you want an actually sharper frequency-domain image, you have to 
increase the length of the DTFT.

\section{STFT}
So far, we've assumed that signals are periodic and stationary. If this is 
not true, we have to change our Fourier transform parameters. 

A STFT can cut out a segment from a signal and move the window 
with a shift $m$ 

\equations{
    X(\Omega, t)
    =
    \int^{\infty}_{-\infty}
    w(t - \tau) x(t) e^{-j \Omega t} \, dt 
    \\
    X(k, m]
    =
    \sum_{n=0}^{N-1}
    w[n - m] x[n] e^{-j 2 \pi k n / N}
}

\section{Uncertainty Principle}
Time resolution and frequency resolution
cannot be improved simultaneously
in the spectrum

\section{Spectrogram}
Magnitude of STFT. 

Every timestamp, you perform an STFT to get a 2d plot 
of frequency over time. 

\subsection{Resolution vs Window Size}
Larger windows mean finer frequency resolution. Smaller windows 
mean. 

A Hamming window is a very good option for a spectrogram. 

Rectangular windows are very noisy with prominent side-lobes. They are not 
good for spectrograms. 

\chapter{Source-Filter Model}
Excitation Generator $\rightarrow$ LTI System h(t) (Linear and Time 
Invariant).

The excitation parameters are 

\begin{itemize}
    \item 
    amplitude (loudness)
    \item 
    frequency (pitch)
    \item 
    phase/delay 
    \item 
    Type (voiced, unvoiced, silenced)
\end{itemize}

The parameters of an LTI system are
\begin{itemize}
    \item 
    IMPULSE RESPONSE 
    \item 
    resonance frequency 
\end{itemize}

In the frequency space, you can just multiply the generated signal 
and the LTI system's frequency response. 

An LTI system \textbf{cannot} create new frequencies in the output.

\subsection{Speech}
Given speech, you can see multiple different syllables, but it 
has very dynamically changing frequencies and amplitude.

Speech contains an envelope of frequencies, and human speech 
is contained in a very narrow bandwidth $(< 2000Hz)$

The sampling rate for speech is usually only 4kHz or 8kHz because 
humans are not very high pitched.

\section{Characterization of Frames}
\begin{itemize}
    \item
    Voiced Sounds
    \item
    Unvoiced Sounds (P)
    \item
    Silence/noise (no active speech)
\end{itemize}

\section{Pitch Detection Algorithm}
Figure out if sound is voiced or unvoiced. 

If voiced, find the frequency. Voiced signals are touder and more sustained.
Unvoiced signals are more abrupt. 

Pitch calculation is found with autocorrelation 
\equations{
    R_{xx}[l]
    =
    \frac{
        \sum_{n=0}^{N-1}
        x[n] x^*[n-l]
    }
    {
        \sum_{n=0}^{N-1}
        |x[n]|^2
    }
}

Auto-correlation is an $N$ length vector that spans the possible
available lags.

$l$ is defined by a circular shift 
\equations{
    x^*[<n-l>_N]
}
So the lag wraps around if you're using a negative number 
(this is already done in python).

The way you figure out the frequency is with good old unit 
analysis because lag is in samples and sampling rate is samples per second.
\equations{
    fs * \frac{1}{l}
    =
    \frac{samples}{s}
    *
    \frac{1}{samples}
    =
    \frac{1}{s}
    =
    Hz
}

\section{Complexity}
big O notation. 

the autocorrelation function is $O(n^2)$ 
Because finding the autocorrelation for a single lag is $O(n)$, 
and then 

It is very similar to a circular convolution 
\equations{
    y[l]
    =
    \sum_{n=0}^{N-1}
    x[n] h[l-n]
}

Convolution involves flipping the out-of-phase portion 

The reason we use circular convolution is because of now DFT's work 

The autocorrelation can be written as 
\equations{
    X[k] X^*[k]
}

\section{Uncertainty Principle}
Time resolution and frequency resolution
cannot be improved simultaneously
in the spectrum.

\section{Time-Windowing}
Effect of time-windowing: Blurring and spreading the original spectrum.
To improve the frequency resolution, use a longer time window.

Rectangular window means higher sidelobes. Increase T and decrease 
$\Delta \Omega$.

\section{Challenges of Pitch Detection}

\section{Summary}
\begin{itemize}
    \item 
    Source-filter model for speech signal 
    \item 
    ptich detection by autocorrelation
    \item 
    autocorrelation and its boost by FFT 
\end{itemize}

\section{310 Review}
Given a discrete time system, how can we use it to filter a continuous 
time signal. 
If we sample at 10kHz and get a 1kHZ signal. We can do a DTFT to 
get a discrete frequency 
\equations{
    \frac{f}{Fs}
    =
    \frac{\omega}{2 \pi}
    =
    \frac{k}{N}
}
So given a frequency $f$ and a sampling rate $Fs$, our DTFT signal should be 
\equations{
    2 \pi \frac{f}{Fs}
    =
    \omega
    =
    0.2 \pi
}

You can do the inverse to get the frequency $\Omega$ from $\omega$

What if we want to change the pitch from 1kHz to 1.2kHz?
The way we can do that is by changing the sampling rate. 
\equations{
    \frac{f}{Fs}
    =
    \frac{f_2}{Fs_2}
    \rightarrow 
    0.1
    =
    \frac{1200}{Fs_2}
    \rightarrow 
    Fs_2 
    =
    12000kHz
}

\subsection{Upsampling}
perform zero-insertion on the signal (different from zero-padding because 
it increases the wavelength of every wave in the signal). We do not 
lose any data from upsampling. Upsampling in the frequency domain 
can be written as 
\equations{
    y[n]
    =
    \begin{cases}
        x[n/M]: n \% M = 0 
        \\
        0 : \textrm{otherwise}
    \end{cases}
    \\
    Y(\omega)
    =
    \sum^{\infty}_{-\infty}
    y[n] e^{-k \omega m}
    =
    \sum^{\infty}_{-\infty}
    x[n/M] e^{-k \omega m}
    =
    \sum^{\infty}_{-\infty}
    x[l] e^{-k \omega m l}
}
So $\omega$ is compressed by $M$
\equations{
    Y(\omega)
    =
    X(M \omega)
}
Remove the aliasing spectrum to get just the correct compressed frequency 

\subsection{Upsampling with Interpolation}
Use a low pass filter (LPF)
\equations{
    x 
    \to 
    \uparrow M 
    \to 
    LPF(\frac{\pi}{M})
}

By doing this, we can get 
\equations{
    y[n]
    =
    \sum_{k = -\infty}^{\infty}
    s[k]
    sinc(\frac{\pi (k - k M)}{M})
}
Fill the missing samples with an interpolation kernel function. 
upsampling \textbf{increases wavelength} $P_0 \to M P_0$. A consequence of this 
is that \textbf{frequency decreases}.

\section{Downsampling}
Remove features 
\equations{
    x[n]
    \to 
    \downarrow L 
    \to 
    y[n]
}
You do a long proof to get the final change in the frequency space 
\equations{
    y[n] = x[Ln]
    \\
    Y(\omega)
    =
    \frac{1}{L}
    \sum_{n=0}^{L-1}
    X\left(\frac{\omega - 2\pi}{L}\right)
}
The signal is stretched by $L$ and shifted by $2 \pi n$. To prevent 
aliasing, we want a smaller bandwidth 
\equations{
    LB < \pi 
    \to 
    B < \frac{\pi}{L}
}
Downsampling \textbf{compresses wavelength} and a consequence of that 
is that \textbf{frequency increases}.

\section{Putting the 2 together}
\equations{
    x 
    \to
    LPF(\pi/L)
    \to
    \downarrow L
    \to
    \uparrow M 
    \to
    LPF(\frac{\pi}{M})
}

You can upsample first, but nothing useful will happen. You should 
always downsample first. 

\section{Modifying Pitch $P_0$}
You can resample to change the pitch (upsampling first)
\equations{
    x 
    \to
    \uparrow M
    \to
    LPF(\frac{\pi}{max(M, L)})
    \to
    \downarrow L 
    \to
    y
    \\
    P_1 
    =
    \frac{M}{L}
    P_0
}
This allows you change pitches by only a certain fraction.
This stretches or compresses the entire spectrum. 
Both pitch and vocal tract response (color of your voice) change! 
How do we modify only pitch?

\section{TD-PSOLA}
This algorithm can modify the fundamental pitch of a signal 
\textbf{without affecting the formants} (vocal tract response). 

TD = time domain 
\hp 
PS = Pitch Synchronous (operate around reference points)

OLA = overlap add (the synthesized signal overlaps and are added 
together to form the final output)

\begin{center}
    Excitation Generator x(t) 
    $\to$ 
    Linear System h(t) 
    $\to$ 
    y(t)
    =
    x(t) * h(t)
\end{center}
You use a dirac delta convolution. 
\equations{
    x[n]
    =
    \sum \delta[n - P_0 k]
    \hp 
    \hat x[n]
    =
    \sum \delta[n - P_1 k]
    \\
    \hat y[n]
    =
    \hat x[n]
    *
     h[n]
    =
    \sum h[n - P_1 k]
}
It essentially adds empty space in between periods to change the frequency 
without changing the essence of the signal. 

\subsection{Finding Epochs}
\begin{itemize}
    \item
    Signal peaks 
    \item
    provide the reference point 
    \item
    lab4 algorithm (autocorrelation)
\end{itemize}

\subsection{Epoch Mapping}
You can find the inputs with pitch and waveform analysis. 
Output epochs are regularly spaces 

\subsection{Signal Synthesis by Windowing}
If you just use the algorithm directly, you will get huge discontinuities 
in the new signal. You need to use a window

You can't use a regular bell shaped window because the peaks are not necessarily 
in the center of the period, so the signal will be ruined. 

Instead, you use a window the size of 2 periods, so that the window peak is 
at the signal peak.

\subsection{Block Processing Challenges}
there are 2 main issues 

\begin{itemize}
    \item
    the windowed interval may stretch across multiple \textbf{input} frames
    \item
    the windowed interval may stretch across multiple \textbf{output} frames
\end{itemize}

The way to fix this is by storing the buffers in 'past', 'present' and 'future' 
for both the input and output. Let the buffer spill into both the past 
and future frames to keep everything continuous.

\chapter{Images}
Audio is a 1d signal, while pictures are a 2d signal (x, y for each signal point).
A digitized image is a bunch of pixels.
You can use linear transformations to change the locations of each pixel. 
You can also change the brightness and colors of pixels. 
You also blur and sharpen images (not really an individual pixel change).
This is known as spacial filtering (Examining/changing local regions).

\section{Intensity}
1 value for 1 pixel.
\begin{itemize}
    \item 
    Binary (0, 1)
    \item 
    Grayscale (0, 255)
    \item 
    color (3 channel RGB, etc)
\end{itemize}

\subsection{Intensity Transformation}
\equations{
    s 
    =
    T(r)
    \hp 
    s, r \in [0, L-1]
    \\
    s = r + C
}
Just increase the intensity by a constant. This is called 
\textbf{enhancing}.

\equations{
    s = L - 1 - r
}
This is known as the \textbf{negative image}. 

\equations{
    s = 
    \begin{cases}
        255 : a < s < b 
        s : \textrm{otherwise}
    \end{cases}
}
This is called \textbf{intensity-level slicing}. It essentially changes 
values within a certain interval to a constant without changing 
anything outside the interval.

\section{Histogram Processing}
Histograms represent a distribution of numerical data. Observing the 
histogram can tell you spikes of certain data. The histogram range is 
called the \textbf{contrast} or \textbf{dynamic range}. Consider how the 
previous transformations affect the histogram. For the lab, we want a 
low-contract, low-dynamic range image to turn into a high-contrast, high
dynamic range

\subsection{Histogram Equalization}
Turn your histogram into a normal distribution.
The probability distribution becomes flat and the cdf is just 
y=x.
\equations{
    s 
    =
    h(r)
    =
    round
    \left(
        \frac{cdf[r] - cdf_{min}}{MN - cdf_{min}} (L-1)
    \right)
}
$M, N$ are the width and height of the image. $L$ is the total number of gray 
levels. CDF$_{min}$ is the smallest \textbf{non-zero} value that the cdf 
can have. cdf[r] is the \textbf{total number} of pixels \textbf{at or 
below} the value $r$.

\section{Spatial Filtering}
Spatial filtering is a predefined operation that is performed on a pixel 
\textbf{and} its neighborhood. 

\subsection{2D Convolution}

a 1d convolution is 
\equations{
    g(t)
    =
    f(t) * w(t) 
    =
    \sum_k f(k) w(t-k)
}

but a 2d convolution is 
\equations{
    g(x, y)
    =
    f(x, y)
    * 
    w(x, y)
    =
    \sum_i \sum_j 
    f(i, j) w(x - i, y - j)
}

Basically, each pixel is defined by a kernel function interacting with the pixel 
and everything around it. Depending on where you start the kernel, you can 
get output sizes that are different from the input sizes.

\section{Convolution Output Domain}
\subsection{Valid}
kernel does not go outside original image. Output size $N - K + 1$ 

\subsection{Same}
Same size $N$ as input image. 

\subsection{Full}
Maximum sized convolution output size N + K - 1 

\section{Smooth Spatial Domain}
'average' the neighborhood of pixels to reduce sharp transitions. 

\subsection{Median filter}
Replace the value of the pixel with the median of the neighborhood. Effectively 
removes impulse noise and salt and pepper noise. 

\section{Sharpening Spatial Filters}
Edge detection. Highlights transitions in intensity. similar to derivative 
operator. It could also be a high pass filter in the frequency domain.

\subsection{Gradient}
Take the first derivative of the image. 
\equations{
    \nabla f 
    =
    \begin{bmatrix}
        g_x \\ g_y 
    \end{bmatrix}
    =
    \begin{bmatrix}
        \frac{\del f}{\del x} \\ 
        \frac{\del f}{\del y}
    \end{bmatrix}
}
There was a formula for the numerical gradient kernel but I didn't really 
understand it at all. 

\subsection{Laplacian}
Second derivative. Difference of first derivative 
\equations{
    L 
    =
    (f(x+1) - f(x)) - (f(x) - f(x-1))
    \\
    =
    f(x+1) + f(x-1) - 2f(x)
}
Much sharper than gradient because it uses more data and gives more information. 
However, it also amplifies noise. 

\section{Prototype}
Do python code and \textbf{make a presentation}. 
You can use a library, but you have to implement your own ideas and data. 

\chapter{3D Signal Processing}
Video data is of the form $f(x, y, t)$. Video processing algorithms 
operate on a frame by frame basis. Frame rate is in frames per second (fps), 
and our algorithm needs to be fast enough to not decrease the fps of the video. 
60fps is the usual benchmark.

\section{Tracking}
Track a Region of Interest (ROI). Follow the object as it traverses. 
The two challenges of this are the fact that the target moves, and we need 
the computation time to be very fast. 

\subsection{Function}
The main paper we use takes advantage of a correlation filter. 
We use a filter such that, given the input signal, returns a sharp signal at 
the center of our green box and nothing everywhere else. 

Given the next frame (slight change in data), the filter should still return 
a sharp dot at the square, but it will be blurry. Then, we recalculate the filter 
for the new box at frame 2. 

\equations{
    f_1 \& s_1 \Rightarrow w_1 
    \rightarrow 
    f_2 * w_1 = s_2 
    \rightarrow 
    f_2 \& s_2 = w_2 
    \rightarrow 
    \ldots
}

\subsection{Filter Itself}
We use a circular filter 
\equations{
    \underset{w}{min} 
    \sum_i 
    (w^T x_i - y_i)^2
}

\section{Ridge Regression}

\equations{
    \underset{w}{min} 
    \sum_i 
    (w^T x_i - y_i)^2
    + 
    \lambda ||W||^2
    \hp 
    w 
    =
    (X^H X + \lambda I)^{-1} X^H y
}

Matrix inversion is an $O(N^3)$ operation, so how to we reduce the time 
complexity of this algorithm? 

\subsection{Circulant Matrices}
All circulant matrices can be diagonalized with a DFT. 

\equations{
    \hat w 
    =
    \frac{\hat x \odot \hat y }{\hat x^* \odot \hat x + \lambda}
}

\subsection{Kernelized Correlation Filter}
idk it was on the slides.

\subsection{Lab 8: Digit Recognition Via Machine Learning}
You get an extra 3\% by doing this lab. THIS IS INDIVIDUAL WORK. No lab quiz. 
Due by the end of the semester, but need to demo at TA office hours. 

\chapter{Machine Learning}
We're going to make a program that input hand-written digits and outputs the 
ASCII code of the number we think matches. 

\subsection{Rule Based Programming}
\begin{itemize}
    \item
    Program a list of command sor rules
    \item
    Know every step required to complete programs task 
    \item 
    Stupid machine learning 
\end{itemize}

\subsection{Actual Machine Learning}
\begin{itemize}
    \item
    Algorithm figures out rules via trial and error and large well 
    described/labeled datasets
    \item
    Can discover rules and correlations that humans cannot see 
\end{itemize}

\section{Regression}
Predict a continuous value based off a dataset with 2 float values per item

You figure out how close the model is to being perfect by using the 
least squared error. 

\section{Classification}
Predict one of many discrete options based off a dataset with an item and a class 
per item. 

\section{Clustering}
A form of unsupervised learning where you divide data into certain classifications 
\textbf{without labels}. 

\section{2D Image Recognition}
Because we're in an ECE course, we treat the AI as a mapping from input to 
output. 

\section{Gradient Descent}
You take the partial derivation of the cost function to find the minimum.

\section{Multiple Features}
You do the same gradient descent and have a similar cost function except you're 
in a multi-dimensional space now. 

\subsection{Learning Rate}
If it's too small, it'll take forever, to learn, and if it's too large, it 
will never converge because 




\end{document}
