\documentclass[fleqn]{report}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{blindtext}
\usepackage{color}
\usepackage[fontsize=16pt]{fontsize}
\usepackage{lipsum}
\usepackage{pgfplots}
\usepackage{physics}
\usepackage{mathtools}
\usepackage[makeroom]{cancel}
\usepackage{ulem}
\usepackage{esint}
\usepackage{enumitem}

\geometry{a4paper, margin=1cm} % Set paper size and margins
\graphicspath{ {../Images/} }
\setlength{\columnsep}{1cm}
\addtolength{\jot}{0.1cm}
\def\columnseprulecolor{\color{blue}}
\date{Fall 2025}

\newcommand{\textoverline}[1]{$\overline{\mbox{#1}}$}

\newcommand{\hp}{\hspace{1cm}}

\newcommand{\const}{\textrm{const}}

\newcommand{\del}{\partial}

\newcommand{\pdif}[2]{ \frac{\partial #1}{ \partial #2} }

\newcommand{\pderiv}[1]{ \frac{\partial}{ \partial #1} }

\newcommand{\comment}[1]{}

\newcommand{\equations} [1] {
\begin{gather*}
#1
\end{gather*}
}

\newcommand{\numequations} [1] {
\begin{gather}
#1
\end{gather}
}

\newcommand{\twovec}[2]{ 
\begin{pmatrix}
#1 \\ 
#2
\end{pmatrix}
}

\title{PHYS 427}
\author{Aiden Sirotkine}

\begin{document}

\setlength{\headsep}{10pt}
\setlength{\topmargin}{-2cm}


\pagestyle{fancy}
\maketitle
\tableofcontents
\clearpage

\chapter{PHYS427}
I did a bunch of stat mech in my silly research group I'm assuming this won't 
be the end of the world. \

Much of this course builds on PHYS213

Homework has to be physically printed and put in a bin so I have to steal 
a ton of paper. 

Just do textbook problems if you don't understand any part of the material.

She seems like she enjoys this course. 

\chapter{Probability and Multiplicity}

\section{Microstates}
Complete description of the system. 

Knowing the position/momentum of every molecule in a box (unrealistic).

\section{Macrostates}
Gas pressure, volume, temperature, etc.

\subsection{Ex: Polymer}
Imagine a polymer 

The microstate would be the state of every single portion of the polymer. 

A macrostate could be the end-to-end extension of the polymer. 

\subsection{Ex: Magnet}
Microstate: 
the configuration of every single electron spin in the entire magnet. 

Macrostate:
magnetization of the magnet (average spin value)

\section{Particles In a Box}
Imagine a box with $N$ particles distributed in both the left half and the right half. 

Microstate: 
Configuration of all $N$ particles in either the right or left half.

Macrostate:
Total number of particles in the left box. 

\subsection{Number of Possible Microstates}
consider $n_L$ as the macrostate which is the number of particles in the left 
box.

\subsection{$\Omega(n_L, N)$}
$\Omega(n_L, N)$ is the number of microstates given 1 possible macrostate.

\begin{itemize}
\item 
Consider $N = 1$ and $n_L = 0$.
This means that only 1 particle has to be in the right box, 
so $\Omega(0, 1) = 1$.

if $n_L = 1$, then the 1 particle has to be in the left side, so 
$\Omega(1, 1) = 1$.

\item
What about N = 2?

if $n_L = 0$, then all particles have to be in the right side,
so $\Omega(0, 2) = 1$.

if $n_L = 1$, then either particle $A$ can be on the left side, or particle $B$ 
can be on the left side, 
so $\Omega(1, 2) = 2$.

if $n_L = 2$, then all particles have to be in the left side,
so $\Omega(2, 2) = 1$.

\item N = 3
$n_L = 0
\rightarrow \Omega(0, 3) = 1$.

$n_L = 1
\rightarrow \Omega(1, 3) = 3$.

$n_L = 2
\rightarrow \Omega(2, 3) = 3$.

$n_L = 3
\rightarrow \Omega(3, 3) = 1$.

\end{itemize}

For this example, the number of possible microstates grows 
exponentially. 

the total number of microstates is known as the ensemble of microstates.

The total number of microstates grows like $2^n$.

The total number of macrostates grows like $n$

For large $n$ the number of microstates becomes unreasonably big.

\subsection{General Formula for $\Omega$?}
I think you just use combinatorics.

For $n_L$ particles on the left side, there are $N - n_L$ particles on the 
right side. 

for the first particle going to $L$, you have $N$ choices.

for the second particle going to $L$, you have $N - 1$ choices.

for the third particle going to $L$, you have $N - 2$ choices.

go all the way until you have $n_L$ particles in the left bin. 

Then you remove all permutations that are the same as each other by 
dividing by $n_L!$

\equations{
    \frac{N(N-1)(N-2) \ldots (N - n_L + 1)}{n_L!}
    \Rightarrow 
    \\
    \frac{N!}{(N - n_L)! n_L!}
    =
    {N \choose n_L}
}

That's just the binomial coefficient.

If we consider $p$ and $q$ as the right and left sides 
of the box, then we can derive another formula.

For this box, $p = 1$ and $q = 1$ because there is 1 left side and 1 right side.
\equations{
    (p + q)^N 
    =
    \sum^N_{n = 0}
    {N \choose n}
    p^n q^{N - n}
    \\
    2^n 
    =
    \sum^N_{n = 0}
    {N \choose n}
    =
    \sum^N_{n = 0}
    \Omega(n, N)
}

\section{Probability of a Macrostate}

An assumption we make is that each microstate is equally likely. 
This is true for microstates, but \textbf{not true} for macrostates.

If a macrostate has many many many possible microstates, then it is more likely 
than a macrostate with fewer possible microstates. 

Let $p(n_L, N)$ be the probability of getting a certain macrostate
\equations{
    p(n_L, N)
    =
    \frac{\Omega(n_L, N)}{\sum^N_{n = 0} \Omega(n, N)}
    =
    \frac{\Omega(n_L, N)}{2^N}
    =
    \frac{{N \choose n_L}}{2^n}
    =
    \frac{N!}{n_L! (N - n_L!)} \cdot \frac{1}{2^N}
}
Graph this and you will see a spike for large $N$'s 

This means that there is a better and better defined macrostate that becomes 
overwhelmingly likely given the probabilities and microstates. 

This very likely macrostate is the equilibrium state.

For $N \to 6.022 * 10^{23} \approx \infty$, you get very well defined equilibrium 
states.

\section{Averages}
To work with the probabilities of macrostates, we have to normalize the function. 

\equations{
    \sum^N_{n = 0} p(n, N) = 1
}
This is easy to check (see lecture note 1)

\subsection{Expectation Value}
\equations{
    \langle n_L \rangle 
    =
    \frac{N}{2}
}

So to find the average of a function that takes $n_L$, 
\equations{
    f(n_L)
    \rightarrow 
    \langle f(n_L) \rangle 
    =
    \sum^N_{n = 0} 
    f(n_L) \cdot p(n_L, N)
    \rightarrow 
    \langle n_L^2 \rangle 
    =
    \sum^N_{n = 0} 
    n_L^2 \cdot p(n_L, N)
    \\
    \langle n_L \rangle 
    =
    \sum^N_{n = 0} 
    n_L \cdot p(n_L, N)
    =
    \sum^N_{n = 0} 
    n_L
    \frac{N!}{n_L! (N - n_L)!} (\frac{1}{2})^N
    = \ldots =
    \frac{N}{2}
}

Everything is dependent on the probability distribution of your macrostates.

\subsection{Math Trick}
\equations{
    (p + q)^N = 
    \sum^N_{n = 0}
    {N \choose n_L} p^{n_L} q^{N - n_L}
    \\
    (p \frac{\del}{\del p}) p^{n_L}
    =
    n_L p^{n_L}
    \\
    \langle n_L \rangle
    =
    \sum^N_{n = 0}
    {N \choose n_L}
    n_L p^{n_L} q^{N - n_L}
    =
    \\
    \left(
        p \frac{\del}{\del p}
        \sum^N_{n = 0}
        {N \choose n_L}
        p^{n_L} q^{N - n_L}
    \right)
    =
    p \frac{\del }{\del p}
    (p + q)^N
    =
    N p (p + q)^{N - 1}
    \\
    p = q = \frac{1}{2}
    \\
    \Rightarrow 
    \langle n_L \rangle 
    =
    \frac{N}{2}
}

\section{Variance}
\equations{
    \sigma_{n_L}^2
    =
    \langle 
        n_L - \langle n_L \rangle 
    \rangle^2
    =
    \langle 
        n_L^2 - 2 n_L \langle n_L \rangle 
        +
        \langle n_L \rangle^2 
    \rangle
    =
    \\
    \langle n_L^2 \rangle 
    -
    2 \langle n_L \rangle^2 + \langle n_L \rangle^2 = 
    \langle n_L^2 \rangle - \langle n_L \rangle^2
}

\section{Discussion 1}
All microstates are equally likely. 

$\Omega$ is the number of microstates in a given macrostate. 

\equations{
    S 
    =
    k_b \ln(\Omega) 
}

The general entropy equation can be written as 
\equations{
    S
    =
    -k_B 
    \sum_i 
    p_i \ln(p_i)
}

\subsection{}
Compute the average number of successful experiments given 
a binomial distribution of probability $p$. 

\equations{
    \langle n \rangle 
    =
    \sum^N_{n=0}
    P_p(n, N) n
    \hp 
    P_p(n, N)
    =
    {N \choose n}
    p^n (1 - p)^{N - n}
    \\
    {N \choose n}
    =
    \frac{N!}{n! (N - n)!}
    \\
    \sum^N_{n=0}
    P_p(n, N) n
    =
    \sum^N_{n=0}
    {N \choose n}
    p^n (1 - p)^{N - n}
    n
    =
    \sum^N_{n=0}
    \frac{N! * n}{n! (N - n)!}
    p^n (1 - p)^{N - n}
    \\
    \sum^N_{n=0}
    n 
    =
    \frac{N(N+1)}{2}
    \hp 
    \sum^N_{n=0}
    p^n 
    =
    p^0 + p^1 + p^2 + p^3
}
I give up I'm stupid 

Supposedly the answer is $Np$ and that makes sense but I have no idea 
how to get to it.

The given answer is 
\equations{
    \langle n \rangle 
    =
    p \frac{\del }{\del p}
    \left(
        (p + q)^N
    \right)
    =
    Np(p + q)^{N-1}
    \hp
    p + q = 1
    \\
    \langle n \rangle 
    =
    Np
}

Stirling's approximation is 
\equations{
    \ln(N!)
    =
    N \ln(N) - N
    \hp 
    N \to \infty
}

\subsection{Paramagnet}
Consider a lattice of $N$ particles with energy 

\equations{
    U 
    =
    - \mu B (N_{\uparrow} - N_{\downarrow})
}

\begin{enumerate}[label=\alph*)]
    \item 
    Calculate the entropy for very large $N$ 
    specifically with just $U$, $N$, and $B$

    Supposedly the answer is given by 
    \equations{
        \Omega 
        = 
        \frac{(N_{\uparrow} 
        - 
        N_{\downarrow})}{N_{\uparrow}! N_{\downarrow}!}
        \hp 
        U 
        =
        - \mu B (N_{\uparrow} - N_{\downarrow})
        \hp 
        N_{\uparrow} + N_{\downarrow} = N
        \\
        \Omega 
        =
        \frac{N!}{(\frac{N}{2} - \frac{U}{2 \mu B})!
        (\frac{N}{2} + \frac{U}{2 \mu B})!}
        \hp 
        S 
        =
        k_B \ln(\Omega)
    }

    Do some log stuff and \textbf{use Stirling's Approximation}
    and then you get a decent number.
    \item 
    Compute the energy of the system at temperature $T$

    Okay so we can solve for temperature, and then we 
    can replace parts of the equation for $T$ to get the energy 
    as a function of $T$.

    \equations{
        \frac{1}{T}
        \equiv
        \frac{\del S}{\del U}
        \hp 
        S 
        =
        k_b \ln(
        \frac{N!}{(\frac{N}{2} - \frac{U}{2 \mu B})!
        (\frac{N}{2} + \frac{U}{2 \mu B})!}
        )
    }
\end{enumerate}

\section{Probability Width}
We talked about for $n_L$ when $\langle n_L \rangle = N/2$, as 
$N$ gets large, there is a significant peak in probability at $N/2$. However, 
we do not know the width of the peak. 

\equations{
    \sigma_{n_L}^2 
    =
    \langle n_L^2 \rangle - \langle n_L \rangle^2
    \hp 
    \langle n_L^2 \rangle
    =
    \frac{N (N+1)}{4}
}

With derivation 
\equations{
    \left( p \frac{\del}{\del p}\right)^2 
    p^{n_L}
    =
    n_L^2 p^{n_L}
    \\
    p \frac{\del}{\del p}
    \left( p \frac{\del}{\del p}\right) p^{n_L}
    \rightarrow 
    p \frac{\del}{\del p}
    \left( p \cdot n_L p^{n_L - 1} \right)
    \rightarrow 
    p \frac{\del}{\del p}
    \left( n_L p^{n_L} \right)
    =
    n_L^2 p^{n_L}
}

Now we can determine the average of the square

Remember that $p = q = 1/2$
\equations{
    \langle n_L^2 \rangle
    =
    \sum^N_{n=0}
    n_L^2 {N \choose n_L}
    p^{n_L} q^{N - n_L}
    \rightarrow 
    \\
    \left( p \frac{\del}{\del p}\right)^2
    \sum^N_{n=0}
    {N \choose n_L}
    p^{n_L} q^{N - n_L}
    =
    \left( p \frac{\del}{\del p}\right)^2
    (p + q)^{N}
    \\
    \langle n_L^2 \rangle 
    =
    \sum_{n_L}
    n_L^2 P(n_L, N)
    \\
    p \frac{\del}{\del p}
    \left( 
        N p (p + q)^{N - 1} 
    \right)
    =
    N p (p + q)^{N - 1}
    +
    N(N -1)
    p^2 (p + q)^{N - 2}
    \\
    \langle n_L^2 \rangle 
    =
    \sum^N_{n_L = 0}
    {N \choose n_L} n_L^2 
    \left(\frac{1}{2}\right)^N
    \equiv 
    \frac{N}{2}
    +
    \frac{N(N-1)}{4}
    =
    \frac{N(N+1)}{4}
    \\
    \sigma_{n_L}^2 
    =
    \langle n_L^2 \rangle
    -
    \langle n_L \rangle^2
    =
    \frac{N(N+1)}{4}
    -
    \left(\frac{N}{2}\right)^2
    =
    \frac{N}{4}
    \\
    \sigma_{n_L}
    =
    \frac{\sqrt{N}}{2}
}

The standard deviation increases, but it increases less quickly
than $N$ and $\langle n_L \rangle$, so the ratio of width to number 
of particles decreases 

\equations{
    \frac{\sigma_{n_L}}{\langle n_L \rangle}
    =
    \frac{1}{\sqrt{N}}
    \underset{N \to \infty}{\longrightarrow}
    0
}

\section{Physical Systems}
We know that 
\equations{
    \Omega 
    =
    \textrm{ Multiplicity}
}
But how does this look for real systems?

\subsection{Closed System}
Energy is conserved. Also known as an isolated system.

No change in $E, U, N$

So we're trying to find 
$\Omega(U, N)$

\subsection{Einstein Solid}
Imagine a lattice of particles connected via strings. 

N identical independent quantum harmonic oscillators. 

The quantum harmonic oscillators have energy levels of the form 
\equations{
    E 
    =
    \hbar \omega 
    (s_i + \frac{1}{2})
    , 
    s_i
    =
    \mathbb{N}
}

This is derived from the Schrodinger Equation 
\equations{
    H \Psi(x, t)
    =
    - \frac{\hbar^2}{2m}
    \frac{\del^2}{\del x^2}
    \Psi(x, t)
    +
    m \omega^2 x^2 \Psi(x, t)
    =
    \epsilon_i \Psi(x, t)
}

But this is not actually important for the material.

Given $N$ oscillators 
\equations{
    U 
    =
    \sum^N_{i=0}
    \epsilon_i
    =
    \hbar \omega (n + \frac{N}{2})
    \hp 
    n 
    =
    \sum^N_i
    s_i
}
$n$ is the number of energy quanta that you have. 

The $+ \frac{1}{2}$ in energy does not affect the statistics whatsoever, 
so we can ignore it. 

To get the multiplicity, we have to know the number of microstates for 
the same amount of energy in $N$ oscillators. 

How many ways are there to have different $\sum s_i$ such that they all 
yield the same quantum number $n$.

Oh my good we're doing stars and bars. 

$\cdot$ is a quanta and $|$ is the positions between the oscillators.

\equations{
    \cdot 
    |
    \cdot 
    \cdot 
    \cdot 
    |
    |
    \cdot 
    \cdot 
    \cdot 
    \cdot 
    |
    \cdot 
    |
    ...
}
represents $s_1 = 1, s_2 = 3, s_3 = 0, s_4 = 4$

There are always $n$ dots and $N-1$ bars going 
into $n + (N - 1)$ slots.

Therefore, the total number of ways to distribute it is. 

\equations{
    \Omega(U, N)
    =
    {n + N - 1 \choose n}
    =
    \frac{(n + N - 1)!}{n! (N - 1)!}
}

\subsection{Stirling's Approximation}
This is literally the more important thing in the 
entirety of statistical mechanics.

\equations{
    \ln(N!)
    \underset{N \to \infty}{\approx} 
    N \ln(N) - N
}
The approximation works from 
a sum from 1 to large $N$ being similar to a continuous integral.

\section{Entropy}
The entropy involves the log of the multiplicity, so we can calculate that 
for our Einstein solid. 

\equations{
    \ln(\Omega(U, N))
    \approx 
    \ln((n + N)!)
    -
    \ln(N!)
    -
    \ln(n!)
    \hp 
    \textrm{Use Stirling's Approx.}
    \\
    \approx 
    (n + N) \ln(n + N) - (n + N)
    -
    N \ln(N) + N - n \ln(n) + n
    \\
    =
    (n + N)\ln(n + N)
    -
    N \ln(N) - n \ln(n)
}

\subsection{High Temperature Limit}
This is known as the high temperature limit 
because we're assuming that $n >> N$, meaning there are many 
possible energy states for each particle.

\equations{
    \ln(n + N)
    =
    \ln(n \left( 1 + \frac{N}{n} \right))
    =
    \ln(n)
    +
    \ln(1 + \frac{N}{n} )
    \\
    \ln(1+x) \approx x: x << 1
    \\
    \ln(n + N)
    \approx 
    \ln(n)
    +
    \frac{N}{n}
}

We than plug this result back into our original expressions 

\equations{
    (n + N)\ln(n + N)
    -
    N \ln(N) - n \ln(n)
    \approx 
    \\
    (n + N) (\ln(n) + \frac{N}{n})
    -
    N \ln(N) - n \ln(n)
    =
    \\
    (n + N)\ln(n) + (n + N)\frac{N}{n}
    -
    N \ln(N) - n \ln(n)
    =
    \\
    n \ln(n) + N \ln(n) + (n + N)\frac{N}{n}
    -
    N \ln(N) - n \ln(n)
    =
    \\
    N (\ln(n) - \ln(N)) + (N + \frac{N^2}{n})
    =
    \\
    N \ln(\frac{n}{N})
    +
    N(1 + \frac{N}{n})
    \approx 
    N \ln(\frac{n}{N})
    + N
}

You can then re-exponentiate that expression. 

\equations{
    \Omega(U, N)
    =
    e^{\ln(\Omega(U, N))}
    =
    (e \frac{n}{N})^N
    =
    \left(
        \frac{eU}{N \hbar \omega}
    \right)^N
    \\
    \Omega 
    \sim 
    U^N
}

For an ideal gas, $\Omega \sim U^f, f \sim N$ 

\subsection{Entropy Property}
For two independent systems: 

\equations{
    \Omega_{A + B}
    =
    \Omega_A
    \Omega_B
    \Rightarrow 
    \sigma_{A+B}
    =
    \sigma_A
    +
    \sigma_B
}

For changing system size

\equations{
    \sigma 
    =
    N 
    \ln(\frac{e U}{N \hbar \omega})
}

If both $N$ and $U$ double in size 

\equations{
    N \to 2N
    \hp 
    U \to 2U
    \\
    \sigma 
    =
    N 
    \ln(\frac{e U}{N \hbar \omega})
}

The variance is independent of system size. 

Intensive quantities are independent of system size $T, p$

$\sigma$ goes up logaritmically with $U$

\subsection{Not High Temperatures}
All of this \textbf{only works because of the high temperature 
approximation}

\equations{
    U >> N \hbar \omega
}

So the limit 
\equations{
    \sigma(U \to 0)
    =
    - \infty
}
is wrong because it ignore the approximation we made. 

\equations{
    \sigma(U \to 0)
    \rightarrow 
    \ln(1)
    =
    0
}
That is what actually happens at low temperatures.

\section{Paramagnet}
Imagine a paramagetic with $N$ independent spins of both $\uparrow$ 
and $\downarrow$. 

\equations{
    N = n_{\uparrow}
    +
    n_{\downarrow}
}

The paramagnet has a magnetic moment given by 
\equations{
    \vec \mu 
    =
    g \frac{q}{2m} \vec s 
    \\
    \epsilon 
    =
    - \vec \mu \cdot \vec B 
    =
    - \mu_z B
}

The general solution for magnetic moment in this case is 
\equations{
    \mu_z 
    =
    g \frac{q \hbar}{2 m} m_s
    \hp 
    m_s 
    =
    -s, -s + 1, \ldots , s-1, s
    \\
    m_s = \frac{1}{2} \textrm{ or } \frac{-1}{2}
    \hp 
    g \approx z 
    \\
    \mu_B
    =
    \frac{e \hbar }{2 m_e}
    \hp 
    \textrm{Bohr Magneton}
    \\
    \epsilon 
    =
    - \mu B
    , 
    \epsilon 
    =
    + \mu B
    \\
    U 
    =
    n_{\uparrow}
    (- \mu B)
    +
    n_{\downarrow}
    (\mu B)
    =
    - \mu B 
    (n_{\uparrow} - n_{\downarrow})
    =
    - \mu B 
    (2 n_{\uparrow} - N)
}

For a binary system, we get 
\equations{
    \Omega(n_{\uparrow}, N)
    =
    {N \choose n_{\uparrow}}
    =
    \frac{N!}{n_{\uparrow}! (N - n_{\uparrow})!}
    \\
    \Omega(U, N)
    =
    \frac{N!}
    {
        \left(
            \frac{N}{2}
            -
            \frac{U}{2 \mu B}
        \right)
        !
        \left(
            \frac{N}{2}
            +
            \frac{U}{2 \mu B}
        \right)
        !
    }
}
Use Stirling Approximation and put the terms together. 

After everything is said, we should get 
\equations{
    \ln(\Omega(U, N))
    =
    N \ln(N) 
    -
    (N - n_{\uparrow})
    \ln( (N - n_{\uparrow}))
    -
    n_{\uparrow }
    \ln( n_{\uparrow })
    \\
    \sigma 
    =
    \ln(\Omega(U, N))
    =
    \\
    N \ln(N) 
    -
    \left(
        \frac{N}{2}
        -
        \frac{U}{2 \mu B}
    \right)
    \ln(
        \frac{N}{2}
        -
        \frac{U}{2 \mu B}
    )
    -
    \left(
        \frac{N}{2}
        +
        \frac{U}{2 \mu B}
    \right)
    \ln(
        \frac{N}{2}
        +
        \frac{U}{2 \mu B}
    )
}

The variance to energy graph looks like a circle because 
the lowest variances are at 2 edges where all is up 
or all is down.

You can calculate $\sigma(U = 0, N)$ by just plugging in the 
numbers, and you get $N \ln(2)$.

\section{Discussion 2}
In thermal equilibrium, a closed system 
is equally likely to be in any of the microstates available 
to it. 

\equations{
    \Omega 
    =
    \textrm{total \# of microstates available}
    \\
    S = k_b \ln{\Omega}
}

\subsection{Mini Equation Sheet}
\equations{
    \frac{1}{T}
    \equiv 
    \left(
        \frac{\del}{\del U}
    \right)_{N, V}
    \\
    \textrm{chemical potential}
    =
    \mu
    \equiv
    -T \left(
        \frac{\del S}{\del N}
    \right)_{U, V}
    \\
    P 
    \equiv
    T \left(
        \frac{\del S}{\del V}
    \right)_{U, N}
}
These are Lagrange Multipliers which I don't remember 

\equations{
    \Omega(U, N)
    =
    \sum_i
    \delta_k(U - U_i)
    \delta_k(N - N_i)
    \\
    \delta_{i, k}(x)
    =
    \begin{cases}
        1 \textrm{ if } x = 0
        \\
        0 \textrm{ otherwise}
    \end{cases}
}
The kronecker delta enforces boundary conditions. 

\equations{
    \delta(ax)
    =
    \delta(x)
    :
    a \neq 0
}

\equations{
    \sum_m \sum_n
    g(m, n)
    \delta(P(m, n))
    =
    \sum_m
    g(m, n = n^*(m))
    \\
    n*(m)
    \Rightarrow 
    P(m, n^*) 
    =
    0
}

\subsection{Stars and Bars}
\equations{
    \Omega(n, N)
    =
    {N + n - 1 \choose n}
}

Imagine a bunch of stars and bars 
\equations{
    \cdot 
    \cdot 
    ||
    \cdot 
    |
    \cdot 
    \cdot 
    |
    \cdot 
    ||
    \cdot 
}

each section between 2 bars is a certain classification, and each star 
in between the bars is an item that is a part of that classification. 

\subsection{Questions}
Find the entropy for multiplicity 

\equations{
    \Omega(U, N, V)
    =
    \frac{V^N (CU)^{3N/2}}
    {N! (3N / 2)!}
}

So you do the thing 

\equations{
    S 
    =
    k 
    \ln(
    \frac{V^N (CU)^{3N/2}}
    {N! (3N / 2)!}
    )
    =
    \ln(
    V^N (CU)^{3N/2}
    )
    -
    \ln(
    N! (3N / 2)!
    )
    \\
    \ln(
    V^N
    )
    + \ln(
    (CU)^{3N/2}
    )
    -
    \ln(
    N!
    )
    - \ln(
    (3N / 2)!
    )
    \\
    N \ln(
    V
    )
    + 3N/2\ln(
    CU
    )
    -
    N\ln(
    N
    )
    +N
    -3N / 2 \ln(
    (3N / 2)
    )
    +3N / 2
    \\
    N \ln(
    V
    )
    + 3N/2\ln(
    CU
    )
    -
    N\ln(
    N
    )
    +N
    -3N / 2 \left(
    \ln(
    (N)
    )
    + \ln(3/2)
    \right)
    \\
    +3N / 2
}

You just do a bunch of math and it works. 

It's not hard everything is fine 

\subsection{Determining Multiplicity Functions}
Consider 
\equations{
    U = \hbar \omega n 
    \hp 
    \epsilon 
    =
    \hbar \omega 
    (n_x + n_y + n_z)
}
Where $n_i = 0, 1, 2, \ldots, \infty$ 

I think we're just gonna do stars and bars but with 3. 

there is a total energy $N$ that needs to be split $3$ different ways 
for the energy to be in $n_x, n_y$, or $n_z$. 

\chapter{Temperature}

\section{Thermal Contact}
Imagine a box $A$ and a box $B$ that are able to interact with each other. 
Each box has multiplicity $\Omega_A$ and $\Omega_B$

The multiplity and energy of the combined boxes is 
\equations{
    \Omega_{AB}
    =
    \Omega_{A}
    \cdot
    \Omega_{B}
    \hp 
    U_{AB}
    =
    U_{A}
    +
    U_{B}
}

$U_A$ and $U_B$ can change, but the combined energy is fixed. 

To write everything in terms of just a single box, we can say that 

\equations{
    \Omega_{tot}
    =
    \sum_{U_A}
    \Omega_A(U_A)
    \cdot
    \Omega_B(U - U_A)
}

We know that approximately 

\equations{
    \Omega_A(U_A)
    \sim
    U_A^N
}

We find that the total multiplicity has a peak centered at
the average $1/U_A$ and with a relative width of $1/\sqrt{N}$.

Consider the natural log $\ln(\Omega)$.
We also know that at the peak

\equations{
    \frac{d \Omega}{d U}
    =
    0
    \hp 
    \frac{d \ln(\Omega)}{d U}
    =
    \frac{1}{\Omega}
    \frac{d \Omega}{d U}
    = 
    0
}


We look for $U_A$ where 
\equations{
    \frac{\del \ln(\Omega_A(U_A) \Omega_B(U - U_A))}{\del U_A}
    =
    0
}
So we do some calculations 

\equations{
    \ln(\Omega_A(U_A) 
    \Omega_B(U - U_A)) 
    =
    \ln(\Omega_A(U_A))
    +
    \ln(\Omega_B(U - U_A)) 
}

So our equation is 

\equations{
    \frac{\del}{\del U_A}
    \left(
        \ln(\Omega_A(U_A))
    \right)
    +
    \frac{\del}{\del U_A}
    \left(
        \ln(\Omega_B(U - U_A))
    \right)
    =
    0
}

Know the derivative 
\equations{
    \frac{\del \ln(U^N)}{\del U}
    =
    \frac{1}{U^N}
    \cdot 
    N (U^{N-1})
    =
    \frac{N}{U}
}

This solves to 

\equations{
    \frac{N_A}{\bar{U_A}}
    -
    \frac{N_B}{\bar{U - U_A}}
    =
    0
    \rightarrow 
    \frac{U}{\bar{U_A}}
    -
    1
    =
    \frac{N_B}{N_A}
    \\
    \overline U_A
    =
    \frac{U}{1 + \frac{N_B}{N_A}}
    \hp 
    \overline{U}_B
    =
    \frac{U}{1 + \frac{N_A}{N_B}}
    =
    U - \overline U_A
}

Notice that from our equations we get 
\equations{
    \frac{\overline U_A}{N_A}
    =
    \frac{\overline U_B}{N_B}
    =
    \frac{U}{N_A + N_B}
}

So our peak multiplicity is found where 

\equations{
    \overline U_A 
    =
    \frac{U}{1 + \frac{N_B}{N_A}}
}

The energy per particle is the same in both $A$ and $B$ and it is equal 
to 
\equations{
    \frac{U}{N}
    =
    \frac{U}{N_A + N_B}
}

\subsection{Peak Sharpness}
We are trying to find how sharp the multiplicity peak is 

\equations{
    N_A 
    =
    N_B 
    \Rightarrow 
    \overline U_A
    =
    \frac{U}{2}
    \hp
    \overline U_B
    =
    \frac{U}{2}
}

For an Einstein solid, we can do a small perturbation and then take a taylor 
expansion around that point to find the equation 

\equations{
    U_A 
    =
    \overline U_A
    +
    \delta U
}

Our energy function is given as 
\equations{
    \Omega(U)
    =
    \left(
        \frac{eU}{N \hbar \omega}
    \right)^N
    =
    \Omega_A(U_A)
    +
    \Omega_B(U - U_A)
}

We can do a taylor expansion around this to get 
\equations{
    \Omega_A(U_A)
    +
    \Omega_B(U - U_A)
    \\
    =
    2N + N \ln(\frac{U_A + \delta U}{N \hbar \omega})
    +
    N \ln(\frac{U_A - \delta U}{N \hbar \omega})
    \\
    =
    2N 
    - 
    2N \ln(N \hbar \omega)
    +
    N \ln(\overline U_A^2 - (\delta U)^2) 
    \\
    =
    2 N \ln(\frac{e U_A}{N \hbar \Omega})
    +
    N \ln(1 - \frac{\delta U^2}{\overline U_A^2})
}

Use a $\ln(1+x) \approx x$ approximation to get 

\equations{
    2 N \ln(\frac{e U_A}{N \hbar \Omega})
    -
    N \frac{\delta U^2}{\overline U_A^2}
}

So now we have 
\equations{
    \Omega 
    =
    \Omega_A(U_A)
    \Omega_B(U - U_A)
    \approx 
    \left(
    \frac{e U_A}{N \hbar \Omega}
    \right)^{2N}
    \exp({- \frac{N \delta U^2}{U_A^2}})
    \\
    \approx 
    \left(
    \frac{e U_A}{N \hbar \Omega}
    \right)^{2N}
    \exp({- \frac{ \delta U^2}{2 \sigma^2}})
}

So our variance and standard deviation are given by 
\equations{
    \sigma_{U_A}
    =
    \frac{\overline U_A}{\sqrt{2N}}
}

And the fractional width is given by 
\equations{
    \frac{\sigma_{U_A}}{\overline U_A}
    =
    \frac{1}{\sqrt{2N}}
}

The multiplicity can again be written as 
\equations{
    \Omega_{tot}
    =
    \sum_{U_{A}}
    \Omega_A(U_A)
    \Omega_B(U - U_A)
    \approx 
    \Omega_A(\overline U_A)
    \Omega_B(U - \overline U_A)
}

This works almost perfectly for large N $(N \approx 10^{20})$

\section{Thermal Equilibrium}
The entropy equation is 
\equations{
    \sigma 
    \equiv 
    \ln(\Omega)
}

When systems $A$ and $B$ are in thermal contact at $t=0$, then 
\equations{
    \sigma_{tot}
    =
    \sigma_A(U_A)
    +
    \sigma_B(U - U_A)
}

initially, let's say that $U_A > U_B$.
Then, over time, $U_A$ will decrease and $U_B$ will increase until both 
reach $\overline U_A$ and $\overline U_B$.

The entropy of $\sigma_A$ might decrease, but the total entropy 
of the two systems will always increase when going towards equilibrium. 

Thermal equilibrium is reached when total entropy is maximized. 

To maximize total entropy, we find a critical point 
\equations{
    \frac{\del \sigma_{tot}}{\del U_A}
    =
    0
    =
    \frac{\del \sigma_{A}}{\del U_A}
    +
    \frac{\del \sigma_{B}}{\del U_B}
    =
    \frac{\del \sigma_{A}}{\del U_A}
    +
    \frac{\del \sigma_{B}}{\del U_B}
    \cdot
    \frac{\del U_B}{\del U_A}
}

So at equilibrium 
\equations{
    \frac{\del \sigma_A}{\del U_A}
    =
    \frac{\del \sigma_{B}}{\del U_B}
}

At thermal equilibrium there is a fundamental temperature $\tau$ 
\equations{
    \frac{1}{\tau}
    =
    \left(
        \frac{\del \sigma}{\del U}
    \right)_N
    \Rightarrow 
    \tau_A 
    =
    \tau_B
}

\subsection{0th Law of Thermodynamics}
If system $A$ is in thermal equilibrium with systems $B$ and $C$, then 
$B$ is in thermal equilibrium with $C$.

This is just the transitive property but with equilibrium.

\subsection{Conversions}

$\tau$ is a form of energy. 
The absolute temperature is $T$ in regular temperature units (Kelvin)

\equations{
    \tau 
    \equiv 
    k_B T 
    \hp 
    k_B
    =
    1.371 * 10^{-23}
    \frac{J}{K}
}

The regular entropy you are used to is 
\equations{
    S 
    \equiv 
    k_B \ln(\Omega)
    \hp 
    \frac{1}{T}
    \equiv
    \left(
        \frac{\del S}{\del U}
    \right)_N
}

So for the einstein solid example we were talking about, the initial 
parameters lead to 

\equations{
    \frac{\del S}{\del U_A}
    \Big|_{U_A(t=0)}
    <
    \frac{\del S}{\del U_B}
    \Big|_{U_B(t=0) = U - U_A(t=0)}
}

So 
\equations{
    T_A > T_B
}

$S_{tot}$ increases when $U_A$ goes down and $U_B$ goes up.
Entropy flows from the hotter system to the cooler system.

\subsection{How Much Energy is Transferred?}
\equations{
    S_{tot}
    =
    \Delta S_A
    +
    \Delta S_B 
    =
    \frac{\del S}{\del U_A}
    \cdot 
    \Delta U_A
    +
    \frac{\del S}{\del U_B}
    \cdot 
    \Delta U_B
    \\
    =
    \frac{1}{T_A}
    \Delta U_A
    +
    \frac{1}{T_B}
    \Delta U_B
    \hp 
    \Delta U_B
    =
    -\Delta U_A
    \\
    \Delta S_{tot}
    =
    \left(
        \frac{1}{T_A}
        -
        \frac{1}{T_B}
    \right)
    \Delta U_A 
}

So if $T_A > T_B$, then 

\equations{
    \left(
        \frac{1}{T_A}
        -
        \frac{1}{T_B}
    \right)
    <
    0
}

So for entropy to increase, $\Delta U_A < 0$. 
Energy flows from hot to cold. This energy transfer is heat $Q$.

\equations{
    \Delta U_A
    =
    T_A \Delta S_A 
    =
    - \Delta U_B 
    =
    - T_B \delta S_B
}

\subsection{Einstein Solid}
\equations{
    \Omega 
    \simeq 
    U^N 
    \\ 
    S 
    =
    k_B \ln(\Omega)
    =
    k_b N \ln(U)
    +
    \textrm{terms independent of $U$}
}

Let's take the derivative with respect to $U$ 

\equations{
    \frac{1}{T}
    =
    \frac{\del S}{\del U}
    =
    N k_B \frac{\del}{\del U} \ln(U)
    =
    \frac{N k_B}{U}
    \rightarrow 
    U 
    =
    N k_B T 
}

Many systems have multiplicities of the form 
\equations{
    \Omega 
    =
    U^f 
    \hp 
    f 
    \propto 
    N
    \Rightarrow 
    k_B T 
    \sim 
    \frac{U}{f}
}

\section{Heat Bath/Reservoir}
Consider a very large room $A$ with temperature $T_A$ connected to your 
experimental system $B$ with temperature $T_B$. $U_A >> U_B$ and $N_A >> N_B$.
 
Because of those, any change in $B$ should not affect the reservoir, so 
$T_A$ does not change.

Temperature is defined as 
\equations{
    \frac{1}{T}
    =
    \left(
        \frac{\del S}{\del U}
    \right)_N
}

All the information about heat baths is given in lecture 3. 

For an einstein solid of the form 
\equations{
    \Omega 
    \sim 
    U^f
}

For a paramagnet, the entropy-energy curve has 2 separate zero-points. 
It looks like a semicircle, and the two zero-points are all spin-up and 
all spin-down. 

If you take the derivative of that graph to get temperature, you see 
that temperature is positive on the spin-up side of the graph, but 
negative for the spin-down part of the graph. 

At the apex, temperature goes to infinity because $1/T \to 0$ 

Let's let an unstable $(T_A \to 0)$ paramagnet be in contact with a system. 

We know that entropy increases on average, so which direction does 
energy move from entropy to increase?
\equations{
    \Delta S_{tot}
    =
    \Delta S_A
    +
    \Delta S_B 
    =
    \frac{\del S_A}{\del U_A}
    \Delta U_A
    +
    \frac{\del S_B}{\del U_B}
    \Delta U_B
    \hp 
    \Delta U_B = - \Delta U_A 
    \\
    =
    \left(
        \frac{1}{T_A}
        -
        \frac{1}{T_B}
    \right)
    \Delta U_A
}

We can do something to get rid of $1/T_A$ so that we get 

\equations{
    \Delta S_{tot}
    =
    -
    \frac{1}{T_B}
    \Delta U_A
    \geq 
    0
    \\
    T_A \to \infty 
    \hp 
    T_B > 0 
    \hp 
    \Delta U_A \leq 0
}

Now let's consider $T_A < 0$ and $T_B > 0$ 

\equations{
    \Delta S_{tot}
    =
    \left(
        \frac{1}{T_A}
        -
        \frac{1}{T_B}
    \right)
    \Delta U_A 
    =
    \left(
        -
        \frac{1}{|T_A|}
        -
        \frac{1}{|T_B|}
    \right)
    \Delta U_A 
    \geq 0
    \\
    U_A < 0
}

\section{Entropy of an Ideal Gas}
Consider a particle in a box of volume 
\equations{
    V 
    =
    L_x
    L_y
    L_z
}

An you have a multiplicity function with inputs 
\equations{
    \Omega(U, N)
}

The hamiltonian operator of an ideal gas is given as 
\equations{
    H(\Psi(\vec r, t))
    =
    -
    \frac{\hbar}{2m}
    \nabla^2 
    \Psi(\vec r, t)
    =
    \epsilon 
    \Psi(\vec r, t)
    \\
    \Psi(\vec r, t)
    =
    \frac{1}{\sqrt{v}}
    e^{i \vec k \vec r - i \omega t}
    \hp 
    \epsilon 
    =
    \frac{p^2}{2m}
    =
    \frac{\hbar^2}{2m}
    |\vec k|^2
    =
    \frac{\hbar^2}{2m}
    (k_x^2 + k_y^2 + k_z^2)
    \\
    k_x 
    =
    \frac{\pi}{L_x} n_x 
    \hp 
    n 
    =
    1, 2, 3, \ldots
}

This looks like a quantum harmonic oscillator. 
The discrete set of energy eigenstates are given in the form 

\equations{
    \epsilon 
    =
    \frac{\hbar^2 \pi^2}{2m}
    -
    \left(
        \frac{n_x^2}{L_x^2}
        +
        \frac{n_y^2}{L_y^2}
        +
        \frac{n_z^2}{L_z^2}
    \right)
}

$\vec k$ can be written in the form 
\equations{
    \vec k 
    =
    \sqrt{
        \frac{2 m \epsilon}{\hbar^2}
    }
}

So now our multiplicity function can be written as 
\equations{
    \Omega(\epsilon, N)
}

So if we consider a large lattice of potential states, each quantum 
state is represented by a point on the lattice. 

The axes are $k_x, k_y, k_z$, and each interval is $\pi/L$

The number of possible quantum states for a certain energy is given 
by all points that intersect a \textbf{shell} of radius $|\vec k|$. 
The energy is given in the form $\epsilon = \hbar^2 |\vec k|^2 / 2m$.

This cannot be solved trivially, but to start, we can find all the points 
that are inside a \textbf{sphere} of radius $|\vec k|$.

We only consider the shell in a \textbf{single octant}, so we 
divide our volume by $8$.

each state space occupies a point in a box with volume
\equations{
    \frac{\pi}{L_x}
    \frac{\pi}{L_y}
    \frac{\pi}{L_z}
    =
    \frac{\pi^3}{V}
}

So we can just divide that volume by our sphere 

\equations{
    \frac{
        \textrm{\# state with energy $\leq \epsilon$}
    }
    {
        \textrm{volume of box each state occupies }
    }
    =
    \Psi(\epsilon)
}

And the number of microstates with energy $\leq \epsilon$ can be written as 
\equations{
    \frac{
        \textrm{
            volume of sphere with radius $r = |\vec k|
            = \sqrt{\frac{2m\epsilon}{\hbar^2}}$
        }
    }
    {
        \textrm{volume per state}
    }
}

This can be written in the form 
\equations{
    \Phi(\epsilon)
    =
    \frac{
        \frac{4}{3} \frac{\pi |\vec k|^3 }{8}
    }
    {
        \frac{\pi^3}{V}
    }
    =
    \frac{V}{6 \pi^2}
    |\vec k|^3
    =
    \frac{V}{6 \pi^2}
    \left(
        \frac{2m}{\hbar^2}
    \right)^{3/2}
    \epsilon^{3/2}
}

This approximation is good for 
\equations{
    |\vec k| 
    >> 
    \Delta k 
    =
    \frac{\pi}{L}
    \hp
    \textrm{ or } 
    \hp
    \epsilon >> \Delta \epsilon 
    =
    \frac{\pi^2 \hbar^2}{2m V^{2/3}}
}

If the energy of the particle is much larger than the 
spacing between each energy level. 

Now, to find the amount of microstate for a \textbf{specific} energy, 
we give our sphere are small change in radius, and we see the resulting change 
in number of microstates from that 

\equations{
    \Omega(\epsilon, \epsilon + \delta \epsilon)
    =
    \Phi(\epsilon + \delta \epsilon)
    -
    \Phi(\epsilon)
    \approx 
    \frac{\del \Phi}{\del \epsilon}
    \delta \epsilon 
    =
    \frac{V}{4 \pi^2}
    \left(
        \frac{2m}{\hbar^2}
    \right)^{3/2}
    \sqrt{\epsilon}
    \delta \epsilon
}

This is the same answer as getting the volume of the shell and 
dividing it by the volume of each state 

\equations{
    \frac{V_{shell}}{V_{state}}
    =
    \frac{
        \frac{4 \pi |\vec k|^2}{8} \delta k
    }
    {
        \frac{\pi^3}{V}
    }
    =====
    \frac{V}{4 \pi^2}
    \left(
        \frac{2m}{\hbar^2}
    \right)^{3/2}
    \sqrt{\epsilon}
    \delta \epsilon
}

\subsection{2D Ideal Gas in an Area $A$}
We have a state area 
\equations{
    \frac{\pi}{L_x}
    \frac{\pi}{L_y}
    =
    \frac{\pi^2}{A}
}

Now you see how many state volumes fit in the quarter circle 
of a certain energy level. 

\equations{
    \Phi_{2d}(\epsilon)
    =
    \frac{
        \frac{1}{4}
        \pi |\vec k|^2
    }
    {
        \left(
            \frac{\pi^2}{A}
        \right)
    }
    \hp 
    k 
    =
    \sqrt{\frac{2m \epsilon}{\hbar^2}}
}

You now perform the same perturbation math to get 
\equations{
    \Omega(\epsilon, \epsilon + \delta \epsilon)
    =
    \frac{\del \Phi}{\del \epsilon}
    \delta \epsilon 
    =
    \frac{A}{4 \pi}
    \left(
        \frac{2m}{\hbar^2}
    \right)
    \delta \epsilon
}

\section{Density of States}
The number of states per energy interval 

\equations{
    D(\epsilon)
    =
    \frac{\del \Phi}{\del \epsilon}
    \hp
    \textrm{ or }
    \hp
    \int^\epsilon_0
    \, d \epsilon' \, 
    D(\epsilon')
    =
    \Phi(\epsilon)
    \\
    \Omega(\epsilon, \epsilon + \delta \epsilon)
    =
    D(\epsilon)
    \delta \epsilon
}

\section{2 Particles in a Box}
Consider 2 particles in a box. Energy is given by 
\equations{
    U 
    =
    \epsilon_1 
    +
    \epsilon_2
    =
    \frac{p_1}{2m}
    +
    \frac{p_2}{2m}
    =
    \frac{\hbar^2 k^2}{2m^2}
}

And our energy space is now given by 

\equations{
    k 
    =
    \sqrt{
        k_{x1}^2
        +
        k_{y1}^2
        +
        k_{z1}^2
        +
        k_{x2}^2
        +
        k_{y2}^2
        +
        k_{z2}^2
    }
}

The states are particles in a lattice in a 6-dimensional $k$-space

Now, each quantum state is in a box of volume 
\equations{
    V_{state}
    =
    \left(
        \frac{\pi^3}{V}
    \right)^2
}

So all states with energy $\epsilon < U$ occupies a volume of 
$\left( 1/8 \right)^2$ of the 6D hypersphere.

So our multiplicity 
$
    \Omega(\epsilon, \epsilon + \delta \epsilon)
$

Are all the states in between $\epsilon$ and $\epsilon + \delta \epsilon$ 
that occupy $1/2^6$ of the hypersphere.

\subsection{$N$ Particles}
All states with energy $\epsilon \leq U$ occupy a volume that's 
$1/2^{3N}$ of a $3N$ dimension hypersphere. So to find all the states, 
in a shell, it's the same volume partition, but with a shell. 

Ignore geometric factors and constants.
\equations{
    k \sim \sqrt{U} 
    \hp 
    (k \sim \sqrt{\epsilon} )
    \\
    N = 1 
    \hp 
    \Phi_1(U)
    \sim 
    V k^3 
    \sim 
    V U^{3/2}
    \\
    \Omega_1(U_1, U + \delta U)
    \sim 
    V k^2 dk 
    \sim 
    V \sqrt{U} dU 
    \\
    N = 2
    \hp 
    \Phi_2(U)
    \sim 
    V^2 k^6
    \sim 
    V^2 U^{3}
    \\
    \Omega_2(U_2, U + \delta U)
    \sim 
    V^N k^5 dk 
    \sim 
    V^2 U^2 dU 
    \\
    N = N
    \hp 
    \Phi_2(U)
    \sim 
    V^N k^{3N}
    \sim 
    V^N U^{3N / 2}
    \\
    \Omega_N(U_N, U + \delta U)
    \sim 
    V^N k^{3N - 1} dk 
    \sim 
    V^2 U^{(3N/2)-1} dU 
}

Because all the particles are identical, we 
divide $\Omega_N$ by $N!$ to make up for the different possible 
permutations.

\equations{
    \Omega_N(U, U + \delta U)
    =
    \frac{g_{3N}}{2^{3N} N!}
    \left(
        \frac{V}{\pi^3}
    \right)^N 
    \left(
        \frac{2m}{\hbar^2}
    \right)^{3N/2} 
    \frac{U^{(3N/2) - 1}}{2} 
    \delta U
}

$g_D$ is the solid angle of the hypershell in $N$ dimensions 

\equations{
    g_3 
    =
    4 \pi
}

Our multiplicity function can also be written as 

\equations{
    \Omega_N(U, U + \delta U)
    =
    \frac{g_{3N}}{2^{3N + 1} N!}
    \left(
        \frac{U}{\Delta \epsilon}
    \right)^{(3N/2) - 1}
    \frac{\delta U}{\Delta \epsilon}
    \hp 
    \Delta \epsilon 
    =
    \frac{\hbar^2 \pi^2}{2m V^{2/3}}
}

The entropy of this system can be written as 
\equations{
    S(U, U + \delta U, V)
    =
    k_B \ln(\Omega_N)
    \\
    =
    Nk_B \ln(V)
    +
    \left(
        \frac{3N}{2}
        -
        1
    \right) k_B \ln(U)
    +
    k_B \ln(f(N)) 
    +
    \ldots
    \\
    \approx
    Nk_B \ln(V)
    +
    \left(
        \frac{3}{2}
    \right) N k_B \ln(U)
    +
    k_B \ln(f(N)) 
}

\section{MISSED LECTURE}
\section{Discussion}
I microcanonical ensemble is when $U$ and $N$ are fixed, and it 
is ideal but not realistic.  

\equations{
    \Omega(U, N)
    =
    \sum_i 
    \delta(U - U_L)
    \delta(N - N_L)
}

A canonical ensemble is an ensemble connected to a thermostat. $N_S$ 
is fixed
\equations{
    U_0 = U_R + E 
}

The probability of a state having a certain energy can be written as 
\equations{
    P(E_i)
    =
    \frac{\Omega_{R + S}(E_i)}{\sum_j \Omega_{R+S}(E_j)}
}

The main way to do things is find $z$ such that 
\equations{
    z 
    =
    \sum_i e^{- \beta E_i}
}

And then take the derivatives of $z$ 
\equations{
    U 
    =
    -\frac{\del z}{\del \beta}
}

\section{Equipartition Theorem}
Imagine a gas coupled to a reservoir. 

\equations{
    \epsilon_1
    =
    \frac{\hbar^2}{2m}
    (k_x^2 - k_y^2 + k_z^2)
    =
    \frac{\hbar^2 \pi^2}{2m}
    (\frac{n_x^2}{L_x^2} - \frac{n_y^2}{L_y^2} + \frac{n_z^2}{L_z^2})
    \\
    z_1 
    =
    \sum e^{-\frac{\epsilon_1}{k_B T}}
    =
    \sum e^{-\frac{
        \left(
            \frac{\hbar^2 \pi^2}{2m}
            (\frac{n_x^2}{L_x^2} - \frac{n_y^2}{L_y^2} + \frac{n_z^2}{L_z^2})
        \right)
    }{k_B T}}
    \\
    z_1(t)
    = 
    \left(
        \sum_{nx=1}^{\infty}
        e^{
            - \frac{\hbar^2 \pi^2}{2m}
            \frac{n_x^2}{L_x^2 k_B T}
        }
    \right)
    \left(
        \sum_{ny=1}^{\infty}
        e^{
            - \frac{\hbar^2 \pi^2}{2m}
            \frac{n_y^2}{L_y^2 k_B T}
        }
    \right)
    \left(
        \sum_{nz=1}^{\infty}
        e^{
            - \frac{\hbar^2 \pi^2}{2m}
            \frac{n_z^2}{L_z^2 k_B T}
        }
    \right)
}

Let's consider just one of these parts 
\equations{
    \sum_{nx=1}^{\infty}
    e^{
        - \frac{\hbar^2 \pi^2}{2m}
        \frac{n_x^2}{L_x^2 k_B T}
    }
    =
    \sum_{nx=1}^{\infty}
    e^{
        -\alpha_x n_x^2
    }
}

This works for low temperatures. For high temperatures (many $n \in N$), 
we get a different answer. Large $T$ means $k_B T >> \hbar^2 \pi^2 / (2m L_x^2)$
for $x, y, z$. It can also be written as 
$k_B T >> \Delta \epsilon = \hbar^2 \pi^2 / (2m V^{2/3})$

\equations{
    \sum_{nx=1}^{\infty}
    e^{
        -\alpha_x n_x^2
    }
    \approx 
    \int^\infty_{0}
    \, dn_x \, 
    e^{- \alpha_x n_x^2}
    =
    \frac{1}{2}
    \sqrt{\frac{\pi}{\alpha_x}}
    \\
    z(\textrm{high $T$})
    \approx 
    \frac{\pi^{3/2}}{2^3 (\alpha_x \alpha_y \alpha_z)^{1/2}}
    L_x L_y L_z 
    =
    \left(
        \frac{m k_B T}{2 \pi \hbar^2}
    \right)^{3/2}
    * V
}

This looks similar to a density times a volume.

Let $k_B T = 1/\beta$. For $N$ particles in a box at the high temperature limit, we can describe 
it with

\equations{
    z_{tot}
    =
    \frac{z_1^N}{N!}
    \hp
    U 
    =
    -
    \frac{\del}{\del \beta} \ln(z_{tot})
    =
    -\frac{\del}{\del \beta} \ln(z_{1}^N)
    =
    -N\frac{\del}{\del \beta} \ln(z_{1})
    \\
    =
    - N \frac{\del}{\del \beta}
    \left(
        \ln(\beta^{-3/2})
        +
        f(\textrm{not } \beta)
    \right)
    \\
    U 
    =
    \frac{3}{2}
    N \frac{1}{\beta}
    =
    \frac{3}{2}
    N k_B T 
    \hp 
    k_B T 
    >> 
    \Delta \epsilon
}

Now we can go back to our density 

\equations{
    z_1 
    =
    \left(
        \frac{m k_B T}{2 \pi \hbar^2}
    \right)^{3/2}
    * V
    \hp 
    n_{\varnothing}
    \left(
        \frac{m k_B T}{2 \pi \hbar^2}
    \right)^{3/2}
    \hp
    z_1(T)
    =
    n_{\varnothing}(T)
    * V
}

$n_{\varnothing}$ is known as the quantum density. $\lambda = h/p$ is the 
De Broglie Wavelength and $p = \hbar / k$. For an ideal gas, our energy 
is described as
\equations{
    \langle \epsilon \rangle
    \frac{3}{2} k_B T 
    =
    \frac{\hbar^2 k^2}{2m}
    =
    \frac{p^2}{2m}
    \hp \hp 
    h 
    =
    2 \pi \hbar
}

And the expected value of $\lambda$ is 
\equations{
    \langle \lambda \rangle 
    =
    \frac{h}{\sqrt{2 m \langle \epsilon \rangle}}
    =
    \sqrt{
        \frac{4 \pi^2 \hbar^2}{3 m k_B T}
    }
}

So with this, we can describe $n_\varnothing$ 
\equations{
    n_{\varnothing}
    =
    \frac{const O(1)}{\langle \lambda \rangle^3}
}

If you have a density of particles on the order of the quantum density, then 
you cannot ignore quantum effects.

\subsection{Example}
Consider Helium at $300K$. 

\equations{
    n_{\varnothing}
    \approx 
    0.8 * 10^25 \frac{1}{cm^3}
    =
    \frac{1}{0.5 \r{A}^3}
    \\
    n_{He}
    =
    2.5 * 10^{19}
}
So we are in the classical limit.

\chapter{Equipartition Theorem}
A diatomic ideal gas can not only be translated around, but also rotated to 
change is potential state. The energy of this atom can be written as 
\equations{
    \epsilon_{mol}
    =
    \epsilon_{trans}
    +
    \epsilon_{vib}
    +
    \epsilon_{rot}
    \\
    z_{mol}
    =
    \sum_{all states}
    e^{- \beta \epsilon_{mol}}
    =
    \left(
        \sum_{\epsilon_{trans}}
        e^{- \beta \epsilon_{trans}}
    \right)
    \left(
        \sum_{\epsilon_{vib}}
        e^{- \beta \epsilon_{vib}}
    \right)
    \left(
        \sum_{\epsilon_{rot}}
        e^{- \beta \epsilon_{rot}}
    \right)
    \\
    \epsilon_{transl}
    =
    \frac{\hbar^2 k^2}{2m}
    \hp 
    z_{transl}
    =
    n_{\varnothing}(T) \cdot V 
    \hp 
    U_{transl}
    =
    \frac{3}{2} N k_B T
}

We can figure out rotational energy in quantum mechanics in the 
form of angular momentum 
\equations{
    J 
    =
    |\vec J| 
    =
    \sqrt{j(j+1) \hbar} \hp j \in \mathbb{N} 
    \\
    J_z 
    =
    m_z \hbar 
    \hp 
    m_z = -j, -j+1, \ldots j-1, j
    \\
    \epsilon_{rot}
    =
    \frac{|\vec J|^2}{2 I}
    =
    \frac{j(j+1) \hbar^2}{2 I}
    = 
    \epsilon * j(j+1)
    \hp 
    j \in \mathbb{N}
    \\
    z_{rot}
    =
    \sum_{J, J_z}
    e^{- \beta \epsilon_{rot}}
    =
    \sum^{\infty}_{j-0}
    (2j + 1) e^{\frac{\epsilon j(j+1)}{k_B T}}
}

For low temperature $T$, then 
\equations{
    z_{rot}
    =
    1 
    +
    3 e^{-2 \beta \epsilon}
    +
    5 e^{-6 \beta \epsilon}
    +
    7 e^{-12 \beta \epsilon}
    +
    \ldots
}

The $\ldots$ corresponds to excited states that are "frozen out", 
or they don't contribute to the physics.

For the high temperature limit, we can make some approximations 
\equations{
    z_{rot}
    \approx
    \int^{\infty}_{0} \, dj \, 
    (2j + 1) e^{\frac{\epsilon j(j+1)}{k_B T}}
    \hp 
    x = \beta \epsilon(j(j+1))
    dx = \beta \epsilon(2j + 1) dj
    \\
    z_{rot}
    =
    \frac{1}{\beta \epsilon}
    \int^{\infty}_{0} \, dj \, 
    e^{-x}
    =
    \frac{1}{\beta \epsilon}
    \\
    U_{rot}
    =
    -\frac{\del}{\del \beta}
    \ln(z_{rot})
    =
    \frac{1}{\beta}
    =
    k_B T
}


Now we can calculate the vibrational degrees of freedom 
\equations{
    \epsilon_{vib}
    =
    \hbar \omega (n + \frac{1}{2})
    \hp 
    n \in \mathbb{N}
    \\
    z_{vib}
    \sum_n 
    e^{- \beta \epsilon_{vib}}
    =
    e^{- \frac{1}{2} \beta \hbar \omega}
    \sum_{n = 0}^{\infty}
    e^{- \beta \hbar \omega * n}
}

We can use a geometric series 
\equations{
    \sum x^n = \frac{1}{1-x}
    \hp 
    e^{-\beta \hbar \omega}
    =
    x
    \\
    z_{vib}
    =
    \frac{e^{- \frac{1}{2} \beta \hbar \omega}}{{1 - e^{- \beta \hbar \omega}}}
    \\
    U_{vib}
    =
    \langle \epsilon_{vib} \rangle 
    =
    - \frac{\del}{\del \beta}
    \ln(z_{vib})
    \\
    =
    + \frac{\del}{\del \beta}
    \left(
        \frac{1}{2} \hbar \omega \beta 
        +
        \ln(1 - e^{- \beta \hbar \omega})
    \right)
    \\
    =
    \hbar \omega 
    \left(
        \frac{1}{e^{\beta \hbar \omega} - 1} 
        +
        \frac{1}{2}
    \right)
    =
    \hbar \omega 
    \left(
        \langle n \rangle 
        +
        \frac{1}{2}
    \right)
}

We can also find the average 
\equations{
    \langle n \rangle 
    =
    \frac{1}{e^{\beta \hbar \omega} - 1}
}

In the low temperature limit $k_B T << \hbar \omega$, we get 
\equations{
    \frac{1}{e^{\beta \hbar \omega} + 1}
    \approx 
    e^{- \beta \hbar \omega}
    \rightarrow 
    U_{vib}
    \approx 
    \hbar
    \left(
        e^{-\beta \hbar \omega}
        +
        \frac{1}{2}
    \right)
}

In the high temperature limit, we can take a taylor expansion to get 
\equations{
    \langle n \rangle 
    =
    \frac{1}{e^{\beta \hbar \omega} - 1}
    \approx 
    \frac{1}{1 + \beta \hbar \omega - 1}
    =
    \frac{1}{\beta \hbar \omega}
    \\
    U_{vib}
    =
    \hbar \omega(\langle n \rangle + \frac{1}{2})
    \approx
    \hbar \omega
    \left(
        \frac{k_B T}{\hbar \omega} + \frac{1}{2}
    \right)
    =
    k_B T + \frac{\hbar \omega}{2}
    \approx 
    k_B T
}

\section{Heat Capacity}
Heat capacity is defined as 
\equations{
    C_V 
    =
    \left(
        \frac{\del U}{\del T}
    \right)_V
    \hp 
    U_{tot}
    =
    U_{transl}
    +
    U_{rot}
    +
    U_{vib}
    \\
    C_{V}
    =
    C_V^{transl}
    +
    C_V^{rot}
    +
    C_V^{vib}
}

For Helium the heat capacity over temperature is a step function. 

Each of the different degrees of freedom become relevant at different times, 
so the specific heat capacity changes depending on the temperature. 

\section{Equipartition Theorem}
This theorem helps us figure out the high temperature limit of systems 
very easily. 

\equations{
    U 
    \propto 
    \alpha 
    \cdot 
    N k_B T
    \hp 
    k_B T >> \Delta \epsilon
}

This can be found with 
\equations{
    z 
    =
    \sum_i 
    e^{\epsilon_i / k_B T}
    \\
    A 
    \int \, dq_1 
    \int \, dq_2
    \int \, dq_3 
    \ldots
    e^{-\beta \epsilon(q_1, q_2, q_3, \ldots, q_N)}
    \\
    p(\epsilon_i)
    =
    \frac{e^{- \beta \epsilon_i}}{z}
    \\
    \langle \epsilon_{qi} \rangle 
    =
    \langle a_i q_i \rangle 
    \\
    \langle \epsilon_{qi} \rangle 
    =
    \frac{1}{2} k_B T
}

\section{Discussion}
Consider a system with a bath $R$ connected to a system $D$ with parameters 
\equations{
    U_R 
    =
    U_0 - \epsilon 
    \hp 
    T_R = T_D = fixed 
}

We can try to find the extrema in the systme $S$ 
\equations{
    dS_{R + D}
    =
    0 
    =
    dS_R + dS_D 
    \\
    \frac{dU_R}{T_R}
    +
    dS_D 
    =
    0
    \rightarrow 
    dU_R 
    =
    d(U_0 - U_D)
    =
    -dU_D
    \\
    0
    =
    \frac{-dU_D}{T}
    + dS_D 
    \rightarrow 
    \frac{-1}{T} 
    d (U_D - TS_D)
    =
    0
    \\
    F 
    \equiv 
    U - TS
}
That is an extremum of $S$ in the canonical ensemble. It is also 
known as the Helmholz Free energy. 

\equations{
    \left(
        \frac{\del F}{\del T}
    \right)_V 
    =
    -S 
    =
    \frac{F - U}{T} 
    \\
    \tilde F = 
    - k_B T \ln(z)
    \Rightarrow 
    |\frac{\del \tilde F }{\del T}| 
    =
    -k_B \ln(z) 
    -
    k_B T \frac{\del}{\del T}
    \ln(z) 
    \\
    =
    -k_B \ln(z)
    +
    \frac{1}{T}
    \frac{\del}{\del B } \ln(z)
}

We are also theoretically able to derive 
\equations{
    \frac{\del \tilde F}{\del T}
    =
    \frac{\tilde F - U}{T }
    \\
    \tilde F(T=0)
    =
    - \lim_{T \to 0}
    k_B T \ln(z) 
    =
    - \lim_{T \to 0}
    k_B T 
    \ln(\sum_i e^{\epsilon_i / k_B T})
    \\
    z(T = 0)
    =
    e^{-\epsilon_0 / k_B T}
    +
    e^{-\epsilon_1 / k_B T}
    \approx 
    e^{-\epsilon_0 / k_B T}
    \\
    \tilde F(T=0)
    =
    - \lim_{T \to 0}
    k_B T 
    \ln(e^{\epsilon_0 / k_B T})
    =
    \epsilon_0
    \\
    F(T=0)
    =
    U(T=0)
    \\
    - \frac{\del}{\del B}
    \ln(z(T \to 0))
    =
    - \frac{\del}{\del B}
    \ln(e^{-B \epsilon_0})
    =
    \epsilon_0 
    \\
    F 
    =
    -k_B \ln(z)
}

\section{Equipartition Theorem}
\equations{
    \langle \epsilon_{q1} \rangle 
    =
    \langle 
        a_1 q_1^2 
    \rangle
    =
    \frac{1}{2} 
    k_B T 
}

This is true for each degree of freedom at the high temperature limit.
This means that for a particle with 3 degrees of freedom, then 
the expected value of the energy is 
\equations{
    \langle U \rangle 
    =
    \frac{3}{2} k_B T
}

We have a function $z(T)$ that can be written as 
\equations{
    z(T) 
    =
    A 
    \int^{\infty}_{-\infty}
    dq_1
    * 
    \ldots 
    *
    \int^{\infty}_{-\infty}
    dq_N
    e^{- \beta \epsilon (q_1, \ldots, q_N)}
    \\
    \epsilon = 
    a_1 q_1^2 
    +
    a_2 q_2^2 
    +
    \ldots 
    +
    a_N q_N^2 
}

And the probability of getting a certain energy is 
\equations{
    p(\epsilon_1)
    =
    \frac{A dq_1 \ldots dq_N e^{-\beta \epsilon(q_1, \ldots , q_N)}}
    {
    A \int^{\infty}_{-\infty} 
    dq_1 \ldots 
    \int^{\infty}_{-\infty} 
    dq_N e^{-\beta \epsilon(q_1, \ldots , q_N)}
    }
    \\
    \langle \epsilon \rangle
    =
    \frac{\int^{\infty}_{-\infty} dq_1 
    \ldots 
    \int^{\infty}_{-\infty} dq_N a_1 q_1^2 
    e^{-\beta \epsilon(q_1, \ldots , q_N)}}
    {
    \int^{\infty}_{-\infty} 
    dq_1 \ldots 
    \int^{\infty}_{-\infty} 
    dq_N e^{-\beta \epsilon(q_1, \ldots , q_N)}
    }
    =
    \frac{\int^{\infty}_{-\infty} dq_1 a_1 q_1^2 e^{- \beta \epsilon_{q1}}}
    {
        \int^{\infty}_{-\infty} dq_1  e^{- \beta \epsilon_{q1}}
    }
    \\
    \langle \epsilon_1 \rangle 
    =
    \frac{
        \int^{\infty}_{-\infty} dq_1 \epsilon_{q1} e^{- \beta \epsilon_{q1}}
    }
    {
        \int^{\infty}_{-\infty} dq_1  e^{- \beta \epsilon_{q1}}
    }
    =
    - \frac{\del}{\del \beta}
    \ln(
        \int^{\infty}_{-\infty} dq_1  e^{- \beta a_1 q_1^2}
    )
    =
    - \frac{\del}{\del \beta}
    \ln(
        \sqrt{\frac{\pi}{\beta a_1}}
    )
    \\
    \langle \epsilon_1 \rangle 
    =
    - \frac{\del}{\del \beta}
    \left(
    \ln(
        \frac{1}{\sqrt{\beta}}
    )
    \right)
    +
    \textrm{f(not $\beta$)}
    \\
    \langle \epsilon_1 \rangle 
    =
    \frac{1}{2 \beta}
    =
    \frac{1}{2} k_B T
}

This works in the high temperature limit for systems of quadratic 
energy $\epsilon = q_1^2 + \ldots$.

An ideal gas in 3 dimensions has an expected total system energy of 
\equations{
    \langle \epsilon \rangle
    =
    N \frac{3}{2} k_B T
}

A 1d harmonic oscillator has energy 
\equations{
    \epsilon 
    =
    \frac{p_1^2}{2m}
    +
    \frac{1}{2}
    \omega x^2 
    \Rightarrow 
    \langle \epsilon \rangle 
    =
    2 
    \frac{1}{2} k_B T
    =
    k_B T
}

For $N$ oscillators, the expected energy is just $N k_B T$.

\subsection{Counterexample}
A paramagnet has energy of the form 
\equations{
    U 
    =
    -N \mu B 
    \tanh(\frac{\mu B}{k_B T})
}
So for the high temperature limit 
\equations{
    U 
    =
    -\frac{N \mu^2 B^2 }{k_B T}
    \neq 
    N k_B T
}

Because the energy is not in the form $k_B T$, we 
\textbf{cannot} use the equipartition theorem.

\subsection{Maxwell Velocity Distribution}
The kinetic energy can be written as 
\equations{
    \epsilon 
    =
    \frac{1}{2} mv^2 
    =
    \frac{1}{2} m 
    \left(
        v_x^2 
        + 
        v_y^2 
        + 
        v_z^2 
    \right)
}

This is equivalent to the energy having mutliple quadratic 
degrees of freedom. The probability of having a specific velocity 
(certain components), we its 
\equations{
    P(\vec v)
    =
    \frac{dv_x dv_y dv_z e^{-\beta \epsilon}}
    {
        \int^{\infty}_{-\infty}
        dv_x 
        \int^{\infty}_{-\infty}
        dv_y
        \int^{\infty}_{-\infty}
        dv_y
        e^{-\beta \epsilon}
    }
}

If we switch from cartesian coordinates to spherical coordinates, 
we get
\equations{
    P(\vec v)
    =
    \frac{
        v^2 dv \sin(\theta) \, d \theta \,  d \varphi \, e^{-\beta \epsilon}
    }
    {
        \int^{\infty}_{0}
        v^2 dv 
        \int^{2 \pi}_{0}
        d \varphi
        \int^{\pi}_{0}
        \sin(\theta) \, d \theta
        e^{-\beta \epsilon}
    }
    =
    \frac{
        v^2 dv e^{-\beta \frac{mv^2}{2}}
    }
    {
        \int^{\infty}_{0}
        v^2 dv 
        e^{-\beta \frac{mv^2}{2}}
    }
    \frac{4 \pi}{4 \pi}
    \\
    \int^{\infty}_{0}
    v^2 e^{-\alpha v^2} \, dv 
    =
    - \frac{\del}{\del \alpha}
    \int^{\infty}_{0}
    e^{-\alpha v^2} \, dv 
    =
    - \frac{\del}{\del \alpha}
    \frac{1}{2}
    \sqrt{\frac{\pi}{\alpha}}
    =
    \frac{1}{4 \pi}
    \left(
        \frac{2 \pi k_B T}{m}
    \right)^{3/2}
    \\
    f(v) dv
    =
    4 \pi 
    \left(
        \frac{m}{2 \pi k_B T}
    \right)^{3/2}
    v^2 
    e^{- \frac{mv^2}{2 k_B T}}
    dv 
}
So we get some properties 

\equations{
    v_{peak}
    =
    \sqrt{\frac{2 k_B T}{m}}
    \hp
    \left(
        \frac{\del f}{\del v}
    \right)
    =0
    \\
    \langle v \rangle 
    =
    \int^{\infty}_{0} \, dv \, 
    f(v) v 
    =
    \sqrt{\frac{8 k_B T}{\pi m}}
    \hp
    \langle v^2 \rangle 
    =
    \int^{\infty}_{0} \, dv \, 
    f(v) v^2
    =
    \frac{3 k_B T}{m}
    \\
    \langle \frac{1}{2} m v^2 \rangle 
    =
    \frac{3}{2} k_B T 
}
Equipartition theorem does hold.

\section{Helmholz Free Energy}
Consider a system $\phi$ with fixed $U, N$. 
It is a microcanonical ensemble, so $U_S$ is at a maximum in 
equilibrium $(dS_\phi = 0)$. 

In a canonical ensemble of fixed $N$, our system is connected to a 
larger bath, 
so $U_0$ is constant, and $T = T_0 = T_\phi$ is fixed. 
$S_{R+F}$ is at an extremum $(d S_{R + F} = 0)$

What qualities of $\phi$ are at an extremeum?
\equations{
    dS(U, V)
    =
    \left(
        \frac{\del S}{\del U}
    \right)_V 
    dU 
    +
    \left(
        \frac{\del S}{\del V}
    \right)_U
    dV
    =
    \frac{1}{T}
    dU 
    +
    \frac{p}{T}
    dV
    \\
    dS_{R + \phi}
    =
    0
    =
    dS_R + dS_\phi 
    =
    \frac{dU_R}{T}
    +
    dS_{\phi}
    \\
    U_0 = U_R + U_{\phi}
    \hp 
    dU_R = -dU_{\phi}
    \\
    dS_{R + \phi}
    =
    -
    \frac{dU_{\phi}}{T}
    +
    dS_{\phi}
    =
    -
    \frac{1}{T}
    \,
    d 
    \left(
        U_{\phi}
        -
        T S_\phi
    \right)
    =
    0
}

So the Helmholz Free energy is given by 
\equations{
    F 
    U_{\phi}
    -
    T S_\phi
    \\
    0
    =
    dS_{R + \phi}
    =
    - \frac{1}{T}
    d F
}
When the entropy is maximized (system is at equilibrium), then $F$ 
is minimized.

With the Helmholz Free Energy, we get 
\equations{
    F = U_\phi - TS_{\phi}
    \hp 
    dF =
    dU - T \, dS - S \, dT
    \hp
    dU 
    =
    T \, dS - p \, d V
    \\
    d F
    =
    - p \, dV - S \, dT 
    \Longrightarrow
    dF 
    =
    - \left(
        \frac{\del F}{\del V}
    \right) dV - 
    \left(
        \frac{\del F}{\del T} 
    \right)
    dT 
    \\
    \hp 
    p
    =
    - 
    \left(
        \frac{\del F}{\del V}
    \right)_T
    \hp 
    S 
    =
    -
    \left(
        \frac{\del F}{\del T}
    \right)_V
}
So now we have both momentum and entropy in terms of the 
Helmholz Free Energy. 

In a microcanonical ensemble, our equations are 
\equations{
    S_{\phi}
    =
    k_b \ln(\Omega_{\phi})
}

But in a canonical ensemble, we should be able to predict the free energy 
\equations{
    F 
    =
    - k_B T 
    \ln(z)
    \hp 
    z 
    =
    \sum_{i=0}^{\infty}
    e^{- \beta \epsilon_i}
}

We can find this with 
\equations{
    \left(
        \frac{\del F}{\del T}
    \right)_V 
    =
    -S 
    =
    \frac{F - U}{T}
}
Use an ansatz 
\equations{
    \tilde F 
    =
    -k_B T \ln(z)
}
And you see that $\tilde F$ fulfills the differential equation.

To make sure that there is no constant offset, you can check 
the term at $T=0$ 
\equations{
    \tilde F(T =0)
    =
    F(T=0)
}

\section{2 Energy Levels}
$\epsilon$ is either $\Delta$ or $0$. What is the entropy $S(T)$? 
What is $S$ for both $T \to 0$ and $T \to \infty$? 

First, start with a partition function $z$ 
\equations{
    z 
    =
    \sum e^{- \beta \epsilon}
    =
    e^{- \beta \Delta}
    +
    e^{- \beta 0}
    =
    1 +
    e^{- \beta \Delta}
}
Then find the free energy 
\equations{
    F 
    =
    -k_B T 
    \ln(z)
    =
    -k_B T 
    \ln( 1 + e^{- \beta \Delta})
}
And then find the entropy 
\equations{
    S =
    - \left(
        \frac{\del F}{\del T}
    \right)_V 
    =
    k_B
    \ln( 1 + e^{- \Delta / k_B T})
    +
    \frac{\Delta}{T}
    \frac{
    e^{- \Delta / k_B T}
    }
    {
    1+
    e^{- \Delta / k_B T}
    }
}
Then you can check $S$ for small and large $T$ 
\equations{
    S(T \to 0)
    =
    k_B \ln(1)
    =
    0
    \hp 
    S(T \to \infty)
    =
    k_B \ln(2)
}
Those both look good.

\section{Summary}
\subsection{Microcanonical Ensemble}
In a microcanonical ensemble, the system is closed, so the 
fixed variables are $N, V, U$. 

The extremum is when $S(U, V)$ is at a maximum

The system is measured with the multiplicity 
\equations{
    \Omega 
    =
    \sum_{\textrm{all states with energy $U$}}
    1
}

And entropy is defined as 
\equations{
    S 
    =
    k_B \ln(\Omega)
    \\
    dS 
    =
    \left(
        \frac{\del S}{\del U}
    \right)_V 
    dU 
    +
    \left(
        \frac{\del S}{\del V}
    \right)_U
    dV
    \\
    dS 
    =
    \frac{1}{T}
    dU 
    +
    \ldots
    \ldots
    \ldots
    \ldots
    \ldots
    \ldots
    \ldots
    \ldots
    \ldots
}

\subsection{Canonical Ensemble}
In a canonical ensemble, the system is connected to a bath.
The fixed variables are $N, V, T$

The extremum is when $F(T, V)$ is at a minimum

The system is defined with a partition function 
\equations{
    z 
    =
    \sum_{\textrm{all states with temperature $T$}} e^{-\epsilon / k_B T}
}

And free energy is defined as 
\equations{
    F 
    =
    - k_B T \ln(z)
    \\
    dF 
    =
    \left(
        \frac{\del F}{\del U}
    \right)_V 
    dU 
    +
    \left(
        \frac{\del F}{\del V}
    \right)_U
    dV
}

\chapter{Heat and Work}
Recall the thermodynamic identities 
\equations{
    dU 
    =
    T \, dS - 
    p \, dV
    \hp 
    dQ 
    =
    T \, dS 
    \hp 
    dW_{on}
    =
    - p \, dV 
    \\
    dU 
    =
    dQ 
    +
    dW_{on}
}
All types of work can be \textbf{completely} interconverted. 
Heat can be \textbf{partially} interconverted, but not completely.
This is because of the 2nd law of thermodynamics.

\section{Heat Engine}
Consider a device that can completely convert heat into work 
\equations{
    T_h \longrightarrow Q_H
}
Could you then use that heat to lift up a mass?
Consider the 1st law of thermodynamics (conservation of energy). 
This requires that, for our engine, 
\equations{
    Q_H = W 
}
What about entropy? We are working in a closed system with a reservoir 
and an engine and a mass 
\equations{
    \Delta S_{tot}
    =
    \Delta S_{res}
    +
    \Delta S_{eng}
    +
    \Delta S_{mass}
    \\
    \Delta S_{res}
    =
    - \frac{Q_H}{T_H}
    < 0
    \hp 
    \Delta S_{eng}
    =
    0 
    \hp 
    \Delta S_{mass}
    =
    0
    \hp
    \Delta S_{tot}
    =
    - \frac{Q_H}{T_H}
}
This violates the 2nd law of thermodynamics, which means our engine 
absolutely cannot be perfectly efficient. However, if we imagine the 
opposite (work to heat), then we see that does not violate the 2nd law 
of thermodynamics. 

Because of the first law, we get that 
\equations{
    Q_H 
    =
    Q_C + W
}
Because of the 2nd law, we get 
\equations{
    \Delta S_{tot} \geq 
    \hp 
    \Delta S_{tot}
    =
    \Delta S_{H, res}
    \Delta S_{C, res}
    =
    - \frac{Q_H}{T_H}
    +
    \frac{Q_C}{T_C}
    \geq 0
}

\section{Efficiency}
The efficiency will be denoted as $\eta$. 
\equations{
    \eta 
    =
    \frac{W}{Q_H}
    =
    \frac{Q_H - Q_C}{Q_H}
    =
    1
    -
    \frac{Q_C}{Q_H}
}

What is the maximum possible efficiency? 
\equations{
    \Delta S_{tot} = 0 
    \Rightarrow 
    \Delta S_{tot}
    =
    - \frac{Q_H}{T_H}
    +
    \frac{Q_C}{T_C}
    = 0
    \hp 
    \frac{Q_C}{Q_H}
    =
    \frac{T_C}{T_H}
}

The carnot effiency is known as the maximum possible attainable efficiency, 
and that is done with 
\equations{
    \eta_{max}
    =
    1
    -
    \frac{Q_C}{Q_H}
    =
    1 - \frac{T_C}{T_H}
}

In practice, this does not happen. 
\equations{
    \Delta S_{tot}
    > 0 
    \hp 
    \Delta S_{H, res}
    =
    - \frac{Q_H}{T_H}
    \hp 
    \Delta S_{C, res}
    =
    \frac{Q_C}{T_C}
    +
    \Delta S_{i, res}
    \\
    \Delta S_{tot}
    =
    -
    \frac{Q_H}{T_H}
    +
    \frac{Q_C}{T_C}
    > 0
    \Rightarrow
    \frac{Q_C}{Q_H}
    >
    \frac{T_C}{T_H}
    \\
    \eta 
    =
    1
    -
    \frac{Q_C}{Q_H}
    <
    1
    -
    \frac{T_C}{T_H}
    =
    \eta_{C}
}

\subsection{Types of Efficiency Losses}
\begin{itemize}
    \item
    Heat bypass: 
    Heat moves from one system to another without being used for work 
    \item
    Thermal Resistance: 
    There's a gradient in temperature in your system
    \item
    Friction Loss: 
    Work is turned to heat 
    \item
    Irreversible gas expansion:
    This type of expansion requires more work to be undone so it 
    is thermodynamically unfavorable.
\end{itemize}

All of these efficiency losses are irreversible, which means that 
one $\Delta S_{loss} > 0$, you cannot remove it.

\section{Carnot Cycle}
Consider a cycle with efficiency 
\equations{
    \eta_C 
    =
    1 - \frac{T_C}{T_H}
}
A working substance (gas) can absorb/expel heat and turn it into work 
(move a piston). 
An isotropic/adiabatic process is a process such that entropy is fixed.

\begin{center}
    heat 
    $\to$
    expand
    $\to$
    contract
    $\to$
    cool down
\end{center}
Because the piston returns to its original state and the gas cools 
down again, the total energy for 1 cycle is 
\equations{
    \oint dU 
    =
    0
    =
    \oint 
    dQ 
    +
    \oint 
    dW 
}
We can solve for each individual step of the Carnot cycle 
\equations{
    \oint dQ 
    =
    \oint dQ_1
    +
    \oint dQ_2 
    \oint dQ_3
    \oint dQ_4 
    =
    Q_H 
    +
    0 
    +
    -Q_H 
    +
    0
    =
    0
    \\
    \oint T \, dS 
    =
    T_H(S_2 - S_1)
    + 0 +
    T_C(S_4 - S_3)
    + 0
    \Rightarrow 
    \\
    \oint dU 
    =
    0
    =
    (T_H - T_C) (S_2 - S_1)
    -
    \oint p \, dV
    \\
    W_{engine}
    =
    \oint p \, dV
    =
    (T_H - T_C) (S_2 - S_1)
}

All these number returns the maximum possible efficiency, but that is not 
practical or realistic. This is because reversible heating and cooling 
processes are too slow for real-world applications.

\subsection{Realizing Carnot Cycle}
for a gas, we assume ideal behavior 
\equations{
    pV 
    =
    n k_B T 
}
There's something on the board called Sackur-Tetrode 
\equations{
    S 
    =
    N k_B 
    \left(
        \ln(\frac{n_Q V}{N})
        +
        \frac{3}{2}
    \right)
}

\subsection{Isothermal Segments}
\equations{
    pV 
    =
    const 
}

\subsection{Isotropic Segments}
Knowing isotropic behavior will be useful for the homework 

\equations{
    S 
    =
    \const 
    =
    N k_B 
    \left(
    \ln(T^{3/2})
    +
    \ln(V)
    \right)
    +
    \const
    \\
    T^{3/2} * V 
    =
    \const
    \hp 
    T 
    =
    \frac{pV}{N k_B}
    \rightarrow 
    (pV)^{3/2}
    * V 
    =
    p^{3/2} V^{5/2}
    =
    \const
    \\
    p V^{\gamma}
    =
    \const 
    \hp 
    \gamma 
    =
    \frac{5}{3}
}
$\gamma = 5/3$ for a monatomic gas, $7/5$ for a diatomic gas, etc etc. 
\equations{
    \gamma 
    =
    1 +
    \frac{2}{f}
}
So the entire process that be written in terms of changes in pressure 
and volume 
\begin{center}
    $\underset{1}
    \rightarrow$
    isothermal
    $\underset{2}
    \rightarrow$
    adiabatic
    $\underset{3}
    \rightarrow$
    isothermal
    $\underset{4}
    \rightarrow$
    adiabatic
\end{center}

\subsection{Work Done}
\equations{
    \oint p \, dV 
    \Rightarrow 
    W 
    =
    (T_H - T_C)
    (S_2 - S_1)
}

\section{Irreversible Processes}
Consider a system with gases one just one side. Energy is constant 
and heat transferred is 0 and the work done is 0. Temperature is also constant. 
\equations{
    \Delta S 
    =
    N k_B 
    \ln(\frac{V_f}{V_i})
    > 0
}
Because the entropy change is positive, this process is irreversible in a 
closed system.
She said sackur tetrodi again and I have no idea what the fuck that means. 

\subsection{1st Law of Thermodynamics}
\equations{
    dU 
    =
    dQ 
    +
    dW_{on}
}

For an reversible process, $dQ = T dS$ 
\equations{
    dU 
    =
    T dS
    +
    dW_{on}
}

For an \textbf{irreversible} process, $T dS > dQ_{irrev}$ 
\equations{
    dW_{on}
    >
    dU 
    -
    T dS
}

For the irreversible case, the total work done on the gas is $W_{on} = 0$, 
but in the 
reversible case, the gas would have to push a piston to expand, so 
$W_{on} = \int p dV < 0$.

\section{Discussion}
Consider a system of conserved $U, N, B$ where $B$ is the magnetic field. 
\equations{
    dU 
    =
    \left(
        \frac{dU}{dS}
    \right)_{NB} dS 
    +
    \left(
        \frac{dU}{dN}
    \right)_{SB} dN 
    +
    \left(
        \frac{dU}{dB}
    \right)_{SN} dB
    =
    T dS 
    +
    \mu dN 
    - 
    M dB
}
The Helmholz free energy is 
\equations{
    F 
    =
    U_S 
    -
    T S_S
    =
    -k_B T \ln(Z)
}
The maxwell relations are as following 
\equations{
    \frac{\del}{\del x}
    \left(
        \frac{\del}{\del y}
        f(x, y)
    \right)
    =
    \frac{\del}{\del y}
    \left(
        \frac{\del}{\del x}
        f(x, y)
    \right)
}

\chapter{Thermodynamic Potentials}
Consider the Helmholz Free Energy 
\equations{
    F 
    =
    U 
    -
    T S
}

In equilibrium, $F$ is minimized for fixed $V, T$. 
Consider starting with a reservoir and building a system out of energy 
just from that reservoir. Your system has an energy $U$, and you can transfer 
heat from the reservoir to the system. Let the system be $S$ and the 
reservoir be $R$.
\equations{
    S_{R + S}
    \geq 0
    \hp
    F_{S}
    =
    U_S 
    -
    T_S S_S
}

$F$ can be intuitively described as the work provided to $S$ at fixed $T$. 
It can also be the work extracted from $R$ to $S$.

\equations{
    \Delta F 
    =
    \Delta U 
    -
    T \Delta S
    \hp 
    \Delta U 
    =
    0 + W_{on}
    \rightarrow 
    \Delta F 
    =
    W_{on}
}
This is in ideal conditions, but in reality there are irreversible 
processes that happen. 
\equations{
    T \Delta S 
    > 
    \Delta Q 
    \Rightarrow 
    \Delta F 
    < 
    W_{on}
}

\section{Enthalpy and Gibbs Free Energy}
consider a system $s$ at fixed $p, T$. 
\equations{
    U_{S} + pV 
    =
    H 
    \hp 
    \textrm{(Enthalpy)}
    \\
    U_{S}
    +
    pV 
    -
    TS 
    =
    G 
    \hp 
    \textrm{(Gibbs Free Energy)}
}
$G$ is the work required from the system $s$ at fixed pressure. It is 
also the amount of work you can extract from the system $s$.

\equations{
    \Delta G 
    =
    \Delta U 
    + 
    p \Delta V 
    -
    T \Delta S 
    =
    0 + W_{on}
    + 
    p \Delta V 
    -
    T \Delta S 
    \Rightarrow 
    \\
    \Delta G 
    =
    W_{on}
    + 
    p \Delta V 
    \hp 
    \textrm{(ideal conditions)}
    \hp 
    \Delta G 
    \leq
    W_{on}
    + 
    p \Delta V 
}

In equilibrium, $G$ is at a minimum for fixed $p, T$. 

In equilibrium, $F$ is at a minimum for fixed $V, T$. 

\section{4 Thermodynamic Potentials}
\equations{
    U = U(S, V)
    \hp 
    dU 
    =
    T dS - p dV 
    =
    d Q + d W_{on}
    \\
    F 
    =
    F(T, V)
    =
    U - TS 
    \\ 
    dF 
    =
    dU - T dS - S dT 
    =
    T dS - p dV - T dS - S dT 
    =
    - p dV - S dT 
    \\
    H 
    =
    H(p, S)
    =
    U + pV
    \\
    dH 
    =
    dU + p dV + V dp
    =
    T dS - p dV + p dV + V dp
    =
    T dS + V dp
    \\
    G 
    =
    G(p, T)
    =
    U + pV - TS 
    =
    H - TS
    \\
    dG 
    =
    dU + p dV + V dp - T dS - S dT 
    \\ 
    = 
    T dS - p dV + p dV + V dp - T dS - S dT 
    = 
    V dp - S dT 
}
The energy, Helmholz Free Energy, Enthalpy, and Gibbs Free Energy 
are the most important potentials in statistical mechanics. 

\section{Maxwell Relations}
All 4 potentials depend on 2 sets of 2 variables. They all contain 
one of $\{ S, T \}$ and one of $\{ p, V \}$.

\equations{
    dU 
    =
    T dS 
    -
    p dV
    =
    \left(\frac{\del U}{\del S}\right)_{V}
    dS 
    +
    \left(\frac{dU}{dV}\right)_{S} 
    dV
    \\ 
    \left(\frac{\del U}{\del S}\right)_V
    =
    T 
    \hp
    -\left(\frac{dU}{dV}\right)_{S} 
    =
    p
    \\
    \left( \frac{\del T}{\del V}\right)_S 
    =
    \frac{\del^2 U}{\del S \del V}
    =
    -\left(\frac{dp}{dS}\right)_{V} 
    \hp 
    \textrm{First Maxwell Relation}
}

Now I can do the same thing with the other potentials 
\equations{
    dF 
    =
    - p dV - S dT 
    =
    \left(
        \frac{\del F}{\del V}
    \right)_T 
    dV 
    +
    \left(
        \frac{\del F}{\del T}
    \right)_V 
    dT 
    \Rightarrow 
    \\
    p 
    =
    -
    \left(
        \frac{\del F}{\del V}
    \right)_T 
    \hp 
    S 
    =
    -
    \left(
        \frac{\del F}{\del T}
    \right)_V 
    \\
    \left(
        \frac{\del S}{\del V}
    \right)_T 
    =
    -
    \frac{\del^2 F}{\del T \del V}
    =
    \left(
        \frac{\del p}{\del T}
    \right)_V 
}

Now Enthalpy 
\equations{
    dH
    =
    T dS + V dp
    = 
    \left( \frac{\del H}{\del S}\right)_T dS 
    +
    \left( \frac{\del H}{\del p}\right)_T dp 
    \Rightarrow 
    \\
    T 
    =
    \left( \frac{\del H}{\del S}\right)_T dS 
    \hp
    V 
    =
    \left( \frac{\del H}{\del p}\right)_T dp 
    \\
    \left( \frac{\del T}{\del p}\right)_T dp 
    =
    \frac{\del^2 H}{\del S \del T}
    =
    \left( \frac{\del V}{\del S}\right)_T dS 
}

And last but not least 
\equations{
    dG 
    =
    V dp - S dT 
    = 
    \left( \frac{\del G}{\del p}\right)_T dp 
    +
    \left( \frac{\del G}{\del T}\right)_p dT 
    \Rightarrow 
    \\
    V 
    =
    \left( \frac{\del G}{\del p}\right)_T
    \hp 
    S 
    =
    -
    \left( \frac{\del G}{\del T}\right)_p
    \\
    -\left( \frac{\del V}{\del T}\right)_p
    =
    \frac{\del^2 G}{\del T \del p}
    =
    \left( \frac{\del S}{\del p}\right)_T
}

All of these relations come from the fact that $S = S(U, V)$ and 
\equations{
    dS = \frac{1}{T} dU + \frac{p}{T} dV
    =
    \frac{\del S}{\del U}
    dU 
    +
    \frac{\del S}{\del V }
    dV 
    \\
    \frac{1}{T}
    =
    \left( \frac{\del S}{\del U}\right)_V 
    \hp 
    \frac{p}{T}
    =
    \left( \frac{\del S}{\del V}\right)_U
}

\subsection{Applications}
Let's say you measure a gas, and you have the temperature and volume of the gas. 
Let's say you want the entropy. $S(U, V) \to S(T, V)$.
\equations{
    dS 
    =
    \left( \frac{\del S}{\del T} \right)_V 
    dT 
    +
    \left( \frac{\del S}{\del V} \right)_T 
    dV 
    \\
    C_v 
    =
    \left( \frac{\del Q}{\del T} \right)_V
    = 
    T \left( \frac{\del S}{\del T} \right)_V
    \hp 
    \left( \frac{\del S}{\del V} \right)_T
    =
    \left( \frac{\del p}{\del T} \right)_V 
    \\
    dS 
    =
    \frac{C_v}{T}
    dT 
    +
    \left( \frac{\del p}{\del T} \right)_V 
    dV
}
Because we measured pressure, we can now get the entropy. 
The way we get the heat capacity is with the experimental 
data and a bunch of integrals that I don't really want to write down. 
\equations{
    S_{tot}
    =
    S(T_i, p_i)
    +
    \Delta S_1 
    + 
    \Delta S_2
    \\
    =
    S(T_i, p_i)
    +
    \int^{Tf}_{Ts} \, dT \, 
    \frac{C_v(T_0, V_i)}{T} 
    + 
    \int^{Vf}_{Vs} \, dV \, 
    \left( \frac{\del p}{\del T}\right)_V
}

\subsection{Monatomic Ideal Gas}
\equations{
    U = \frac{3}{2} N k_B T 
    \Rightarrow 
    C_v 
    =
    \frac{3}{2} N k_b
    =
    T
    \left( \frac{\del S}{\del T}\right)_V
    \hp 
    T dS 
    =
    dU 
    + 
    p dV 
    =
    dU 
    +
    0
    \\
    C_v 
    =
    \frac{3}{2} N k_b
    =
    \left(
    \frac{dU}{dT}
    \right)_V
    \\
    \Delta S_1
    =
    \int^{Tf}_{Ts} \, dT \, 
    \frac{C_v(T_0, V_i)}{T} 
    =
    \int^{Tf}_{Ts} \, dT \, 
    \frac{\frac{3}{2} N k_B}{T} 
    =
    \frac{3}{2} N k_B
    \ln(\frac{T_f}{T_i})
    \\
    \Delta S_2 
    =
    \int^{Vf}_{Vs} \, dV \, 
    \left( \frac{\del p}{\del T}\right)_V
    =
    \int^{Vf}_{Vs} \, dV \, 
    \left( \frac{\del }{\del T}\right)_V
    \frac{N k_B T }{V}
    =
    \int^{Vf}_{Vs} \, dV \, 
    \frac{N k_B }{V}
    \\
    =
    N k_B
    \ln(\frac{V_f}{V_i})
    \\
    \Delta S_{tot}
    =
    \frac{3}{2} N k_B
    \ln(\frac{T_f}{T_i})
    +
    N k_B
    \ln(\frac{V_f}{V_i})
}
This is consistent with the Sackur-Tetrode Equation for a monatomic ideal gas.
\equations{
    S(T, V)
    =
    N k_B \ln(T^{3/2} \cdot V ) + \const
    \rightarrow 
    \frac{3}{2} N k_B
    \ln(T)
    +
    N k_B
    \ln(V)
    + \const
}

\chapter{Chemical Potential}
Consider 2 systems that can exchanges particles with each other. 
$(U_1 + U_2)$, $(V_1 + V_2)$, $(N_1 + N_2)$ are all fixed.
At equilibrium, $S = (S_1 + S_2)$ is maximized, so $dS_1 + dS_2 = 0$.
\equations{
    dS_1
    =
    \left(
        \frac{\del S_1}{\del U_1}
    \right)_{V_1, N_1}
    \del U_1
    +
    \left(
        \frac{\del S_1}{\del V_1}
    \right)_{U_1, N_1}
    \del V_1
    +
    \left(
        \frac{\del S_1}{\del N_1}
    \right)_{U_1, V_1}
    \del N_1
}

Because everything is fixed, we can say that 
\equations{
    dU_1 = -dU_2 
    \hp
    dV_1 = -dV_2 
    \hp
    dN_1 = -dN_2 
    \\
    \left(
        \frac{\del S_1}{\del U_1}
        -
        \frac{\del S_2}{\del U_2}
    \right)_{V_1, N_1}
    \del U_1
    +
    \left(
        \frac{\del S_1}{\del V_1}
        -
        \frac{\del S_2}{\del V_2}
    \right)_{U_1, N_1}
    \del V_1
    +
    \left(
        \frac{\del S_1}{\del N_1}
        -
        \frac{\del S_2}{\del N_2}
    \right)_{U_1, V_1}
    \del N_1
    \\
    =
    \left(
        \frac{1}{T_1}
        - 
        \frac{1}{T_2}
    \right)_{V_1, N_1}
    \del U_1
    +
    \left(
        \frac{p_1}{T_1}
        - 
        \frac{p_2}{T_2}
    \right)_{U_1, N_1}
    \del V_1
    +
    \left(
        -
        \frac{\mu_1}{T_1}
        + 
        \frac{\mu_2}{T_2}
    \right)_{U_1, V_1}
    \del N_1
    \\
    = 0
    \Longrightarrow
    T_1 = T_2
    \hp 
    p_1 = p_2
    \hp 
    \mu_1 = \mu_2
}

This is the equilibrium condition for the 2 systems that can exchange particles. 
Because $N_1$ and $N_2$ are now non-constant, our equations change 
\equations{
    dS 
    =
    \frac{1}{T} dU 
    +
    \frac{p}{T} dV 
    - 
    \frac{\mu}{T} dN
    \hp 
    dU 
    =
    T dS 
    - 
    p dV 
    +
    \mu dN
    \\
    \mu  
    =
    \left(
        \frac{\del U}{\del N}
    \right)_{S, V}
}

\section{Chemical Potential}
A temperature difference derived heat flow. A pressure difference derives 
volume change. A chemical potential difference derives particle change.
\equations{
    \mu_1 > \mu_2 
    \Rightarrow
    \mu_{1}
    \to 
    \mu_{2}
    \\
    \mu 
    =
    -T \frac{\del S}{\del N}
    \rightarrow 
    \frac{\del S_1}{\del N_1}
    <
    \frac{\del S_2}{\del N_2}
    \\
    dS = dS_1 + dS_2 > 0
    \Rightarrow 
    \left(
        \frac{\del S_1}{\del N_1}
        -
        \frac{\del S_2}{\del N_2}
    \right)
    dN_1 
    > 0
    \Rightarrow
    dN_1 < 0
}

Chemical potentials moves from high to low. 

\subsection{With a Reservoir}
We have two systems $A_1$ and $A_2$ connected to a reservoir $R$ at fixed $T$.
The equilibrium state is when the Helmholz Free Energy is minimized. 
Let's assume that $dV = 0$ and $dT = 0$.
\equations{
    F = F_1(T, V_1, N_1) + F_2(T, V_2, N - N_1)
    \hp 
    dF 
    =
    dF_1
    +
    dF_2
    =
    0
    \\
    dF 
    =
    \left( 
        \frac{\del F_1}{\del N_1}
    \right)_{T, V_2}
    dN_1
    +
    \left( 
        \frac{\del F_2}{\del N_2}
    \right)_{T, V_2}
    dN_2
    =
    \left( 
        \frac{\del F_1}{\del N_1}
        -
        \frac{\del F_2}{\del N_2}
    \right)_{T, V_2}
    =
    0
    \\
    \frac{\del F_1}{\del N_1}
    =
    \frac{\del F_2}{\del N_2}
}



Let's propose that $\mu = \del F / \del N$
\equations{
    dU 
    =
    T dS 
    - 
    p dV 
    + 
    \mu dN 
    \\
    F = U - TS 
    \Rightarrow
    dF 
    =
    dU - S dT + T dS 
    =
    -S dT 
    - 
    p dV 
    +
    \mu dN 
    \\
    dF 
    =
    \frac{\del F}{\del T} dT 
    + 
    \frac{\del F}{\del V} dV 
    +
    \frac{\del F}{\del N} dN
}

That is the proof, so we now know chemical potential for a canonical 
ensemble. 

\section{Ideal Gas}
\equations{
    F 
    =
    -k_B T \ln(Z)
    \hp 
    Z_{I.D.} 
    =
    \frac{ (n_Q * V)^{N} }{N!}
    \\
    F 
    =
    -k_B T N
    \ln(n_Q * V)
    +
    k_B T
    \ln(N!)
    \\
    =
    -k_B T N
    \ln(n_Q * V)
    +
    k_B T N
    \ln(N)
    - 
    k_B T N
    \\
    \mu 
    =
    \frac{\del F}{\del N}
    =
    -k_B T
    \ln(n_Q * V)
    +
    k_B T
    \ln(N)
    +
    k_B T
    - 
    k_B T
    \\
    =
    -k_B T
    \ln(\frac{n_Q * V}{N})
    \hp 
    \frac{N}{V} = n
    \\
    \mu 
    =
    k_B T
    \ln(\frac{n}{n_Q})
}

For a classical ideal gas, $n << n_Q$, so the chemical potential 
is always negative. We also now know that the chemical potential is proportional 
to the log of the density. 

\section{Internal and External Chemical Potential}
This is the Nernst Equation. Consider an ideal gas with charge. You can 
counteract $\mu$ with a difference in potential energy $U$.
The energy is given by 
\equations{
    \epsilon_i
    =
    \frac{p_i^2}{2m}
    q U_{1 or 2}
    \hp 
    Z_{tot}
    =
    \frac{1}{N_1!}
    \left(
        \sum_i
        e^{-\beta \epsilon_i}
    \right)^{N_1}
    =
    \frac{1}{N_1!}
    \left(
        e^{- \beta q U_1}
        \sum_i
        e^{-\beta \frac{p_i^2}{2m}}
    \right)^{N_1}
    \\
    =
    \frac{1}{N_1!}
    \left(
        e^{- \beta q U_1}
        n_Q * V
    \right)^{N_1}
    \hp 
    F 
    =
    - k_B T \ln(Z)
    \\
    F 
    ====
    -N_1 k_B T \ln(e^{\beta q U_1} n_Q V_1)
    +
    N_1 k_B T \ln(N_1) - N_1 k_B T
    \\
    \mu 
    =
    \frac{\del F}{\del N}   
    ===
    k_B T \ln(\frac{n_1}{n_Q}) 
    + 
    q U_1
    =
    \mu_{int}
    +
    \mu_{ext}
}

At equilibrium, the chemical potential can be written as 
\equations{
    \mu_1 = \mu_2 
    \hp 
    \Delta \mu_{ext}
    =
    q U_1
    -
    q U_2
    =
    - \Delta \mu_{int}
    =
    k_B T \ln(\frac{n_2}{n_1})
    \hp 
    \\
    q U_1
    -
    q U_2
    =
    k_B T \ln(\frac{n_2}{n_1})
}

This is known as the Nernst Equation. This equation also works for the 
chemical potential in a charge dilute solution.
\equations{
    \mu_{sol}
    = 
    \mu_{sol0}
    +
    k_B T \ln(\frac{n_{sol}}{n_0})
}

\subsection{Atmospheric Density}
\equations{
    \mu(h)
    =
    k_B T \ln(\frac{n(h)}{n_Q})
    + 
    mgh 
    =
    \mu_{int}
    +
    \mu_{ext}
    \\
    n(h) 
    = 
    n(0) e^{-\frac{mgh}{k_B T}}
    \hp 
    p(h) 
    =
    p(0)
    e^{-\frac{mgh}{k_B T}}
}


\section{Discussion}
\equations{
    I(\alpha)
    =
    \int^{\infty}_{-\infty} \, dx \, 
    e^{- \alpha x^2}
    \\
    I(1)^2
    =
    \int^{\infty}_{-\infty} \, dx \, 
    \int^{\infty}_{-\infty} \, dx \, 
    e^{- (x^2 + y^2)}
    = 
    \int^{2 \pi}_{0} \, d \theta \, 
    \int^{\infty}_{0} \, d r \, 
    \gamma e^{- \gamma^2}
    \\
    =
    2 \pi 
    \left(
        -
        \frac{1}{2}
        e^{-\gamma^2}
    \right)
    \Big|^{\infty}_{0}
    =
    \sqrt{\pi}
}

\section{Gibbs Free Energy}
\equations{
    G 
    =
    U + pV 
    - TS 
    =
    H - TS 
}
Where $H$ is the heat (i think).
Remember that 
\equations{
    dS 
    =
    \left(
        \frac{\del S}{\del U}
    \right)_{V}
    dU 
    +
    \left(
        \frac{\del S}{\del V}
    \right)_{U}
    dV 
    =
    \frac{1}{T} dU 
    +
    \frac{p}{T} dV 
}

For the total system $R + s$, we know that 
\equations{
    dS_{R + s} 
    =
    0
    =
    dS_{R}
    +
    dS_{s}
    =
    \frac{dI_{R}}{T_{R}}
    +
    \frac{p_{R}}{T_{R}} dV_R 
    +
    dS_{s}
    \\
    U_{tot}
    =
    \const 
    =
    U_R + U_s 
    \rightarrow 
    dU_R = - dU_s
    \\
    V_{tot}
    =
    \const 
    =
    V_R + V_s 
    \rightarrow 
    dV_R 
    =
    - dV_s
}

Plug those things into our total entropy equation and get 
\equations{
    dS_{tot}
    =
    -
    \frac{dI_{s}}{T_{R}}
    -
    \frac{p}{T} dV_s
    +
    dS_{s}
    =
    - \frac{1}{T} 
    d 
    \left(
        U_s 
        +
        p V_s 
        -
        T S_s
    \right)
}

We now have the equilibrium condition of entropy in terms of the 
system $s$ alone.

\equations{
    G 
    =
    U_s 
    +
    p V_s 
    -
    T S_s
    \hp
    dS_{tot}
    =
    0 
    \Rightarrow 
    0
    =
    - dG
}

To maximize entropy, you minimize $G$.

\section{Summary}
\subsection{Microcanonical Ensemble}
Closed system with fixed $U, V$. 

At equilibrium, we maximize entropy $\Delta S \geq 0$.

\subsection{Canonical Ensemble}
System $s$ coupled to reservoir $S$ of set temperature. 
$V_s$ and $T$ are constant. For this, we use the Helmolz Free Energy $F$.
At equilibrium, we minimize Helmholz Free Energy $\Delta F_s \leq 0$. 

\subsection{Fixed Pressure Reservoir}
Consider a system with fixed $p, T$ instead of fixed $V_s, T$.

At equilibrium, Gibbs Free Energy is minimized $\Delta G \leq 0$.

\subsection{Math}
We showed that 
\equations{
    U 
    =
    U(S, V)
    \hp 
    F = F(T, V) = U - TS 
    \hp 
    G 
    =
    G(T, p)
    =
    H - TS 
    \\
    dU 
    =
    T dS 
    -
    pdV 
    +
    \mu dN 
    \\
    dF 
    =
    - p dV 
    -
    S dT 
    +
    \mu dN 
    \\
    dG 
    =
    -S dT 
    +
    V dp 
    +
    \mu dN
}

\section{Relating G to $\mu$}
Consider a monatomic ideal gas. 
\equations{
    G 
    =
    U + pdV - TS
    \\ 
    U 
    =
    \frac{3}{2}
    k_B T 
    \hp 
    pV 
    =
    N k_B T 
    \hp
    S 
    =
    N k_B 
    \left(
        \ln(\frac{n_Q}{n}) + \frac{5}{2}
    \right)
    \\
    G_{ideal gas}
    =
    \frac{3}{2} N k_B T 
    +
    N k_B T 
    -
    N k_B T 
    \left(
        \ln(\frac{n_Q}{n}) + \frac{5}{2}
    \right)
    =
    N k_B T 
    \left(
        \ln(\frac{n}{n_Q})
    \right)
    \\
    =
    N * \mu_{ideal}
    =
    G_{ideal}
    \hp 
    \mu 
    =
    k_B T 
    \ln(\frac{n}{n_Q})
}
While we proved this for the ideal gas, this is actually true in all cases.
The proof is also supposedly very short. 

An extensive quantity doubles if every feature doubles 
$N \to 2N, V \to 2V, U \to 2U$. 

Examples are $U, V, S, F$.

Intensive quantities stay the same if the entire system doubles proportionally.
Examples are $T, p, n = \frac{N}{V}, \mu $

\equations{
    G 
    =
    U + pV 
    -
    TS
}
Because $U, V, S$ are all extensive, then $G$ is just a sum of extensive 
properties, and thus is also extensive. However $G$ is only a condition 
based off of $G(T, p, N)$, and $T$ and $p$ are both intensive, so 
\equations{
    G(T, p, 2N) = 2G
}

Because of this, we know that $G$ must be proportional to $N$ since it's the 
only extensive variable and $G$ itself is extensive. 
\equations{
    G 
    \sim
    N * \varphi(T, p)
}

Now we have to figure out $\varphi$ 
\equations{
    dG 
    =
    - S dT  
    +
    V dp 
    +
    \mu dN
    \Rightarrow 
    \left(
        \frac{dG}{dN}
    \right)_{T, p}
    =
    \mu
    \\ 
    dG 
    =
    \left(
        \frac{d}{dN} (N * \varphi(T, p))
    \right)_{T, p}
    =
    \varphi(T, p)
    =
    \mu
}

This does not work for the Helmholz Free Energy $F$ because it is depedent 
on 2 extensive variables $V$ and $N$.

\subsection{Generalizing $G_n$}
This is specifically if you have $n$ species
\equations{
    G(T, p, N_1, N_2, \ldots, N_n)
    =
    \sum_{i = 1}^{n}
    N_i \mu_i(T, p)
}

\subsection{Chemical Equilibrium}
Chemical reactions for fixed $p, T$ can use $G$ instead. 
Consider the nitrogen fixation reaction 
\equations{
    N_2 + 3H_2 
    \leftrightarrow
    2 NH_3
    \hp 
    -N_2 - 3H_2 
    +
    2 NH_3
    =
    0
}

The chemical reaction can be written as 
\equations{
    \nu_1 A_1 
    +
    \nu_2 A_2 
    +
    \ldots 
    +
    \nu_n A_n 
    =
    \sum_{i=1}^{n}
    \nu_i A_i
    =
    0
}
Where $A_i$ denotes the $i$th chemical species. 
For constants $p, T$, equilibrium is minimizing $G$. 

\equations{
    dG 
    =
    0
    =
    -S dT 
    +
    V dP 
    +
    \mu dN 
    \rightarrow 
    dG 
    =
    \sum_{i = 1}^{n}
    \mu_i dN_i
}

Where $dN_i$ is the change in the number of molecules of species $i$ 
in the reaction. 
Let $d \hat N$ denote the number of reaction cycles 

\equations{
    dN_{N2} 
    =
    -1 * d \hat N
    \hp
    dN_{H2} 
    =
    -3 * d \hat N
    \hp
    dN_{NH3} 
    =
    +2 * d \hat N
    \\
    dG 
    =
    0
    =
    -1 \mu_{N2}
    -
    3 \mu_{H2}
    +
    2 \mu_{NH3}
}
This is the condition of diffusive equilibrium.

In general, this can be written as 
\equations{
    dN_i 
    =
    \nu_i d \hat N_i
    \hp 
    dG = 0
    \Rightarrow 
    \sum_{i = 1}^{n}
    \mu_i \nu_i 
    =
    0
}

If we consider $N_2, H_2, NH_3$ each as ideal gases, we can get 
\equations{
    p_i 
    =
    n_i k_B T 
    \hp 
    n_i = \frac{N_i}{V}
    \\
    \mu_i 
    =
    k_B T 
    \ln(\frac{n_i}{n_Q(T)})
    =
    k_B T 
    \ln(\frac{p}{n_Q k_B T})
    =
    \mu_{i0}
    +
    k_B T \ln(\frac{p_i}{p_0})
}

$p_i$ is specifically the partial pressure of gas $i$ in the system.
\equations{
p_0 = 1atm 
\hp 
\mu_0 = k_B T \ln(\frac{p_0}{n_Q k_B T})
}

Now we plug $\mu_{N2}, \mu_{H2}, \mu_{NH3}$ into our equilibrium equation 
\equations{
    dG 
    =
    \\
    - \mu_{N2}^{ \varnothing}
    -
    k_B T 
    \ln(\frac{p_{N2}}{p_0})
    -
    3
    \left(
    \mu_{H2}^{ \varnothing}
    +
    \ln(\frac{p_{H2}}{p_0})
    \right)
    +
    2
    \left(
    \mu_{NH3 \varnothing}
    +
    \ln(\frac{p_{NH3}}{p_0})
    \right)
    \\
    =
    k_B T 
    \ln(
        \frac{p_{N2} * (p_{H2})^{3}}{(p_{NH3})^{2} * (p_{0})^{2}}
    )
    =
    - \mu_{N2}^{ \varnothing}
    -
    3\mu_{H2}^{ \varnothing}
    +
    2\mu_{NH3 \varnothing}
}

The left side is $\Delta G_0$ which is the standard Gibbs Free Energy of 
a reaction. 

\equations{
    \frac{(p_{NH3})^{2} * (p_{0})^{2}}{(p_{N2}) * (p_{H2})^{3}}
    =
    e^{- \frac{\Delta G_0}{k_B T}}
    =
    K(T)
    =
    \textrm{ equil. constant}
    \hp 
    \Delta G_0 < 0 
    \hp 
    K(T) > 1
}

This is known as the law of mass action. 
Because $K(T)$ is constant, you can figure out the change in resulting 
partial pressure if the partial pressure of the reactants changes.

If you do all the math for the water reaction 2H2 + O2 = 2H2O, you get 
\equations{
    \frac{(p_{H2O})^{2} p_0}{(P_{H2})^{2} * (p_O2)^{2}}
    =
    e^{- \frac{\Delta G_0}{k_B T}}
    \equiv 
    K(T)
}

dilute solutions have a different equations that's on the last page of lecture 
11.


\chapter{Grand Canonical Ensemble (Not on Midterm 1)}
Consider a reservoir $R$ and a system $\rho$ of $N=N$ and $\epsilon = \epsilon$. 
The reservoir has parameters $N_0 - N$ and $U_0 - \epsilon$. 
\equations{
    U_0 
    =
    U_R 
    + 
    \epsilon 
    =
    \const 
    \hp
    N_0 
    =
    N_R 
    +
    N 
    =
    \const
}

For a gibbs ensemble or a grand sum ensemble. We consider the system in a 
single microstate and the multiplicity of the system. 
\equations{
    p(N_i, \epsilon_i)
    =
    \frac{
        \Omega_{R + \epsilon}(N_i, \epsilon_i)
    }
    {
        \sum_{N_i, \epsilon_i} \Omega_{R + \rho}(N_i, \epsilon_i)
    }
    \\
    \frac{p(N_1, \epsilon_1)}{p(N_2, \epsilon_2)}
    =
    \frac{
        \Omega_{R}(N_0 - N_1, U_0 - \epsilon_1) \cdot \perp
    }
    {
        \Omega_{R}(N_0 - N_2, U_0 - \epsilon_2) \cdot \perp
    }
    =
    \frac{
        e^{S_R(N_0 - N_1, U_0 \epsilon_1) / k_B}
    }
    {
        e^{S_R(N_0 - N_2, U_0 \epsilon_2) / k_B}
    }
}

For a reservoir $R$, we can do a taylor expansion around $U_0$ for 
$U_0 >> \epsilon_1$ or $\epsilon_2$ and $N_0 >> N_1$ and $N_2$ 

\equations{
    S_R(N_0 - N_i, U_0 - \epsilon_i)
    \approx 
    S_R(N_0, U_0)
    -
    \frac{\del S_R}{\del N} \Big|_{N_0}
    N_i 
    -
    \frac{\del S_R}{\del U} \Big|_{U_0}
    \epsilon_i 
    +
    \ldots 
    \\
    =
    S_R(N_0, U_0)
    +
    \frac{\mu}{T} N_i 
    -
    \frac{\epsilon_i}{T}
    \\
    \frac{p(N_1, \epsilon_1)}
    {
    p(N_2, \epsilon_2)
    }
    =
    \frac{e^{(\mu N_1 - \epsilon_1) / k_B T}}
    {
    e^{(\mu N_2 - \epsilon_2) / k_B T}
    }
    \hp 
    \sum_N \sum_\epsilon p(N, \epsilon) = 1
    \\
    p(N_1, \epsilon_1)
    =
    \frac{
        e^{- (\epsilon_i - \mu N_i) / k_B T}
    }
    {
        \sum_{N_j} \sum_{\epsilon_j}
        e^{- (\epsilon_j - \mu N_j) / k_B T}
    }
}

$ e^{- (\epsilon_i - \mu N_i) / k_B T} $
Is known as the Gibbs Factor. The Gibbs Sum or Grand Sum is 
\equations{
    \zeta(\mu, T)
    =
    \sum_{N_j} \sum_{\epsilon_j}
    e^{- (\epsilon_j - \mu N_j) / k_B T}
}

I don't know what's happening anymore I don't really care I'm tired 
\equations{
    \zeta(\mu, T)
    =
    \sum_{N}
    e^{-\mu N / k_B T}
    *
    \sum_{\epsilon}
    e^{-\epsilon / k_B T}
    =
    \sum_N \lambda^N Z(N, T)
    \hp
    \lambda 
    =
    e^{\mu / k_B T}
    \\
    \langle N \rangle 
    =
    \frac{
    \sum_N 
    \sum_{\epsilon(N)} 
    N
    \lambda^N
    e^{-\epsilon / k_B T}
    }
    {
    \zeta(\mu, T)
    }
    =
    k_B T 
    \frac{\del}{\del \mu}
    \ln(\zeta(\mu, T))
    =
    \lambda 
    \frac{\del}{\del \lambda}
    \ln(\zeta(\lambda, T))
    \\
    U 
    =
    \langle \epsilon \rangle 
    =
    \frac{
        \sum_N \sum_{\epsilon(N)}
        \epsilon e^{-(\epsilon - \mu N) / k_B T}
    }
    {
        \zeta(\mu T)
    }
}

\subsection{Example: Myoglobin}
I didn't want to pay attention so I did not.

\section{Multiple Species}
Consider a system $\rho$ with $N_A, N_B, \epsilon_A, \epsilon_B$ attached 
to a reservoir $R$. 

\equations{
    \frac{p_1}{p_2}
    =
    \frac{
    \Omega_R(N^{\varnothing}_{A} - N_{A1}, N^{\varnothing}_{B} - N_{B1},
    U_0 - \epsilon_{A1} - \epsilon_{B1})
    }
    {
    \Omega_R(N^{\varnothing}_{A} - N_{A2}, N^{\varnothing}_{B} - N_{B2},
    U_0 - \epsilon_{A2} - \epsilon_{B2})
    }
    =
    e^{\Delta S_R / k_B T}
}

\subsection{Example: Carbon Dioxide}
I don't care

\chapter{Photon Gas}

Given a box of length $L$ at temperature $T$. Inside the box is EM radiation. 
\equations{
    |\vec k|^2 
    =
    k^2 
    =
    \frac{\omega^2}{c^2}
}

And an EM wave of frequency $\omega$ has energy 
\equations{
    \epsilon_n = \hbar \omega (n + \frac{1}{2})
    \approx 
    \hbar \omega n
}

What is the partition function for a single mode? 
\equations{
    Z 
    =
    \sum^{\infty}_{s=0}
    e^{-\beta \epsilon_s}
    =
    \sum^{\infty}_{s=0}
    e^{- \beta \hbar \omega s}
    =
    \frac{1}{1 - e^{- \beta \hbar \omega } }
}

And the average energy can be calculated with 
\equations{
    U 
    =
    \langle \epsilon \rangle 
    =
    - \frac{\del}{\del \beta} \ln(Z)
    =
    + \frac{\del}{\del \beta}
    \ln(1 - e^{- \beta \hbar \omega})
    =
    \frac{\hbar \omega}{ e^{\beta \hbar \omega} - 1 }
    =
    \hbar \omega \langle s \rangle 
}

And $\langle s \rangle$ is the average number of photons in the mode 
at temp $T$. 
For high temperatures, it starts functioning as the classical case 
\equations{
    \langle s \rangle 
    \approx 
    \frac{k_B T}{\hbar \omega}
    \Rightarrow 
    U 
    =
    \langle \epsilon \rangle 
    =
    k_B T
}

It follows the equipartition theorem.

\subsection{Standing Waves}
Standing waves are described by a discrete set of integers $n_x, n_y, n_z$. 
such that 
\equations{
    \omega = c |\vec k| 
    \hp
    \omega_n = c \sqrt{k_x^2 + k_y^2 + k_z^2}
    \hp 
    k_x 
    =
    \frac{n_x \pi}{L_x}
    \\
    Z_{tot}
    =
    \prod_{modes}
    \frac{1}{1 - e^{- \beta \hbar \omega_n}}
    \hp 
    U_{tot}
    =
    \sum_{modes=n}
    \frac{\hbar \omega_n}{e^{\beta \hbar \omega_n} - 1}
    =
    \sum_{modes=n}
    \hbar \omega_n \langle s_n \rangle
}

\subsection{Continuum Basis}
Let's assume that each particle is in an empty space of $\pi/L_x$ by $\pi/L_y$
by $\pi/L_z$. So, it occupies a volume $\pi^3 / V$. If we consider only 
the quarter circle shell of points such that $a < |\vec k|^2 < a + \delta$.

What we're trying to solve is 
\equations{
    \# modes: f \in [\omega, \omega + d \omega] 
    =
    \frac{V(shell)}{V(\textrm{per k space})}
    =
    D(\omega) d\omega
    \\
    =
    2 * 
    \frac{\frac{4 \pi k^2}{8} dk}{ \frac{\pi^3}{V}}
    =
    \frac{V}{\pi^2} k^2 dk 
    =
    \frac{V}{\pi^2} \frac{\omega^2 d \omega}{c^3}
}

Remember that there are 2 polarizations per mode (hence the beginning 2 
coefficient). 
To find energy, we calculate 
\equations{
    U_{tot}
    =
    \sum_n \langle \epsilon_n \rangle 
    =
    \sum_n 
    \frac{\hbar \omega}{e^{\beta \hbar \omega_n} - 1}
    \approx 
    \int^{\infty}_{0} \, d \omega \, 
    D(\omega) 
    \frac{\hbar \omega}{e^{\beta \hbar \omega_n} - 1}
    \\
    =
    \int^{\infty}_{0} \, d \omega \, 
    \frac{V}{\pi^2} \frac{\omega^2 }{c^3}
    \frac{\hbar \omega}{e^{\beta \hbar \omega_n} - 1}
    \hp
    \frac{U_{tot}}{V}
    =
    \mu_{tot}
    =
    \int^{\infty}_{0} \, d \omega \, 
    \frac{\hbar \omega^3 }{\pi^2 c^3}
    \frac{1}{e^{\beta \hbar \omega_n} - 1}
    \\
    \mu(\omega)
    =
    \frac{\hbar \omega^3 }{\pi^2 c^3}
    \frac{1}{e^{\beta \hbar \omega_n} - 1}
}
$\mu(\omega)$ is the energy density in the frequency band 
$[\omega, \omega + d \omega]$.

This is known as the \textbf{Planck Radiation Law}

You can graph it to get a black body radiation something 
\equations{
    x = \beta \hbar \omega = \frac{\hbar \omega}{k_B T}
    \hp 
    \frac{d}{dx}
    \frac{x^3}{e^x -1} \overset{!}{=} 0
    \\
    \frac{d}{dx}
    \frac{x^3}{e^x -1} 
    = 
    \frac{3x^2}{e^{x} - 1}
    - 
    \frac{x^3 e^{x}}{(e^x - 1)^2}
    =
    \left(
        \frac{x^2}{e^x - 1}
    \right)
    \left(
        3 
        -
        \frac{x e^{x}}{e^x - 1}
    \right)
}

This equation has trivial solution $x=0$ and nontrivial solution: 
\equations{
    3 
    =
    \frac{x e^{x}}{e^x - 1}
    \Rightarrow 
    3 (1 - e^{-x}) = x
}

This can be solved numerically, and the numerical solution is known as 
\textbf{Wien's Law}.

\subsection{Wien's Law}
\equations{
    2.82 
    =
    x_{max}
    = 
    \beta \hbar \omega_{max}
    =
    \frac{\hbar \omega_{max}}{k_B T}
    \Rightarrow 
    \hbar \omega_{max}
    =
    2.82 k_B T
}

\subsection{Def: Black Body}
Idealized object that absorbs all EM radiation. In thermal equilibrium 
at temp $T$, a black body emits radiation with spectrum $\mu(\omega)$.

\subsection{Example: Cosmic Microwave Background}

$mu(\omega)$ at $T = 273 K \rightarrow \lambda_{peak} =$ microwave.

\section{Low Temperature Limit (Ultraviolet Catastrophe)}
Let $\hbar \omega << k_B T$
\equations{
    \mu(\omega)
    =
    \frac{\hbar \omega^3}{\pi^2 c^3} 
    \frac{1}{e^{\beta \hbar \omega} - 1}
    \approx 
    \frac{\hbar \omega^3}{\pi^2 c^3} 
    \frac{k_B T}{\hbar \omega}
    \hp 
    e^x 
    \approx 
    1 + x + \ldots 
    \\
    \mu(\omega)
    =
    \omega^2 \frac{k_B T}{\pi^2 c^3}
    \propto
    \omega^2 k_B T
}
This is the Raleigh-Jeans Law.
This is the classical limit without quantum mechanics. However, this breaks 
everything because the total energy diverges 
$\int^{\infty}_{0} d \omega \, \mu(\omega) = \infty$ (which is bad).
This catastrophe is what caused us to think of photons and discover quantum 
mechanics.

\section{Stefan-Boltzmann Law}
\equations{
    \mu_{tot}
    =
    \int^{\infty}_{0} \, d \omega \, 
    \mu(\omega)
    =
    \int^{\infty}_{0} \, d \omega \, 
    \frac{\hbar \omega^3}{\pi^2 c^3} 
    \frac{1}{e^{\beta \hbar \omega} - 1}
    \hp 
    x = \beta \hbar \omega 
    =
    \frac{\hbar \omega}{k_B T}
    \\
    \mu_{tot}
    =
    -
    \frac{(k_B T)^4}{pi^2 (\hbar c)^3}
    \int^{\infty}_{0} \, d x \, 
    \frac{x^3}{e^x - 1}
    \propto 
    T^4
}

This is dramatically different from an ideal gas where $U \propto T$.
We can solve the integral and plug it in to get 

\subsection{Stefan-Boltzmann Law}
\equations{
    \mu_{tot}
    =
    \frac{\pi^2}{15}
    \frac{(k_B T)^4}{(\hbar c)^3}
}

We can do shenanigans with entropy now. 
\equations{
    dU 
    =
    T dS - pdV 
    =
    T dS 
    \Rightarrow 
    dS 
    =
    \frac{1}{T} dU 
    =
    \alpha V \frac{4T^3 dT}{T}
    \\
    dS 
    =
    4 \alpha V T^2 dT 
    \hp 
    S 
    =
    \int dS 
    =
    \int
    4 \alpha V T^2 dT 
    =
    \frac{4 \alpha}{3} V T^3 + C
    \hp
    (C = 0)
    \\
    S(V, T)
    =
    \frac{4 \alpha}{3} V T^3 + C
    \hp 
    S(U, V)
    =
    \frac{4}{3}
    (\alpha V)^{1/4} U^{3/4}
    \hp 
    S 
    \propto 
    U^{3/4}
}

Very different from an ideal gas.

\section{Discussion (october 14th)}
It looks like we're just reviewing the different ensembles

Micro-canonical ensembles mean no reservoir, constant $N, V, U$, and they
are governed by 
\equations{
    S = k_B \ln(\Omega)
}

Canonical ensembles have a reservoir and thus constant $N, V, T$ with 
the small system having energy $\epsilon$. 
It is governed by 
\equations{
    Z = \sum_{i} e^{- \beta \epsilon_i}
    \hp 
    F 
    =
    k_B T \ln(Z)
    =
    U - TS
}

The Grand Canonical or Gibbs ensemble allows particles to move 
so what's constant is $\mu, U, T$ with small system energy $\epsilon$ 
and the equations are 
\equations{
    \zeta(\mu, T)
    =
    \sum_N \sum_{\epsilon(N)}
    e^{- (\epsilon - \mu N) / k_B T}
    =
    \sum_N 
    \lambda^N Z(N, T)
    \hp 
    \lambda 
    \equiv 
    e^{\mu / k_B T}
    \\
    \Phi 
    =
    G 
    =
    U - TS 
    -
    \mu \langle N \rangle
    \hp 
    \langle N \rangle 
    =
    \sum_N \sum_{\epsilon(N)}
    N
    e^{- (\epsilon - \mu N) / k_B T}
    =
    \lambda \frac{\del}{\del \lambda} \zeta(\lambda, T)
    \\
    \Phi = G 
    =
    -k_B T \ln(\zeta)
    \\
    \langle \epsilon \rangle 
    =
    \frac{\mu}{\beta} \frac{\del}{\del \mu} \zeta 
    -
    \frac{\del}{\del \beta}
    \ln(\zeta)
}

\chapter{Debye Model}
Instead of looking at a photon gas, we look at lattice vibrations 
within a solid (\textbf{phonons}). I remember talking about these in the quantum 
+ chips summer school and that's about it. 

Consider a 3D lattice of atoms that are elastically coupled (Let's call 
them springs), and the lattices takes up a volume $V$. 

At high temperature
\equations{
    \epsilon_i 
    =
    \frac{p_i^2}{2m} + \frac{1}{2} kx_{i}^{2}
    \hp 
    U_{tot}
    N * 3 * 2 * \frac{1}{2} k_B T 
    =
    3 N k_B T
    \\
    C_{v}
    =
    \left(
        \frac{\del U}{\del T}
    \right)_{V}
    =
    3 N k_B T
}
(3 * 2 comes from 3 translational degrees of freedom + kinetic 
energy + potential energy)

At low temperature, we have to consider quantum shenanigans 
\equations{
    \epsilon_s 
    =
    \hbar \omega 
    \left(
        s 
        +
        \frac{1}{2}
    \right)
}

This looks very similar to an Einstein Solid, which has a calculated 
energy of 
\equations{
    U_1
    =
    \frac{\hbar \omega}{e^{\beta \hbar \omega} - 1}
    \hp 
    U_N 
    =
    3 N
    \frac{\hbar \omega}{e^{\beta \hbar \omega} - 1}
    \\
    C_{v}
    =
    \left(
        \frac{\del U_{tot}}{\del T}
    \right)_{V}
    =
    \left(
        \frac{\hbar \omega}{k_B T}
    \right)^2 
    k_B
    \frac{e^{\beta \hbar \omega} }{(e^{\beta \hbar \omega} - 1)^2}
}

for high temperature $k_B T >> \hbar \omega$, we use the taylor 
series $e^x \approx 1 + x + \ldots$ to get 
\equations{
    U_{tot}
    =
    3N 
    \frac{\hbar \omega}{1 + \beta \hbar \omega - 1} 
    =
    3 N \frac{1}{\beta} = 3 N k_B T 
    \hp 
    C_{v}
    =
    \left(
        \frac{\del U_{tot}}{\del T}
    \right)_{V}
    =
    3 N k_B
}

At low temperature $k_B T << \hbar \omega$, we get 
\equations{
    U_{tot}
    \approx 
    3 N \hbar \omega e^{-\beta \hbar \omega}
    \hp
    C_{v}
    =
    3 N k_B
    \left(
        \frac{\hbar \omega}{k_B T}
    \right)^2 
    e^{-\hbar \omega / k_B T}
}

Experimentally, we see that the specific heat capacity starts at 
0 for $T = 0K$, and it goes up to an asymptote $3 N k_B$ as $T$ 
gets higher. $C_{V}$ goes up linearly with $T^2$ in experiments at low $T$. 

For an insulator we calculate $C_V = A T^3$, and for a metal, 
$C_V = \gamma T + A T^3$. These are both wrong, so we have to figure 
out what's up with the Einstein model to get the experimental result of $T^2$
The biggest issue with the Einstein model is that it assumes that 
every atom has the same frequency $\omega$.

\section{Debye Model}
Atoms are \textbf{not} independent oscillators. Consider atoms of a 
lattice spacing $a$.

The force on a single atom is given by 
\equations{
    F_i 
    = 
    \kappa
    \left(
        r_{i + 1}
        -
        r_{i}
        - 
        a
    \right)
    - 
    \kappa
    \left(
        r_{i}
        -
        r_{i - 1}
        - 
        a
    \right)
    =
    m \frac{d^2 r_{i}}{d t^2}
    =
    \kappa
    \left(
        r_{i + 1} - 2r_i + r_{i-1}
    \right)
    \\
    \frac{df}{dx}
    \lim_{a \to 0}
    \frac{f(x + a) - f(x)}{a}
    \Rightarrow
    \frac{d^2f}{dx^2}
    =
    \lim_{a \to 0}
    \frac{f(x + a) - 2f(x) + f(x-a)}{a^2}
    \\
    \frac{r_{i + 1} - 2r_i + r_{i-1} }{a^2}
    \approx 
    \frac{d^2 r}{dx^2}
    \approx 
    \frac{m}{\kappa a^2}
    \frac{d^2 r}{dt^2}
    \hp 
    \frac{m}{\kappa a^2}
    =
    \frac{1}{C_S^2}
}
$C_s$ is the speed of sound in the lattice. We can just treat the set 
of lattices as a wave equation. 

\equations{
    \tau(x, t)
    \approx
    e^{ikx - i \omega t}
    \hp 
    \omega 
    =
    c_s k
}

This approximation is valid for $\lambda = \frac{2 \pi}{k} >> a$ 
so there's a long wavelength. 
The Debye model assumes that $\omega = c_s k$ is always valid.
Atoms move either parallel or perpendicular to the wave 
propagation direction, and there's 1 parallel and 2 perpendicular directions. 
The speed of sound in the lattice $c_s$ could depend on the polarization 
of the wave. All we know is that we should get 
\equations{
    U_{tot}
    =
    ?
    \hp 
    C_{v}
    =
    \left(
        \frac{\del U_{tot}}{\del T}
    \right)_{V}
    \simeq 
    T^3
}

\section{1D Chain}
Let's consider a 1d chain bounded at distance $L_x$ from each other. 
The standing waves will have the following forms 
\equations{
    \lambda = 2 L_x 
    \hp 
    k_x 
    =
    \frac{2 \pi}{\lambda}
    =
    \frac{\pi}{L_x}
    \hp
    \lambda = L_x 
    \hp 
    k_x 
    =
    \frac{2 \pi}{L_x}
    \hp
    \lambda = \frac{2}{3} L_x 
    \hp 
    k_x 
    =
    \frac{3 \pi}{L_x}
}

etc etc, and the maximum standing wave will be of the form 
\equations{
    k_{max}
    =
    \frac{\pi}{a}
    =
    \frac{N_x \pi}{L_x}
    \hp 
    k_{x}
    =
    \frac{n_x \pi}{L_x}
    \hp
    n_x
    =
    1, 2, 3, \ldots, N_x
}

And it's the same for $y$ and $z$.

With this, the total number of modes as be written as 
\equations{
    \#(modes)_{tot}
    =
    3 N_x N_y N_z 
    =
    3N
}
And there 3 is because of the polarization states of the waves.

\subsection{Photon Gas}
For a photon gas, we have 
\equations{
    \omega = ck
}
You are always 2 transverse polarizations

\equations{
    k_{x, y, z}
    =
    n_{x, y, z}
    \frac{\pi}{L_{x, y, z}}
    \hp 
    n_{x, y, z}
    =
    1, 2, 3, \ldots, \infty
    \hp
    2 \sum_{n_x, n_y, n_z} 
    \to 
    \infty
}

\subsection{Phonon Gas}
and phonon gas, we have 
\equations{
    \omega 
    =
    c_s k : \lambda >> a 
}
You are always 2 transverse and 1 longitudinal polarization. 

\equations{
    k_{x, y, z}
    =
    n_{x, y, z}
    \frac{\pi}{L_{x, y, z}}
    \hp 
    n_{x, y, z}
    =
    1, 2, 3, \ldots, N_{x, y, z}
    \hp
    3 \sum_{n_x, n_y, n_z} 
    =
    3N
}

\section{Phonon Energy}
\equations{
    U_{tot}
    =
    \sum_{n=1}^{3N}
    \frac{\hbar \omega_n}{e^{\beta \hbar \omega_N} - 1}
}
Where $n$ is shorthand for $n_x, n_y, n_z$. The way to do this is 
by transforming it into an integral with large $N$.
\equations{
    \# modes(\omega \in [\omega, \omega + d \omega])
    =
    \# polarizations
}

The easiest way to do this is the spherical shell method that we 
used in lecture 13 or for the derivation of the ideal gas.
\equations{
    D(\omega) d \omega 
    =
    3 
    \frac{\frac{4 \pi k^2 dk}{8}}
    { \frac{\pi^3}{V} }
    =
    \frac{3V}{2 \pi^2 } k^2 dk
    =
    \frac{3}{2}
    \frac{V}{\pi^2}
    \frac{\omega^2 d \omega}{c_s^3}
    \\
    U_{tot}
    =
    \sum_{n=1}^{3N}
    \frac{\hbar \omega_n}{e^{\beta \hbar \omega_N} - 1}
    =
    \int_{0}^{?} \, d \omega \, 
    D(\omega)
    =
    \int_{0}^{?} \, d \omega \, 
    D(\omega)
    \frac{\hbar \omega_n}{e^{\beta \hbar \omega_N} - 1}
}

Let's assume that instead of a square of $L_x, L_y, L_z$, we instead 
have a sphere of radius $r$ such that there is an equal amount of modes 
as the cube which is $\# modes = 3N$.

\equations{
    3N 
    =
    3 
    \frac{\frac{4}{3} \pi \frac{k_D^3}{8}}{\frac{\pi^3}{V}}
    =
    \frac{V}{2 \pi^2} 
    k_D^3
    =
    \frac{V}{2 \pi^2} 
    \left(
        \frac{\omega_D}{c_s}
    \right)^3
    \Rightarrow 
    \omega_D 
    =
    6 \pi^2 
    \left(
    \frac{N}{V}
    \right)^{1/3}
    c_s
}

That is known as the Debye frequency, which is the upper limit 
of our integral.

\equations{
    U_{tot}
    =
    \int^{\omega_D}_{0} \, d \omega \, 
    D(\omega)
    \frac{\hbar \omega}{e^{\beta \hbar \omega} - 1}
    =
    \frac{3 V \hbar}{2 \pi^2 c_s^3}
    \int^{\omega_D}_{0} \, d \omega \, 
    \frac{\omega^3}{e^{\beta \hbar \omega} - 1}
    \\
    \frac{3 V \hbar}{2 \pi^2 c_s^3}
    \left(
    \frac{k_B T}{\hbar}
    \right)^4
    \int^{x_D(T)}_{0} \, d x \, 
    \frac{x^3}{e^{x} - 1}
}

that is dependent on the Debye Temperature 
\equations{
    \theta_D 
    =
    \frac{\hbar \omega_D}{k_B}
    \Rightarrow 
    x_D 
    =
    \frac{\theta_D}{T}
    \\
    \Theta_D 
    =
    \frac{\hbar \omega_D}{k_B}
    =
    \left(
        6 \pi^2
        \frac{N}{V}
    \right)^{1/3}
    \frac{\hbar c_s}{k_B}
    =
    T * x_D 
    \\
    U_{tot}
    \simeq
    \frac{3 V \hbar}{2 \pi^2 c_s^3}
    \left(
    \frac{k_B T}{\hbar}
    \right)^4
    \int^{\infty}_{0} \, d x \, 
    \frac{x^3}{e^{x} - 1}
    \approx
    \frac{3 V \hbar}{2 \pi^2 c_s^3}
    \left(
    \frac{k_B T}{\hbar}
    \right)^4
    \frac{\pi^4}{15}
    \\
    U_{tot}
    ==
    \frac{3 \pi^4}{5}
    N k_B T 
    \left(
        \frac{T}{\theta_D}
    \right)^3
    \\
    C_V 
    =
    \left(
        \frac{\del U}{\del t}
    \right)_V
    =
    \frac{12 \pi^4}{5}
    N k_B 
    \left(
        \frac{T}{\theta_D}
    \right)^3
}
This is for the low temperature limit 

For the high temp limit we get 
\equations{
    C_v = 3 N k_B
}

The Debye model gets the low and high temperature limits correct, but 
does not do as well in the middle.

\chapter{Quantum Statistics}
Consider 
\equations{
    n_Q
    =
    \left(
        \frac{m k_B T}{2 \pi \hbar^2}
    \right)^{3/2}
    =
    \frac{1}{\lambda_T^3}
}
Where $\lambda_{T}$ is the De Broglie wavelength. 

For $n \sim n_Q$, the wavefunctinos overlap which means that quantum effects 
are important. For high temperature $k_B T >> \Delta \epsilon$, then it's 
functionally continuous.

\equations{
    Z_N 
    =
    \frac{Z_{1}^{N}}{N!}
    \hp 
    Z_1 
    =
    n_Q V
}

Fermions have 1/2 integer spin. Bosons have integer spin. 
Quantum particles have orbitals each with unique energy, so our partition 
function is of the form 
\equations{
    Z 
    =
    e^{- \beta \epsilon_{1}}
    +
    e^{- \beta \epsilon_{2}}
    +
    e^{- \beta \epsilon_{3}}
    + 
    \ldots
}

\section{Fermions}

If we consider 2 particles in a 3 orbital system, our partition function 
is of the form 
\equations{
    Z_{FD}
    =
    e^{- \beta (\epsilon_1 + \epsilon_2)}
    +
    e^{- \beta (\epsilon_2 + \epsilon_3)}
    +
    e^{- \beta (\epsilon_1 + \epsilon_3)}
}

If we use just Maxwell Boltzmann statistics, we get 
\equations{
    Z_{MB}
    =
    \frac{Z_{1}^N}{Z!}
    =
    \frac{1}{Z}
    \left(
    e^{- \beta \epsilon_{1}}
    +
    e^{- \beta \epsilon_{2}}
    +
    e^{- \beta \epsilon_{3}}
    \right)^2
    \\
    =
    e^{- \beta (\epsilon_1 + \epsilon_2)}
    +
    e^{- \beta (\epsilon_2 + \epsilon_3)}
    +
    e^{- \beta (\epsilon_1 + \epsilon_3)}
    +
    \frac{1}{2}
    \left(
    e^{- 2\beta \epsilon_{1}}
    +
    e^{- 2\beta \epsilon_{2}}
    +
    e^{- 2\beta \epsilon_{3}}
    \right)
}

We get 3 incorrect terms because MB statistics overcounts the multiple 
occupancy states. At high temperatures, this doesn't matter because the 
vast majority of states don't worry about the Pauli Exclusion Principle, but 
it is important for Quantum Mechanics.

\section{Bosons}
If we consider 2 bosons with 3 possible states (they have the same spin), 
we get the true partition 
function of 

\equations{
    Z_{BE}
    =
    e^{- \beta (\epsilon_1 + \epsilon_2)}
    +
    e^{- \beta (\epsilon_2 + \epsilon_3)}
    +
    e^{- \beta (\epsilon_1 + \epsilon_3)}
    +
    e^{- 2\beta \epsilon_{1}}
    +
    e^{- 2\beta \epsilon_{2}}
    +
    e^{- 2\beta \epsilon_{3}}
}

The Maxwell Boltzmann results actually \textbf{undercounts} the partition 
function because it includes the 1/2 coefficient

\equations{
    Z_{MB}
    =
    e^{- \beta (\epsilon_1 + \epsilon_2)}
    +
    e^{- \beta (\epsilon_2 + \epsilon_3)}
    +
    e^{- \beta (\epsilon_1 + \epsilon_3)}
    +
    \frac{1}{2}
    \left(
    e^{- 2\beta \epsilon_{1}}
    +
    e^{- 2\beta \epsilon_{2}}
    +
    e^{- 2\beta \epsilon_{3}}
    \right)
}

MB works fine as long as the probability for \textbf{multiple occupancy}
is \textbf{negligible}. This is true when $n << n_Q$

\section{Average Occupancy}
Consider an ideal gas. For a particle to be in state $i$, the probability is 
\equations{
    p(i)
    =
    \frac{e^{- \beta \epsilon_i}}{Z_i}
}

For $N$ particles, the average number of occupants in state $i$ is
\equations{
    \langle N_i \rangle_{MB}
    =
    N p_i 
    =
    \frac{N}{Z_i}
    e^{- \beta \epsilon_i}
    =
    \frac{N}{n_Q V}
    e^{- \beta \epsilon_i}
}

% So with this, we can extrapolate to find the average number of occupants 
% in state $i$ 
% \equations{
%     \langle N_i \rangle_{MB}
%     =
%     \frac{N}{n_Q V} e^{- \beta \epsilon_i} 
%     =
%     \frac{N}{n_Q V} e^{- \beta \epsilon_i} 
% }

This is true for $n << n_Q$ where we neglect mutliple occupancy. thus 
$\langle N_i \rangle << 1$.

\section{Fermi-Dirac and Bose-Einstein Distributions}
(That's what FD and BE are in the equations above)
\equations{
    E_{tot}
    N_1 \epsilon_1
    +
    N_2 \epsilon_2
    +
    N_3 \epsilon_3
    + 
    \ldots
    =
    \sum_{i} N_i \epsilon_i
}

$N_i$ is either 0 or 1 for fermions and can be $0 \to N$ for bosons.
If it wasn't for the Pauli Exclusion Principle, quantum objects would
be exactly the same as photons/phonons statistically (i think).

We can use the grand canonical ensemble to implement the restraint 
$N = \sum N_i$. The system will have the $i$th orbital, and the 
reservoir will contain all other orbitals. We are in equilibrium with 
$R, T, \mu$.

\subsection{Fermions}
Calculate the Gibbs sum and get the FD distribution.
\equations{
    \zeta 
    =
    e^{\beta (\mu * 0 - \epsilon_i * 0)}
    +
    e^{\beta (\mu * 1 - \epsilon_i * 1)}
    =
    1 + 
    e^{\beta (\mu - \epsilon_i)}
}
Now we can find the average occupancy 
\equations{
    \langle N_{i} \rangle_{FD}
    =
    \frac{0 + 1 * ( 1 + e^{\beta (\mu - \epsilon_i)})}{\zeta}
    =
    \frac{ e^{\beta (\mu - \epsilon_i)} }{
    1 + 
    e^{\beta (\mu - \epsilon_i)}
    }    
    =
    \frac{1}{
    e^{\beta (\epsilon_i - \mu)}
    + 1
    }
    =
    f_{FD}
    (\epsilon_i, T)
}
You can solve the same thing with 
\equations{
    \langle N_i \rangle 
    =
    \frac{1}{\beta}
    \frac{\del}{\del \mu}
    \ln(\zeta)
}

But what about the chemical potential? The chemical potential $\mu$ must satisfy 
the constraint
\equations{
    N \overset{!}{=}
    \sum_i \langle N_i \rangle_{FD}
    =
    \sum f_{FD}(\epsilon_i, T)
}

The average energy can be found with 
\equations{
    U 
    =
    \sum_i 
    \langle N_i \rangle_{FD}
    \epsilon_{i}
    =
    \sum_i 
    f_{FD}(\epsilon_i, T)
    \epsilon_{i}
    =
    \frac{\epsilon_i}{
    e^{\beta (\epsilon_i - \mu)}
    + 1
    }
}

The graph of the Fermi-Dirac distribution is something. For $T=0$ 
$f_{FD}$ is at a constant $1$ until $\epsilon_i = \mu$, in which it's a 
step function to 0. For higher $T$, the step function becomes a 
smooth decrease.

At $\epsilon_i = \mu \Rightarrow \langle N_i \rangle_{FD} = \frac{1}{2} \sim k_B T$

\subsection{Bosons}
\equations{
    \zeta_i 
    =
    e^{\beta(\mu * 0 + \epsilon_i * 0)}
    +
    e^{\beta(\mu * 1 + \epsilon_i * 1)}
    +
    \ldots 
    +
    e^{\beta(\mu * N + \epsilon_i * N)}
}

If $N \sim 10^{20} \sim \infty$, then 
\equations{
    \zeta_i 
    =
    \sum_{n_i = 0}^{\infty}
    e^{\beta (\mu - \epsilon_i) n_i}
    =
    \frac{1}{1 - e^{\beta (\mu - \epsilon_i)}}
    \\
    \langle N_i \rangle_{BE} 
    =
    \frac{1}{\beta}
    \frac{\del}{\del \mu}
    \ln(\zeta_i)
    =
    \frac{e^{\beta(\mu - \epsilon_i)}
    }{1 - e^{\beta (\mu - \epsilon_i)}}
    =
    \frac{1}{e^{\beta (\epsilon_i - \mu)} - 1}
    =
    f_{BE}(\epsilon_i, T)
}

\section{Summary}
\equations{
    f_{BE/FD}(\epsilon_i, T)
    =
    \frac{1}{e^{\beta (\epsilon_i - \mu)} \pm 1}
    \hp
    N 
    =
    \sum
    \frac{1}{e^{\beta (\epsilon_i - \mu)} \pm 1}
}

The BE distribution is infinite at $\epsilon_i = \mu$, and goes down exponentially. 

Maxwell Boltzmann is $\infty$ at $\epsilon_i = 0$ and goes down 
exponentially 

Fermi-Dirac is $1$ at $\epsilon_i = 0$ and goes down in a mellow step function. 

For $\epsilon_i >> \mu$ 
\equations{
    f_{BE} 
    \approx
    e^{-\beta (\epsilon_i - \mu)}
    \approx
    f_{FD} 
    \\
    \langle N_i \rangle_{MB}
    \frac{n}{n_Q }
    e^{-\beta \epsilon_i}
    =
    e^{- \beta (\epsilon_i - \mu)}
    \\
    \mu 
    =
    k_B T \ln(\frac{n}{n_Q})
}

\textbf{All distributions agree} for large energy.

\section{Discussion 7}
Quantum many particle wave functions. 

There are 2 types of particles: Bosons and Fermions. 

Bosons are $\psi = + \psi$ and fermions are $\psi = - \psi$, 
whatever the fuck that means. 

\equations{
    \psi(x, x_2)
    =
    \frac{1}{\sqrt{2}}
    \left(
        \psi_a(x_a)
        \psi_a(x_a)
        +
        \psi_a(x_b)
        \psi_b(x_a)
    \right)
    \begin{cases}
        = 0 : \textrm{Fermion (Pauli Exclusion Principle)}
        \\
        \neq 0 : \textrm{Boson}
    \end{cases}
}

\subsection{Spin-Statistics Theorem}
Bosons have integer spin $(0, 1, 2, \ldots)$

Fermions have half-integer spins $(1/2, 3/2, 5/2, \ldots)$

\section{Degenerate Fermi Gases}
Gases where we're near the limit $n \sim n_Q$ so multiple occupancy 
plays a role. We see them in white dwarves and neutron stars, and well 
as electrons in metals. 

\equations{
    n_Q 
    =
    \left(
        \frac{m k_B T}{2 \pi \hbar^2}
    \right)^{3/2}
}

If we consider $T_d$ (degeneracy) such that $n(T_d) = n_Q(T_d)$, then 
we can solve for $T_d$
\equations{
    n
    =
    \left(
        \frac{m k_B T_d}{2 \pi \hbar^2}
    \right)^{3/2}
    \Rightarrow 
    T_d 
    =
    \frac{2 \pi \hbar^2}{m k_B} n^{2/3}
    \hp 
    \frac{T_d}{T}
    =
    \left(
        \frac{n}{n_Q(T)}
    \right)^{2/3}
}

if $T < T_d$, then $n > n_Q(T)$, it is more dense, and quantum shenanigans 
needs to be considered. 
 
if $T > T_d$, then $n < n_Q(T)$, it is less dense.

If we consider a typical metal, $T_d = 50,000-100,000K$, so room temp 
is way less than that, and metals are considered degenerate (need to deal 
with quantum shenanigans). The Fermi-Dirac distribution of a metal $f_{FD}$ 
is close to a step function for $T << T_d$. 

\subsection{Sommerfeld Theory of Metals}
Assume electrons in the metal are a gas of free electrons (electrically neutral). 

In reality, electrons feel forces from nearby atoms, but ignore long range 
forces because metal are electrically neutral. 

\subsection{Chemical Potential}
\equations{
    N 
    \overset{!}{=}
    \sum_n \langle N(\epsilon_n, T) \rangle 
    =
    \sum_n f_{FD}(\epsilon_n, T)
    =
    \frac{1}{e^{\beta(\epsilon_n - \mu)} + 1}
    \\
    U(T)
    =
    \sum_n 
    \langle N(\epsilon_n, T) \rangle * \epsilon_n  
    =
    \sum_n f_{FD}(\epsilon_n, T) * \epsilon_n 
    =
    \sum_n
    \frac{\epsilon_n}{e^{\beta(\epsilon_n - \mu)}}
}
The $+1$ at the end was dropped in lecture idk why. 

\subsection{Particle in a Box Approach}
\equations{
    \epsilon_n 
    =
    \frac{\hbar^2 k_n^2}{2m}
    \hp
    k_n^2
    =
    k_x^2
    +
    k_y^2
    +
    k_z^2
    \hp 
    k_{x}
    =
    \frac{n_x \pi}{L_x}
    \\
    \Delta \epsilon
    =
    \frac{\hbar^2 \pi^2}{2m L^2}
    =
    \frac{\hbar^2 \pi^2}{2m V^{2/3}}
    \approx 
    10^{-15} eV 
    \Rightarrow 
    \sum_n 
    \to 
    \int_{0}^{\infty} d \epsilon
    \\
    \# States([\epsilon, \epsilon + d \epsilon])
    =
    \# spin * 
    \left(
        ??
    \right)
}

You do the big circle geometry thing for a shell of radius 
$k = \sqrt{\frac{2 m \epsilon}{\hbar^2}}$ and axes $k_x, k_y$.
Also know that are there 2 spins per electron. 
\equations{
    \# States([\epsilon, \epsilon + d \epsilon])
    =
    \# spin * 
    \left(
        \frac{\textrm{Volume of Shell}}{\textrm{Volume per state}}
    \right)
    =
    2
    \left(
         \frac{4 \pi k^2 \frac{dk}{8}}
        {\frac{\pi^3}{V}}
    \right)
    \\
    =
    \frac{V}{\pi^2} k^2 dk 
    =
    \frac{V}{\pi^2}
    \left(
        \frac{2m}{\hbar^2}
    \right)^{3/2}
    \sqrt{\epsilon}
    d \epsilon
    \hp 
    dk 
    =
    \sqrt{\frac{2m}{\hbar^2}}
    \frac{d \epsilon}{2 \sqrt{\epsilon}}
}

This is different from a photon gas where $\epsilon = \hbar \omega = \hbar c k$.

But now that we have all this shenanigans, we can calculate the density of 
states. 
\equations{
    N 
    =
    \sum_n \langle N(\epsilon_n, T) \rangle 
    =
    \int_{0}^{\infty} \, d \epsilon \, 
    D(\epsilon)
    f_{FD}(\epsilon_n, T)
    \\
    =
    \frac{V}{2 \pi^2}
    \left(
        \frac{2m}{\hbar^2}
    \right)^{3/2}
    \int_{0}^{\infty} \, d \epsilon \, 
    \frac{\sqrt{\epsilon}}{e^{\beta(\epsilon - \mu)} + 1}
    \\
    U
    =
    \sum_n \epsilon_n f_{FD}(\epsilon_n, T)
    =
    \int_{0}^{\infty} \, d \epsilon \, 
    D(\epsilon) * \epsilon * f_{FD}(\epsilon, T)
    \\
    =
    \frac{V}{2 \pi^2}
    \left(
        \frac{2m}{\hbar^2}
    \right)^{3/2}
    \int_{0}^{\infty} \, d \epsilon \, 
    \frac{\epsilon^{3/2}}
    {e^{\beta(\epsilon - \mu)} + 1}
}

These integrals cannot really be solve analytically, but we know that 
for $T \to 0$ (Which we can say is true because $T << T_D$), 
the FD distribution is a step function, which makes the integrals 
simple. 
\equations{
    f_{FD}
    \approx 
    \begin{cases}
        1 : \epsilon < \mu(0)
        \\
        0 : \epsilon > \mu(0)
    \end{cases}
    \Rightarrow 
    \\
    N 
    =
    \int_{0}^{\mu(0)} d \epsilon \, 
    D(\epsilon)
    * 1
    =
    \frac{V}{2 \pi^2}
    \left(
        \frac{2m}{\hbar^2}
    \right)^{3/2}
    \int_{0}^{\mu(0)} d \epsilon \, 
    \sqrt{\epsilon}
    =
    \frac{V}{2 \pi^2}
    \left(
        \frac{2m}{\hbar^2}
    \right)^{3/2}
    \frac{2}{3}
    (\epsilon_F)^{3/2}
}

We can use this to solve for chemical potential 
\equations{
    \epsilon_F 
    =
    \mu(0)
    =
    \frac{\hbar^2}{2m}
    \left(
        3 \pi^2 \frac{N}{V}
    \right)^{2/3}
    =
    \textrm{Fermi Energy}
    \propto 
    n^{2/3}
}

\subsection{Fermi Energy}

The Fermi Energy is the highest energy 
level that is occupied by electrons at $T=0$. Because 
the Fermi energy is proportional to density, it is an \textbf{intensive}
quantity.

We can also calculate the energy 
\equations{
    U 
    =
    \sum_n \epsilon_n f_{FD}(\epsilon_n, T)
    =
    \int_{0}^{\mu(0)} \, d \epsilon \, 
    D(\epsilon) * \epsilon * 1
    =
    \frac{V}{2 \pi^2}
    \left(
        \frac{2m}{\hbar^2}
    \right)^{3/2}
    \int_{0}^{\mu(0)} \, d \epsilon \, 
    \epsilon^{3/2}
    \\
    =
    \frac{2}{5}
    \frac{V}{2 \pi^2}
    \left(
        \frac{2m}{\hbar^2}
    \right)^{3/2}
    \epsilon_{F}^{5/2}
    =
    \frac{2}{5}
    \frac{3}{2}
    \left(
    \frac{2}{3}
    \frac{V}{2 \pi^2}
    \left(
        \frac{2m}{\hbar^2}
    \right)^{3/2}
    \epsilon_{F}^{3/2}
    \right)
    \epsilon_{F}
    =
    \frac{3}{5}
    N 
    \epsilon_F
}

For very low $T$, $U \neq 0$ (Lecture 15 and Pauli Exclusion Principle).

\subsection{Fermi Temperature}
\equations{
    T_F 
    \overset{!}{=}
    \frac{\epsilon_F}{k_B}
    \hp 
    T_F \approx T_D 
    \Rightarrow 
    k_B T_D 
    =
    \frac{2 \pi \hbar^2}{m}
    n^{2/3}
    \approx 
    (3 n^2)^{2/3}
    \frac{\hbar^2}{2m}
    n^{2/3}
    =
    \epsilon_F
    \\
    T_F =
    50,000-100,000
    \Rightarrow 
    k_B T_{normal}
    << 
    \epsilon_F
}

\subsection{Fermi Pressure}
\equations{
    p(T)
    =
    - 
    \left(
        \frac{\del F}{\del V}
    \right)_{T, N}
    \hp 
    T = 0 
    \hp 
    F = U 
    \hp 
    p(0)
    =
    - 
    \left(
        \frac{\del U}{\del V}
    \right)_{T, N}
    \\
    U(T \to 0)
    =
    \frac{3}{5}
    N \epsilon_F 
    =
    \frac{3}{5}
    N
    \frac{\hbar^2}{2m}
    \left(
        3 \pi^2 
        \frac{N}{V}
    \right)^{2/3}
    \hp 
    \epsilon_F 
    =
    \frac{\hbar^2}{2m}
    \left(
        3 \pi^2 
        \frac{N}{V}
    \right)^{2/3}
    \\
    U 
    =
    \frac{3}{5}
    (3 \pi^2)^{2/3}
    \frac{\hbar^2}{2m}
    \frac{N^{5/3}}{V^{2/3}}
    \Rightarrow 
    p(0)
    =
    \frac{2}{3}
    \alpha 
    \frac{N^{5/3}}{V^{5/3}}
    =
    \frac{2}{3}
    \frac{U(0)}{V}
}
There's something weird with 5/3 and 2/3 here but I can't figure out what 

\subsection{Classical Ideal Gas}
\equations{
    pV = N k_B T 
    \Rightarrow 
    p 
    =
    \frac{2}{3}
    \frac{U}{V}
    \hp 
    U_{I.G.}
    =
    N * \frac{3}{2} k_B T 
    \hp 
    T \to 0 : U \to 0
}
So our Fermi Pressure actually matches the Classical interpretation of pressure. 

There's some shenanigans with degeneracy pressure $p = - \frac{\del U}{\del V}$ 
but I can't see the blackboard. Also $\Delta \epsilon \sim 1/ V^{2/3}$

\subsection{Degeneracy Pressure}
\equations{
    p \sim 10^{9} \frac{N}{m^2}
}

\section{Heat Capacity of Metals}
We discussed how 
\equations{
    C_V
    =
    \gamma T 
    + 
    A T^3
}

$AT^3$ comes from photons, and $\gamma T$ comes from the conduction of 
electrons. At small $T > 0$, only electrons within $k_B T$ of $\epsilon_F$ 
can move to an excited state (Because for a less-energetic electron, all the 
nearby states above it are already filled up by other electrons). 

Only a fraction of electrons $\sim \frac{k_B T}{\epsilon_F}$
\equations{
    N \frac{k_B T}{\epsilon_F} k_B T
    =
    N \frac{k_B^2 T^2}{\epsilon_F} k_B T
}

Take the derivative and get $C_V \propto T$
    
\section{Degenerate Fermi Gas Derivation}
Consider a gas where $n > n_Q$, $T < T_d ~ 10^5 K$. 
We know that 
\equations{
    N = \int^{\infty}_{0} \, d \epsilon \,
     D(\epsilon) f_{FD}(\epsilon, T)
    \hp 
    U = 
    \int^{\infty}_{0} d \epsilon \, 
    \epsilon
    D(\epsilon) f_{FD}(\epsilon, T)
    \\
    \epsilon_F 
    =
    \mu(0)
    \hp 
    U(0), 
    T_F 
    =
    \frac{\epsilon_F}{k_B}
    \approx 
    T_d
}

\subsection{Sommerfeld Expansion}
\equations{
    \mu(T)
    =
    \mu(0)
    +
    O(T^2)
    \hp
    U(T)
    =
    U(0)
    +
    O(T^2)
    =
    \frac{3}{5}
    N \epsilon_F
    +
    O(T^2)
}

\subsection{Math}
\equations{
    I 
    =
    \int^{\infty}_{0} d \epsilon \, 
    \epsilon^n D(\epsilon) f(\epsilon, T)
    :
    n=
    \hp 
    \begin{cases}
        0 : I = N \\
        1 : I = U 
    \end{cases}
}
We can treat $f(\epsilon, T)$ as a step function and define 
$\phi = \epsilon^n D(\epsilon)$.

\equations{
    \psi(\epsilon)
    =
    \int^{\epsilon}_{0} d \epsilon' \, 
    \phi(\epsilon')
    \Rightarrow 
    \phi(\epsilon)
    =
    \frac{d \psi}{d \epsilon}
    \\
    I 
    =
    \int^{\infty}_{0} d \epsilon \, 
    f(\epsilon, T) \phi(\epsilon)
    =
    \int^{\infty}_{0} d \epsilon \, 
    f(\epsilon, T)
    \frac{d \psi}{d \epsilon}
    \hp 
    \textrm{by parts}
    \\
    =
    f(\epsilon, T)
    \psi(\epsilon)
    \Big|^{\infty}_{0}
    -
    \int^{\infty}_{0} d \epsilon \, 
    \frac{d f(\epsilon, T)}{d \epsilon}
    \psi(\epsilon)
    =
    0
    -
    \int^{\infty}_{0} d \epsilon \, 
    \frac{d f(\epsilon, T)}{d \epsilon}
    \psi(\epsilon)
    \\
    I 
    =
    -
    \int^{\infty}_{0} d \epsilon \, 
    \frac{d f(\epsilon, T)}{d \epsilon}
    \psi(\epsilon)
}
Because $f(\epsilon, T)$ is functionally a step function, its roughly 0 everywhere,
and then very large at the step itself 
\equations{
    -\frac{d f(\epsilon, T)}{d \epsilon}
    =
    \frac{\beta e^{\beta (\epsilon - \mu)}}
    {e^{\beta (\epsilon - \mu)} + 1}
}

We can do a taylor expansion around $\epsilon = \mu$ 
to get function as a sum. 
\equations{
    \psi(\epsilon)
    =
    % \phi(\mu)
    % + 
    % \phi'(\mu)
    % (\epsilon - \mu)
    % +
    % \frac{1}{2}
    % \psi''(\mu)
    % (\epsilon - \mu)^2
    % +
    % \ldots
    % =
    \sum_{m = 0}^{\infty}
    \frac{1}{m!}
    \psi^{(m)}(\mu)
    (\epsilon - \mu)^m
    \\
    I 
    =
    -
    \sum_{m = 0}^{\infty}
    \frac{1}{m!}
    \psi^{(m)}(\mu)
    \int^{\infty}_{0} d \epsilon \, 
    \frac{d f(\epsilon, T)}{d \epsilon}
    (\epsilon - \mu)^m
    \\
    =
    \sum_{m = 0}^{\infty}
    \frac{1}{m!}
    \psi^{(m)}(\mu)
    \int^{\infty}_{0} d \epsilon \, 
    \frac{\beta e^{\beta (\epsilon - \mu)}}
    {e^{\beta (\epsilon - \mu)} + 1}
    (\epsilon - \mu)^m
    \hp 
    x 
    =
    \beta (\epsilon - \mu)
    \\
    =
    \sum_{m = 0}^{\infty}
    \frac{1}{m!}
    \psi^{(m)}(\mu)
    *
    \beta^{-m}
    \int^{\infty}_{- \beta \mu} d x \, 
    \frac{e^{x} x^m}
    {(e^{x} + 1)^2}
}

We do some more math 
\equations{
    \mu \approx 
    \epsilon_F 
    =
    k_B T 
    : 
    T << T_F 
    \hp
    \beta \mu 
    =
    \frac{\mu}{k_B T}
    \approx 
    \frac{T_F}{T}
    >> 1 
    \Rightarrow 
    \lim_{x \to \infty} e^{-x} \to 0
    \\
    \approx 
    I 
    =
    \sum_{m = 0}^{\infty}
    \frac{1}{m!}
    \psi^{(m)}(\mu)
    *
    \beta^{-m}
    \int^{\infty}_{-\infty} d x \, 
    \frac{e^{x} x^m}
    {(e^{x} + 1)^2}
}

Do a lowest order term $(m = 0$ because we already did a taylor series)
\equations{
    I_0
    =
    \int^{\infty}_{-\infty} d x \, 
    \frac{e^{x} x^m}
    {(e^{x} + 1)^2}
    = 
    \frac{-1}{(e^{x} + 1)}
    \Big|^{\infty}_{-\infty}
    =
    1
}

Let's try all odd $m$ 
\equations{
    I_{odd}
    =
    \frac{x^m e^{x}}{(e^{x} + 1)^2}
    =
    \frac{x^m}{(e^{x} + 1) (e^{x} + 1) e^{-x}}
    =
    \frac{x^m}{(e^{x} + 1) (1 + e^{-x})}
    \\
    \int^{\infty}_{-\infty} dx \,
    \frac{x^m}{(e^{x} + 1) (1 + e^{-x})}
    =
    0
}

This is true because its an odd function for all odd $m$. Now 
let's try to find $I_m$ for even $m$ 
\equations{
    I_2 
    =
    \int^{\infty}_{-\infty} dx \,
    \frac{x^2 e^{x}}{(e^{x} + 1)^2}
    =
    \ldots 
    =
    2 \zeta(2)
    =
    \frac{\pi^3}{3}
}

We don't need to worry about the higher terms because they're all small 
and vanish. Now we put everything back together to get 
\equations{
    \int^{\infty}_{0} d \epsilon \, 
    f(\epsilon, T) 
    \phi(\epsilon)
    =
    \sum_{m \textrm{ even}}^{\infty}
    \frac{1}{m!}
    \psi^{(m)}(\mu)
    \beta^{-m}
    I_m
    % \int^{\infty}_{-\infty} d x \, 
    % \frac{e^{x} x^m}
    % {(e^{x} + 1)^2}
    \\
    =
    \psi(\mu)
    \cdot 1
    +
    \frac{\pi^2}{6}
    (k_B T)^2 
    \psi''(\mu)
    +
    \ldots
}

That is just the $m=0$ and $m=2$ term. 
And now we back to solving for $\psi$
\equations{
    \psi(\epsilon)
    =
    \int^{\epsilon}_{0} d \epsilon'
    \phi(\epsilon')
    =
    \int^{\epsilon}_{0} d \epsilon'
    (\epsilon')^n D(\epsilon')
    \\
    I 
    =
    \psi(\mu)
    \cdot 1
    +
    \frac{\pi^2}{6}
    (k_B T)^2 
    \psi''(\mu)
    +
    \ldots
    =
    \int^{\mu}_{0} d \epsilon \,
    \phi(\epsilon)
    +
    \frac{\pi^2}{6}
    (k_B T)^2 
    \phi'(\mu)
    +
    \ldots
}

\equations{
    \phi 
    =
    \epsilon^n D(\epsilon)
    =
    \begin{cases}
        n = 0 
        \Rightarrow 
        \int 
        \overset{!}{=}
        N 
        \\
        n = 1 
        \Rightarrow 
        U(\epsilon)
    \end{cases}
}

Let's start with $n=0$ 
\equations{
    \phi(\epsilon) = D(\epsilon)
    \Rightarrow 
    I_{n=0}
    =
    \int^{\infty}_{-\infty}
    f(\epsilon, T) D(\epsilon)
    \overset{!}{=}
    N
    \\
    =
    \int^{\mu}_{0} d \epsilon \,
    D(\epsilon)
    +
    \frac{\pi^2}{6}
    (k_B T)^2 
    D'(\epsilon)
    +
    \ldots
    \hp
    \int^{\mu}_{0} d \epsilon \,
    =
    \int^{\epsilon_F}_{0} d \epsilon \,
    +
    \int^{\mu}_{\epsilon_F} d \epsilon \,
    \\
    I_{n=0}
    =
    N 
    =
    \int^{\epsilon_F}_{0} d \epsilon \,
    D(\epsilon)
    +
    \int^{\mu}_{\epsilon_F} d \epsilon \,
    D(\epsilon)
    +
    \frac{\pi^2}{6}
    (k_B T)^2 
    D'(\epsilon)
    \Rightarrow 
    \\
    N 
    =
    N 
    +
    D(\epsilon_F) (\mu - \epsilon_F)
    +
    \frac{\pi^2}{6}
    (k_B T)^2 
    D'(\epsilon)
    \Rightarrow 
    \\
    0
    =
    D(\epsilon_F) (\mu - \epsilon_F)
    +
    \frac{\pi^2}{6}
    (k_B T)^2 
    D'(\epsilon)
    \hp 
    \mu 
    \approx 
    \epsilon_F 
}

Now we solve for $\mu$ 
\equations{
    \mu(T)
    \approx 
    \epsilon_F 
    -
    \frac{\pi^2}{6}
    (k_B T)^2 
    \frac{D'(\epsilon)}{D(\epsilon)}
    \\
    D(\epsilon_F)
    =
    \frac{V}{2 \pi^2}
    \left(
    \frac{2m}{\hbar^2}
    \right)^{3/2}
    \epsilon_F^{1/2}
    \hp
    D'(\epsilon_F)
    =
    \frac{V}{2 \pi^2}
    \left(
    \frac{2m}{\hbar^2}
    \right)^{3/2}
    \frac{1}{2 \epsilon^{1/2}}
    =
    \frac{D(\epsilon_F)}{2 \epsilon_F}
    \\
    \mu(T)
    \approx 
    \epsilon_F 
    - 
    \frac{\pi^2}{6}
    (k_B T)^2 
    \frac{1}{2 \epsilon_F}
    =
    \epsilon_F 
    - 
    \frac{\pi^2}{12}
    \left(
    \frac{k_B T}{\epsilon_F}
    \right)
    k_B T
}

\subsection{$U$}
Now we can calculate $U$ 
\equations{
    U(T)
    =
    \int^{\infty}_{0} d \epsilon \, 
    \epsilon D(\epsilon) f(\epsilon, T)
    =
    \int^{\mu}_{0} d \epsilon \, 
    \epsilon D(\epsilon)
    +
    \frac{\pi^2}{6}
    (k_B T)^2 
    \frac{d}{d \epsilon}
    \left(
        \epsilon D(\epsilon)
    \right)
    \Big|_{\epsilon_F}
    \\
    =
    \int^{\epsilon_F}_{0} d \epsilon \, 
    \epsilon D(\epsilon)
    +
    \int^{\mu}_{\epsilon_F} d \epsilon \, 
    \epsilon D(\epsilon)
    +
    \frac{\pi^2}{6}
    (k_B T)^2 
    \left(
    D(\mu)
    +
    \mu 
    D'(\mu)
    \right)
    \\
    =
    \frac{3}{5} N \epsilon_F
    +
    \epsilon_F D(\epsilon_F)
    \left(
        \mu - \epsilon_F
    \right)
    +
    \frac{\pi^2}{6}
    (k_B T)^2 
    \left(
    D(\mu)
    +
    \mu 
    D'(\mu)
    \right)
}

plug in $\mu(T)$ and use $D'(\epsilon_F) = D'(\epsilon_F)/2 \epsilon_F$ 
and $\mu = \epsilon_F - \frac{\pi^2}{12} (\frac{k_B T}{\epsilon_F}) k_B T$
\equations{
    U(T)
    =
    U(0)
    -
    \frac{\pi^2}{12} (\frac{(k_B T)^2}{\epsilon_F}) \epsilon_F 
    D(\epsilon_F)
    +
    \frac{\pi^2}{6} (k_B T)^2
    D(\epsilon_F)
    +
    \frac{\pi^2}{6} (k_B T)^2
    \epsilon_F
    \frac{D(\epsilon_F)}{2 \epsilon_F}
    \\
    =
    U(0)
    -
    \frac{\pi^2}{6} (k_B T)^2
    D(\epsilon_F)
    =
    U(T = 0)
    -
    \frac{\pi^2}{4} N k_B T
    \left(
        \frac{k_B T}{\epsilon_F}
    \right)
    +
    \ldots
}

\subsection{$C_v$}
We can find the specific heat capacity
\equations{
    C_V 
    =
    \frac{\pi^2}{2}
    N k_B
    \frac{k_B}{\epsilon_F}
    =
    \gamma T 
}

\section{Discussion (Didn't Write Anything)}

\section{Bose Einstein Condensante (Eventually) or Band Gaps idk}
\subsection{Particles in a Box}
You have a bunch of particles with energy $\epsilon(k) = \frac{|hbar^2 k^2}{2m}$

Metals have a band gap. Bascially the electrons are in a little energy parabola
under the Fermi energy. 

\subsection{Bose-Einstein Condensate}
Porfessor DeMarco's group. When $n \sim n_Q$, interesting things happen. 
If you bring particles to a temperature such 
that $k_B T < \epsilon_1 - \epsilon_0$, then every particle will be in the 
ground state necessarily. However, we found out that in the Bose-Einstein 
Distribution, we can actually have $k_B T > \epsilon_1 - \epsilon_0$.

The B-E Distribution, we have 
\equations{
    \langle N(\epsilon, T) \rangle_{BE} 
    =
    f_{BE}(\epsilon, T)
    =
    \frac{1}{e^{\beta (\epsilon - \mu)} - 1}
    \\
    N 
    =
    \sum
    \langle N(\epsilon, T) \rangle_{BE} 
    =
    \sum
    \frac{1}{e^{\beta (\epsilon - \mu)} - 1}
    \Rightarrow 
    \textrm{ get $\mu$}
    \\
    \lim_{T \to 0}
    \langle N(\epsilon, T) \rangle_{BE} 
    =
    N 
    =
    \frac{1}{e^{\beta (\epsilon - \mu)} - 1}
    \hp 
    \lim_{T \to 0}
    \mu(T)
    \approx 
    \epsilon_0 
    - 
    \frac{k_B T}{N}
}

Given $V = 1cm, N = 10^{22} 4He, m = 6.6 \cdot 10^{-27}, T = 1mK$, we 
get an energy difference of $\epsilon_1 - \epsilon_0 \approx 1.5 \cdot 10^{18} eV$. 
We see that $\mu = \epsilon_0 - \frac{k_B T}{N} = \epsilon_0 - 10^{-29}$, so 
it's functionally extremely close to the ground state. We see that 
$e^{\beta (\epsilon_0 - \mu)}$ is much close to 1 than 
$e^{\beta (\epsilon_1 - \mu)}$.

\subsection{Maxwell Boltzmann (Incorrent)}
\equations{
    \frac{\langle N(\epsilon_1, T) \rangle_{MB}}
    {\langle N(\epsilon_0, T) \rangle_{MB}}
    =
    e^{- \beta \Delta \epsilon}
    =
    e^{- 1.8 * 10^{-11}}
    \approx 
    1
}
The maxwell boltzmann distribution tells us that there is an even 
amount of particles in the ground state as the 1st excited state, which is 
experimentally incorrect. 

\subsection{Bose-Einstein Statistics}
\equations{
    \langle N(\epsilon_1, T) \rangle_{BE}
    =
    \frac{1}{e^{-\beta (\epsilon_1 - \mu)} - 1}
    =
    \frac{1}{e^{1.8 \cdot 10^{-11}} - 1}
    \approx 
    \frac{1}{ 1 + 1.8 \cdot 10^{-11} - 1 }
    \approx 
    5 \cdot 10^{10}
    \\
    \frac{ \langle N(\epsilon_1, T) \rangle_{BE} }
    { \langle N(\epsilon_0, T) \rangle_{BE} } 
    =
    \frac{5 \cdot 10^{10}}{10^{22}}
    =
    5 \cdot 10^{-12}
}
The Bose-Einstein statistics show us that the ground state is far far more 
occupied than the 1st excited state, which is what we expect experimentally 
even though $k_B T >> \Delta \epsilon$.
This is called Bose-Einstein Condensation. At what temperature do we actually 
start seeing this condensation? 

\subsection{Maximum $T$ For Condensation}
Apparently it can't really be proved analytically, but we can represent 
it graphically. 

\equations{
    N 
    =
    \langle N_0(T) \rangle_{BE}
    +
    \langle N_{\epsilon}(T) \rangle_{BE}
    \hp
    \langle N_{\epsilon}(T) \rangle_{BE}
    =
    \int^{\infty}_{0} d \epsilon \, 
    D(\epsilon) f(\epsilon, T)
    \\
    =
    \frac{V}{4 \pi^2}
    \left(
        \frac{2m}{\hbar^2}
    \right)^{3/2}
    \int^{\infty}_{0} d \epsilon \, 
    \frac{\sqrt{\epsilon}}{e^{-\beta (\epsilon - \mu)} - 1}
    \equiv 
    F(\mu)
}

$F(\mu)$ tells you for what $\mu$ and $T$ is $N_\epsilon$ on the order of $N$ 
(i think I'm not sure at all). $T_2$ is the critical temperature such that 
\equations{
    N(T \leq T_{E})
    =
    2.612 n_Q(T)
    \hp
    \langle N(T = T_{E}) \rangle 
    \overset{!}{=}
    N
    = 
    2.612 n_Q(T_E) V
    \\
    \frac{\langle N_0(\epsilon_0) \rangle}{N}
    =
    1
    -
    \left(
        \frac{T}{T_E}
    \right)^{3/2}
}

\chapter{Phase Transformations}
Remeber the following identities from past lectures (11). 
\equations{
    G(T, p)
    =
    U + pV - TS = H - TS 
    \hp 
    G 
    =
    \sum_{i = 1}^{n}
    N_i \mu_i(T, p)
    \\
    dG 
    =
    -S dT 
    +
    V dp 
    +
    \sum_{i = 1}^{n}
    \mu_i(T, p) \, d N_i 
}

There are many types of phase transitions in stat mech beyond the normal ones 
\begin{itemize}
    \item
    gas/liquid/solid/plasma
    \item
    superfluid/normal fluid
    \item
    superconductor
    \item
    crystallographic phases in solids
    \item
    paramagnetic/ferromagnetic
\end{itemize}

\subsection{Phase Diagrams}
You've seen them. They show at what $p, T$ are materials liquid, solid, gas. 
The critical point is the point where liquid and gas are indisinguishable.

\section{Coexistence Curves}
These are the boundaries on the phase diagram, and at these boundaries, 
2 phases can exist in equilibrium at the same time. Gibbs Free Energy is 
minimized in equilibrium, so 
\equations{
    G 
    =
    \sum_{i=1}^{2}
    N_i \mu_i 
    =
    N_1 \mu_1 
    +
    N_2 \mu_2 
    \\
    dG 
    =
    0
    =
    -S dT  + V dp + 
    \sum_{i=1}^{2}
    N_i \mu_i 
    \Rightarrow 
     \mu_1 dN_1
    =
    -
    \mu_2 dN_2  
    \Rightarrow 
    \\
    (\mu_1 - \mu_2) dN_1 
    =
    0
    \Rightarrow 
    \mu_1 = \mu_2
}
So we are in diffusive equilibrium when Gibbs free energy is minimized 
at a phase transition (neither phase is moving to the other).

\section{TOO BUSY REGISTERING FOR CLASSES TO PAY ATTENTION}
let $v$ is volume per particle $V/N$. 

The liquid-gas boundary is equation
\equations{
    \left(
        \frac{dp}{dt}
    \right)_{l \iff g}
    =
    \frac{s_g - s_l}{v_g - v_l}
    > 0
}

The solid-liquid phase boundary is 
\equations{
    \left(
        \frac{dp}{dt}
    \right)_{s \iff l}
    =
    \frac{s_l - s_s}{v_l - v_s}
    < 
    0
}

\section{Latent Heat}
\equations{
    h_2 - h_1 
    =
    T (s_2 - s_1)
    =
    T \Delta S 
    =
    L
}

The specific heat capacity can be written in terms of enthalpy 
\equations{
    C_p 
    =
    T
    \left(
        \frac{\del S}{\del T}
    \right)_{p}
    =
    \left(
        \frac{\del U}{\del T}
    \right)_{p}
    +
    p 
    \left(
        \frac{\del V}{\del T}
    \right)_p 
    \\
    =
    \left(
        \frac{\del}{\del T}
        \left(
            U 
            +
            pV
        \right)
    \right)_{p}
    =
    \left(
        \frac{\del H}{\del T}
    \right)_p
    \Rightarrow 
    H 
    =
    \int C_p(T) dT 
    \hp 
    \Delta H 
    =
    \int 
    (C_{p2} - C_{p1}) dT
}

\section{Clausius-Clapeyron}

I think this is right but I'm not sure because it was first brought up 
when I was registering for classes
\equations{
    \left(
        \frac{dp}{dT}
    \right)
    =
    \frac{\Delta s}{\Delta v}
    \Rightarrow 
    \frac{L}{T \Delta v}
    =
    \left(
        \frac{dp}{dT}
    \right)_{\textrm{Co-lx curve}}
}

\subsection{Vapor Pressure Equation}
\equations{
    p(T)
    =
    p_0 
    e^{-L / k_B T}
}

\chapter{Van der Waals Model} 
Consider an ideal gas such that $pV = n k_B T$. If we were to model pressure 
and volume with everything else constant, it would be inversely proportional. 

However, what actually happens is that $pV = N k_B T$ until there is a phase 
transition, and then pressure stays constant and V decreases as a liquid, and 
pressure increases dramatically as a solid. 

\section{Non-Ideal Gas (With Interactions)}
\equations{
    \epsilon_{tot} 
    =
    \sum_{i=1}^{N} 
    \frac{p_i^2}{2m} 
    +
    \sum_{pairs} 
    \phi(\vec r_i - \vec r_j)
}

Because each of pair of particles interact with each other in a unique way, so 
we need to consider every interaction between every set of particles.

There are $N(N-1)/2$ terms ($N$ choose $2$). 
\equations{
    \sum_{pairs}
    =
    \sum_{i=1}^{N}
    \sum_{j=i+1}^{N}
    =
    \frac{1}{2}
    \underset{i \neq j}{
	\sum_{i=1}^{N}
	\sum_{j=1}^{N}
    }
}

We can use this to find the partition function. The classical partition function 
for a non-ideal gas is 
\equations{
    Z_{tot}
    =
    \frac{1}{h^{3N}}
    \frac{1}{N!}
    \int d^3 p_1 
    \ldots
    \int d^3 p_N
    \int d^3 r_1 
    \ldots
    \int d^3 r_N
    \Rightarrow 
    \\
    \exp(- \beta 
    \left(
    \sum_{i} \frac{p_i^2}{2m} 
    +
    \frac{1}{2}
    \sum_{i, j, i \neq j} 
    \phi_{ij}
    \right)
    )
}

You can pull out a factor
\equations{
    \frac{1}{N!}
    \int 
    \frac{d^3 p_i}{(2 \pi \hbar)^3}
    e^{-\beta \frac{p_i^2}{2m}}
}

Which then gives us 
\equations{
    \frac{1}{N!}
    \left[
	\left(
	    \int 
	    \frac{d^3 p_i}{(2 \pi \hbar)^3}
	    e^{-\beta \frac{p_i^2}{2m}}
	\right)^N
	*
	\int d^3 r_1 
	\ldots
	\int d^3 r_n 
	\exp(
	    -\frac{\beta}{2}
	    \sum_{i \neq j}
	    \phi(\vec r_i - \vec r_j)
	)
    \right]
}

If we consider an ideal gas ($\phi = 0$), then we see that this equation 
returns $V^N$, which is what we expect. 

\section{Mean Field Theory} 
Instead of considering every interaction across every particle everywhere, you 
can just consider the average energy of the interactions and use that for your 
calculations. 

This assumptions allows our energy calculation to be written as 
\equations{
    \epsilon_i 
    =
    \frac{p_i^2}{2m}
    +
    \langle 
    \frac{1}{2} 
    \sum_{i \neq j} 
    \phi(|\vec r_i - \vec r_j|)
    \rangle
    =
    \frac{p_i^2}{2m}
    +
    U_{eff}
    \\
    Z_{tot}
    =
    \frac{1}{N!}
    \left(
	n_Q(T) 
	\cdot 
	\int d^3 \vec R \,
	e^{-\beta U_{eff}}
    \right)^N
}

$U_{eff}$ looks like the orbitals that we observed in classical mechanics where there's a 
nice potential well but it increases dramatically as we get too close to $R_0$ 
and it increases from negative to 0 as $R \to \infty$. 
\equations{
    \phi(R < R_0) \to \infty
    \hp
    \phi(R \to \infty) \to 0 
    \hp
    \phi(R > R_0) < 0
}

Now we have to solve that integral. 
\equations{
    U_{eff} \to \infty 
    \Rightarrow 
    e^{- \beta U_{eff}} \to 0
}

This means that the volume integral will only be non-zero in the volume $V - V_{excl}$
where $V_{excluded} = (N-1)b \approx Nb$ where $b$ is the volume of 1 particle. 

\subsection{"Smooth Potential" Approximation} 
Let's replace $U_{eff}$ by the average of it over space (assuming uniform particle 
density), this allows us to get 
\equations{
    U_{eff} 
    =
    \langle 
    \frac{1}{2}
    \sum_{i \neq j} 
    \phi(|\vec r_i - \vec r_j|)
    \rangle 
    =
    \frac{N - 1}{2} 
    \langle \phi \rangle 
    \approx 
    \frac{N}{2}
    \langle \phi \rangle 
    \approx 
    -\frac{Na}{V}
    \\
    \langle \phi \rangle 
    =
    - \frac{1}{V}
    \int^{\infty}_{R0}
    4 \pi R^2 dr |\phi(R)|
    =
    -\frac{2a}{V} 
    \hp 
    R_0 
    =
    2a
}

$a$ is the information of the spatial dependence of $\phi$ (attractiveness). 
Now our energy can be written purely in terms of two information bearing variables 
$a$ and $b$. 

\equations{
    Z_{tot}
    =
    \frac{1}{N!} 
    \left(
	n_Q
	\cdot 
	(V - Nb)
	e^{+\beta \frac{Na}{V}}
    \right)^N
    \Rightarrow 
    F_{vdW}
    =
    - k_B T \ln(Z_{tot}) 
    \\
    =
    -N k_B T 
    \ln(n_Q (V - Nb))
    +
    k_B T \ln(N!) 
    -
    \frac{N^2 a}{V}
}

VdW is Van der Waals

You use Stirling's Approximation and simplify terms to get 
\equations{
    F_{VdW}
    =
    -N k_B T 
    \left[
	\ln(\frac{n_Q (V - Nb)}{N})
	+
	1
    \right]
    -
    \frac{N^2a}{V}
}

This is the free energy of the ideal gas, expect $V \to V - Nb$ and 
add a $\left( \frac{- N^2 a}{V} \right)$ attraction term.

\equations{
    p 
    =
    - 
    \frac{dF_{vdw}}{dV} 
    =
    \frac{N k_B T}{V - Nb} 
    -
    \frac{N^2a}{V^2}
}

Set $a$ and $b$ to 0 to get the ideal gas law. 

Typical values of $a$ and $b$ are usually 
a = 2.5 eV \AA 
\hp
b = (4 \textrm{\AA})$^3$

In terms of the graphical difference the VDW forces usually add a saddle 
point and a little bump. What actually happens is that there is a phase transition
that completely ignores the bump as the gas turns into a liquid. 

At critical temperature $T_c$, we get 
\equations{
    \left(
    \frac{\del p}{\del V} 
    \right)_{T, N}
    =
    0 
    \hp 
    \left(
    \frac{\del^2 p}{\del V^2} 
    \right)_{T, N}
    =
    0 
}

This is graphically represented by the inflection point. 
The critical point is when $p = p_c, V = V_c, T= T_c$, each of which 
are functions of $a$ and $b$. $a$ and $b$ are material dependent parameters. 

You can normalize $p, V, T$ to $\hat p, \hat V, \hat T$ to remove $a$ and $b$ 
to get the Law of Corresponding States 

\subsection{Law of Corresponding States} 
\equations{
    \left(
    \hat p 
    +
    \frac{3}{\hat V^2} 
    \right)
    \left(
    3 \hat V 
    - 
    1
    \right)
    =
    8 \hat T 
}

This equation works well for noble gases because their VDW forces are relatively small. 

\section{Maxwell Construction}
We can finally understand phase changes with non-ideal gases. 

We can graphically represent the Gibbs Free Energy for constant $N, T$ 
\equations{
    dG 
    =
    -S dT 
    +
    V dp 
    +
    \mu dN 
    =
    V dp 
}

Graphing G over p given increasing $V$ gives you a loop, and because equilibrium is when 
$G$ is minimized, the loop is ignored and $V$ actually jumps during equilibrium 
(phase change). 

\section{Weiss Model of ferromagnetism}
Consider a spin 1/2 electron in a magnetic field of $\vec B$. 
\equations{
    \epsilon
    =
    - \mu_B B s 
    \hp 
    s 
    =
    \pm 1
}

\subsection{Single Spin}
\equations{
    z 
    =
    e^{\mu_B B / k_B T}
    +
    e^{-\mu_B B / k_B T}
    =
    2 \cosh(\frac{\mu_B B}{k_B T})
    \\
    \langle s \rangle 
    =
    \tanh(\frac{\mu_B B}{k_B T})
}

\subsection{Many Spin}
Consider a material with $n = N / V$ density of spins. 

The magnetization can be written as 
\equations{
    M(B, T)
    =
    n \mu_B \langle s \rangle 
    =
    n \mu_B
    \tanh(\frac{\mu_B B}{k_B T})
}

If we consider this for a very small magnetic field $B$ such that 
$\tanh(x) \approx x$, we get 
\equations{
    M(B, T)
    =
    n \mu_B^2
    \frac{B}{k_B T}
}

\subsection{Curie Law}
The magnetic susceptibility can be written as 
\equations{
    \chi(B, T)
    =
    \left(
        \frac{\del M}{\del B}
    \right)_{B=0}
    =
    \frac{n \mu_B^2}{k_B T}
    \sim 
    \frac{1}{T}
}

This is true for a paramagnet with no interactions. 

In ferromagnetism, we have $M \neq 0$ even when $B = 0$ 

\subsection{Exchange Interaction}
Consider a pair of electron spins $i, j$.
\equations{
    \epsilon_{each}
    =
    -2 J_{ij} s_i s_j
    \Rightarrow 
    \epsilon_{total}
    =
    -
    \sum_{i}
    \mu_B B s_i
    -
    \frac{1}{2}
    \sum_{i=1}^{N}
    \sum_{j=1}^{N}
    2 J_{ij} s_i s_j
}

\subsection{Ising Model}

Instead of considering every single possible particle pair, we can consider 
only nearest neighbor interactions to get a similar effect. 
\equations{
    \epsilon_{total}
    =
    -
    \sum_{i}
    \mu_B B s_i
    -
    J
    \sum_{i, j = NN}
    s_i s_j
}

This is known as the Ising Model, and it is very useful. It is only analytically 
solvable is 1D and 2D if we assume $B=0$. In 3D it's impossible. 

\subsection{Mean Field Theory}
Replace the interaction term with the average over all spins. 
\equations{
    \epsilon_i 
    =
    - \mu_B B s_i 
    -
    J s \sum_{j \neq i} s'_i
    \approx 
    - \mu_B B s_i 
    -
    J s \sum_{j \neq i} \langle s_i \rangle 
    =
    - \mu_B B s_i 
    -
    J s \langle s_i \rangle N_{nn}
    \\
    =
    - \mu_B B s_i 
    -
    \mu_B B_{eff} s_i 
    =
    -\mu_B (B + B_{eff}) s_i
    \\
    M 
    =
    n \mu_B \langle s_i \rangle
    \hp 
    \lambda 
    =
    \frac{J N_{nn}}{n}
    \hp 
    B_{eff}
    =
    \lambda \cdot M 
}

The effective magnetization can be written as 
\equations{
    M(B, T)
    =
    n \mu_B 
    \tanh(\frac{\mu_B (B + B_{eff})}{k_B T})
    \\
    M(B=0, T)
    =
    n \mu_B 
    \tanh(\frac{\mu_B \lambda M}{k_B T})
}

This is a transcendental equation that can't solve analytically, but can 
be solved graphically (because $M$ is on both sides). 

We can find the inflection point of this graph, and that gives us the Curie 
temperature where materials stop being magnetic 
\equations{
    l'(M)
    =
    \frac{dM}{dM}
    =
    1
    =
    r'(M)
    =
    \frac{n \mu_B \lambda}{k_B}
    \\
    T_C 
    =
    \frac{n \mu_B^2 \lambda}{k_B}
}

\chapter{Landau Theory}
Using the Ising model, we looked at a magnetic solid and saw that the 
magnetization is 
\equations{
    M 
    \sim 
    (T_C - T)^{\beta}
    \hp 
    \beta 
    =
    \frac{1}{2}
}
that $\beta$ is true for mean field theory, and more than 3 dimensions. For 3D models, 
you can run simulations and get something different. 

Landau Theory is universal, which means it works for all materials 
regardless of macroscopic properties. 

\section{Landau Theory of Phase Transitions}
This is the general theory of 2nd order phase transitions. 
it's phenomenological and uses mean field theory. 

\section{Order Parameter}
We can consider a condensated phase (liquid) as ordered because all the particles 
are in similar states, so $\xi > 0$, while above the temperature 
$T_C$ would be disorded, 
so order parameter $\xi=0$. $\xi$ is the order parameter. 

Imagine gas as disordered and liquid as ordered with $\xi = \rho - \rho_C$, so it is 
described by density.

For a magnet, paramagnets are disordered and ferromagnets are ordered with 
$\xi = M$, magnetization. 

We can consider a regular gas as disordered and a superfluid as ordered with 
$\xi = |\psi|^2$, the superfluid fraction. 

\section{Landau Free Energy}
$F_L(\xi, T)$ is minimized in equilibrium. Because of this, we can say that 
$\xi$ is small near $T_C$, so we can expand $F_L(\xi, T)$ around $\xi=0$
\equations{
    F_L(\xi, T)
    =
    g_0(T)
    +
    g_1(T)\xi
    +
    \frac{1}{2}
    g_2(T) \xi^2
    +
    \frac{1}{3}
    g_3(T) \xi^3
}

And in equilibrium, we know the derivative is 0, so we get 
\equations{
    \left(
    \frac{\del F}{\del \xi}
    \right)_{T}
    =
    0
    =
    g_1(T)
    +
    g_2(T) \xi
    +
    g_3(T) \xi^2 
    +
    \ldots
}

\subsection{Symmetry}
If $B=0$, expect $F_L$ to be an even function because $\uparrow \uparrow \uparrow$
and $\downarrow \downarrow \downarrow$ should be the same. This means that $F_L$ 
can be written as 
\equations{
    F_L
    =
    g_0(T)
    +
    \frac{1}{2}
    g_2(T) \xi^2
    +
    \frac{1}{4}
    g_4(T) \xi^4
}

We skip the odd parts of the function. At equilibrium, we get 
\equations{
    \frac{\del F_L}{\del \xi}
    =
    0
    =
    g_2(T) \xi 
    +
    g_4(T) \xi^3
    \hp
    \xi
    =
    0
    \hp 
    \xi 
    =
    \pm
    \sqrt{\frac{-g_2(T)}{g_4(T)}}
    \\
    \frac{\del^2 F_L}{\del \xi^2}
    =
    g_2(T) + 3g_4(T) \xi^2 
}
$\xi_0$ is minimum if $g_2(T) > 0$, $\xi_{\pm}$ is minimum if $g_2(T) < 0$

if $g_2 \geq 0$, then expect $\xi_{eq} = 0$ for $T \geq T_C \Rightarrow g_2 \geq 0$
for $T > T_C$.

if $g_2 \leq 0$, then expect $\xi_{eq} \neq 0$ for $T < T_C \rightarrow g_2 < 0$ for 
$T < T_C$

You can do a taylor expansion near $T_C$ to get 

\equations{
    F_L
    =
    g_0(T)
    + 
    \frac{1}{2} \alpha (T - T_C) \xi^2
    +
    \frac{1}{4} g_4(T) \xi^4
}

For a symmetric 0 B field, we can get a free energy of 
\equations{
    F_L(T)
    =
    \begin{cases}
	g_0(T) : T \leq T_C 
	\\
	g_0(T) - \frac{\alpha^2 (T_C - T)^2}{4g_4} : T \leq T_C 
    \end{cases}
}

\section{Landau's Postulate}
All 2nd order phase transitions have such a form for $F_L(\xi, T)$ close 
to $T_C$.

This is the same for $\beta, \gamma$ exponents.

\section{$B \neq 0$} 
\equations{
    F_L(\xi, T)
    =
    g_0(T)
    + 
    \frac{1}{2} \alpha 
    (T - T_C) \xi^2 
    +
    \frac{1}{4} g_4 \xi^4 
    -
    H \cdot \xi
}
H is the generalized force (H=B for Ising model)

\subsection{Equilibrium}
\equations{
    \left(
    \frac{\del F_L}{\del \xi}
    \right)_{T}
    =
    0
    =
    \alpha(T- T_C) \xi 
    +
    g_4 \xi^3 - H 
}

If we take this to lowest order, we get 
\equations{
    \xi 
    =
    \frac{H}{\alpha(T - T_C)}
    \\
    \chi
    =
    \left(
    \frac{\del \xi}{\del H}
    \right)_{H=0}
    \sim 
    (T - T_C)^{\gamma} 
    \hp 
    \gamma
    =
    1
    : 
    T > T_C 
}

\section{Summary}
$H$ is phenomenological (but is interpretable for magnets)

quantitatively right, but still has mean field exponents. 

You need actual field theory to get better agreement with experiments. 

\section{Extension to 1st order Phase Transitions}
\equations{
    F_L
    =
    g_0
    +
    \frac{1}{2} \alpha (T - T_C) \xi^2 
    +
    \frac{1}{3} g_3 \xi^3 
    +
    \frac{1}{4} g_4 \xi^4
    \\
    \left(
    \frac{\del F_L}{\del \xi}
    \right)_{T}
    =
    0
    =
    \alpha (T - T_C)\xi 
    +
    g_3 \xi^2 
    +
    g_3 \xi^4 
    =
    \xi
    \left(
	\alpha(T - T_C)
	+
	g_3 \xi 
	+
	g_4 \xi^2
    \right)
    \\
    \xi_{\pm}
    =
    \frac{- g_3 \pm \sqrt{g_3^2 - 4 g_4 \alpha(T - T_C)}}{2 g_4}
}

\section{MIDTERM2}
\equations{
    (1 + e^{-2\beta \epsilon})
    (1 + e^{-\beta \epsilon} + e^{-2\beta \epsilon})
    =
    (1 + e^{-\beta \epsilon} + e^{-2\beta \epsilon})
    +
    (e^{-2\beta \epsilon} + e^{-3\beta \epsilon} + e^{-4\beta \epsilon})
    \\
    =
    1
    +
    e^{-\beta \epsilon}
    +
    2e^{-2\beta \epsilon}
    +
    e^{-3\beta \epsilon}
    +
    e^{-4\beta \epsilon}
}

I'm so sad this is tragic.

\chapter{Kinetic Theory}
How a system approaches equilibrium. Mechanisms of heat transfer include 
\begin{itemize}
    \item
    Radiation (light)
    \item
    Conduction (material transfer)
    \item
    Convection (bulk flow)
\end{itemize}

\section{Kinetics}
Consider a statistical ensemble that can explore all accessible microstates. The
system is isolated coupled to a reservoir (canonical), 
which means that $U, T = \const$.

Consider a paramagnet. 

\section{Ergodic Hypothesis}
Given enough time, a system will explore all possible microstates. 

\section{Master Equation}
$P_i(T)$ is the probability that a system if found in state $i$ at time $t$.

$P_i(T)$ will increase if the system transitions into state $i$. 

$P_i(T)$ decreases if $i$ goes into other states. 

We can consider a transition probability per unit time $W_{i \to j}$

\equations{
    \frac{dP_2}{dt}
    =
    P_1 \cdot W_{1 \to 2} 
    -
    P_2 \cdot W_{2 \to 1} 
    \\
    \frac{dP_1}{dt}
    =
    P_2 \cdot W_{2 \to 1} 
    -
    P_1 \cdot W_{1 \to 2} 
}

If we consider an arbitrary number of states, we get 
\equations{
    \frac{dP_i}{dt}
    =
    \sum_i W_{j \to i} \cdot P_j
    -
    \sum_j W_{i \to j} \cdot P_i 
}

Term 1 is all the transitions into $i$, and Term 2 is all the transitions out 
of state $i$. This is one of the most important equations in non-equilibrium 
statistical mechanics. 

\subsection{Principle of Detailed Balance} 
The system is steady state if $\frac{dP_i}{dt} = 0$ for all $i$.
\equations{
    \sum_j W_{j \to i} \cdot P_j
    =
    \sum_j W_{i \to j} \cdot P_i
    \Rightarrow 
    \textrm{into $i$}
    =
    \textrm{out of $i$}
}

However, for detailed balanced, every individual transition must be balanced 
\equations{
    W_{j \to i} \cdot P_j 
    =
    W_{i \to j} \cdot P_i 
    \forall i,j
}

The number of transitions from $j \to i$ per time equals the number of 
transitions $i \to j$ per time.

\subsection{Example}
Consider a system with 3 states that cycle into each other in only 1 direction.
\equations{
    W_{1 \to 2}
    \to
    W_{2 \to 3}
    \to
    W_{3 \to 1}
    \to 
    \ldots
    \\
    \frac{dP_1}{dt}
    =
    -W_{1 \to 2} \cdot P_1 
    +
    W_{3 \to 1} \cdot P_3
}

This is not equilibrium, so the system cannot be in detailed balance even if it's in 
a steady state. 

However, every every transition was bi-directional, we could have detailed balance. 
\equations{
    \frac{dP_1}{dt}
    =
    \left(
	-W_{1 \to 2} P_1
	+
	W_{2 \to 1} P_2
    \right)
    +
    \left(
	-W_{1 \to 3} P_1
	+
	W_{3 \to 1} P_3
    \right)
}

If everything equals each other and it's 0, then we have detailed balance.

In equilibrium, steady state is necessary, but not sufficient. We need detailed 
balance. This is important for simulations. 

For a closed system, we make a fundamental assumption that $P_i = P_j$

\section{System Coupled to a Reservoir} 
\equations{
    P_i 
    =
    \frac{e^{-\epsilon_i / k_B T}}{Z}
}

You can plug this into the detailed balance equation to get 
\equations{
    \frac{W_{j \to i}}{W_{i \to j}}
    =
    \frac{P_i}{P_j} 
    =
    e^{-(\epsilon_i - \epsilon_j) / k_B T}
}

The probability of going from high energy to low energy is higher than going 
from low energy to high.

\equations{
    \epsilon_j
    >
    \epsilon_i 
    \Rightarrow 
    W_{j \to i}
    >
    W_{i \to j}
}

\section{Einstein Relations}
Consider a gas of photons at temperature $T$. Interactions that can happen are 
spontaneous emission, absorption, and stimulated emission. 

Absorption has a proportionality constant $B \mu(\omega)$ where 
\equations{
    \mu(\omega)
    =
    \frac{\hbar \omega^3}{\pi^2 c^3} 
    \frac{1}{e^{\beta \hbar \omega} - 1}
}

Emission has constant $A$, and stimulated emission has $B' \mu(\omega)$ Each one 
moves the state by $\hbar \omega$

Write down the master equation to get 
\equations{
    \frac{dP_1}{dt}
    =
    0
    =
    W_{2 \to 1} \cdot P_2 
    -
    W_{1 \to 2} \cdot P_1
    =
    (A + B' \mu(\omega)) P_2
    -
    B \mu(\omega) P_1
}

The first part is both types of emission, and the 2nd part is absorption.
Use the known equations of the canonical ensemble to get
\equations{
    \frac{W_{2 \to 1}}{W_{1 \to 2}}
    =
    \frac{P_2}{P_1}
    =
    \frac{A + B' \mu(\omega)}{B \mu(\omega)}
    =
    e^{\beta \hbar \omega}
}

That gives us 
\equations{
    A(e^{\beta \hbar \omega} - 1)
    =
    \frac{\hbar \omega^3}{\pi^2 c^3} 
    \left(
    B e^{\beta \hbar \omega} - B'
    \right)
}

This is true for all $T$. It has only a single solution 
\equations{
    A 
    =
    B 
    \frac{\hbar \omega^3}{\pi^2 c^3} 
    \hp 
    B
    =
    B'
}

You only need to know 1 of the 3 transition probabilities to calculate the 
other 2. There's also no way to get detailed balance for all $T$ 
of a photon gas unless you have all 3 properties 

\section{Chemical Kinetics}
Consider chemical reaction. 
\equations{
    A + B 
    \iff 
    AB
}

If we want equilibrium, then the rate going into AB should equal the rate going 
out of AB. The rate into AB can be written as 
\equations{
    \frac{d[AB]}{dt}
    =
    k_{+} [A][B]
    -
    k_{-} [AB]
}

$k_{-}$ has units 1/s and is a first order rate constant. $k_{+}$ is a second order 
rate constant and has units $1 / (t*concentration)$. At equilibrium, we use the 
master equation to get 
\equations{
    \frac{d[AB]}{[A][B]}
    =
    \frac{k_{+}}{k_{-}}
}

This gives us the law of mass action
\equations{
    \frac{[AB] n_0}{[A][B]}
    =
    e^{\Delta G_{0} / k_B T}
    =
    K(T)
    =
    \frac{k_{+}}{k_{-}}
}

\chapter{Thermal Radiation}
We learned about black body radiation and the raleigh equation and the ultra-violet 
catastrophe and all that jazz. The energy density is proportional to $T^4$.

If we imagine a star. It's basically just a big ol photon gas with some energy 
being released. 
\equations{
    J_{u}
    =
    \sigma_b T^4
}

\section{Black Body}
An object that just absorbs all EM radiation and emits radiation according
to its temperature $T$.

equilibrium means that there is no net energy flow (the energy absorbed perfectly 
equals the energy emitted).

\equations{
    J_{cavity}
    =
    J_{blackbody}
    =
    \sigma_b T^4
}

I have not paid attention to anything whatsoever 

\section{Kirchhoff's Law}
\equations{
    a(\omega)
    =
    e(\omega)
    =
    P_{refl}
    =
    r(\omega)
    \cdot 
    P_{incident}
    \hp
    e(\omega)
    +
    r(\omega)
    =
    1
}

A good absorber is a good emitter. A good reflector is a 
poor absorber and poor emitter. 

\section{The Greenhouse Effect}
The sun can be pretty decently approximated as a black body.
There's an energy flux density and a $P_{inc}$ idk what $P$ is.

We can figure out the average temperature of the earth by looking at the amount 
of energy the sun hits it with and the amount of energy it emits. We assumed the earth 
is a black body and that is not true.

\section{Boltzmann Transport Equation}
We did some derivation shenanigans and got 
\equations{
    \frac{\del f}{\del t}
    +
    \vec v 
    \cdot 
    \vec \nabla_{T} f
    +
    \frac{\vec F}{m} 
    \cdot 
    \vec \nabla_{v} f 
    =
    \left(
	\frac{\del f}{\del t}
    \right)
    \hp
    \vec \nabla_{v}
    =
    \begin{pmatrix}
	\frac{\del}{\del v_x} \\ \frac{\del}{\del v_y} \\ \frac{\del}{\del v_z}
    \end{pmatrix}
}

At equilibrium, the right hand side goes to 0. If we assume the derivative is linear, 
we can get something nice 
\equations{
    \frac{\del f}{\del t}
    =
    -
    \frac{f - f_0}{\tau}
    \Rightarrow 
}

This relaxation approach allows us to solve the equation iteratively. Our dot products also 
simplify to get a steady state equation of 
\equations{
    v_z \frac{\del f}{\del z}
    =
    -
    \frac{f - f_0}{\tau}
}

If we consider the Maxwell Distribution for velocities, we can actually figure out $f_0$ 
\equations{
    f_0(\vec r, \vec v)
    =
    n(\vec r)
    \left(
	\frac{m}{2 \pi k_B T}
    \right)^{3/2}
    e^{- \frac{mv^2}{2 k_B T}}
    \Rightarrow 
    f
    =
    f_0 
    -
    v_z \tau_z 
    \frac{\del f}{\del z}
}

This is solved iteratively by plugging $f$ into $\frac{\del f}{\del z}$

\equations{
    f_1 
    \approx 
    f_0 
    -
    v_z \tau_c 
    \frac{\del f_0}{\del z}
    \hp
    f_2 
    \approx 
    f_0 
    -
    v_z \tau_c 
    \frac{\del f_0}{\del z}
    +
    (v_z \tau_c)^2
    \frac{\del^2 f_0}{\del z^2}
    \hp 
    \ldots
}

First order is generally enough. 

We can find the flux of the particles 
\equations{
    \vec J 
    =
    n \vec v 
    =
    \int d^3 v \, 
    f(\vec r, \vec v, t)
    \cdot 
    \vec v 
}

We reduce it do just 1 dimension to get 
\equations{
    J_z 
    =
    \int d^3 v \,
    f_1 v_z 
    =
    \int d^3 v \,
    f_0 v_z 
    -
    \int d^3 v \,
    v_z^2 \tau_c
    \frac{\del f_0}{\del z}
}

Because $f_0$ is even in $v_z$, the integral every goes to 0, so the average flux is 0, 
which makes sense in equilibrium.
\equations{
    \int d^3 v \,
    f_0 v_z 
    =
    n 
    \langle v_z \rangle 
    =
    0
}

\subsection{Fick's Law}
This means that our flux is only dependent on the 2nd part,so we get 
\equations{
    J_z 
    =
    - \tau_c \langle v_z^2 \rangle \frac{\del n}{\del z}
    =
    -D \vec \nabla n
    \hp 
    \langle v_z^2 \rangle 
    \approx 
    \frac{1}{3}
    \langle v^2 \rangle 
    \hp 
    D 
    =
    \frac{1}{3}
    \bar v l
}

This is attained by integrating over the maxwell velocity distribution, which I did not 
write down.

\section{Propagation}
We know the heat conduction equation 
\equations{
    \frac{\del T}{\del t}
    =
    D_T \nabla^2 T
    \hp 
    D_T
    =
    \frac{\kappa}{\hat c_v}
    =
    \frac{1}{3} \bar v l
}

The particle diffusion equation is very similar 
\equations{
    \frac{\del n}{\del t}
    =
    D \nabla^2 v 
    \hp
    D
    =
    \frac{1}{3} \bar v l
}

This is very similar to a 3d wave equation where 
\equations{
    \frac{\del^2 u}{\del t^2}
    =
    c \nabla^2 u 
}
where $c$ is the speed of wave propagation

A pulse in 1 dimension has the equation 
\equations{
    \frac{\del \theta}{\del t}
    =
    D 
    \frac{\del^2 \theta}{\del z^2}
}

We can put this into fourier space to get a more readable answer. 
\equations{
    \tilde \theta(k, t)
    =
    \int^{\infty}_{-\infty} dz \, 
    e^{-ikz} \theta(z, t)
    \hp
    \theta(k, t)
    =
    \frac{1}{2 \pi}
    \int^{\infty}_{-\infty} dk \, 
    e^{ikz} \tilde \theta(z, t)
    \\
    \frac{\del \theta}{\del t}
    =
    \frac{1}{2 \pi}
    \int^{\infty}_{-\infty} dk \, 
    e^{ikz} \frac{\del}{\del t} \tilde \theta(z, t)
    \hp
    \frac{\del^2 \theta(z, t)}{\del z^2}
    =
    \frac{1}{2 \pi}
    \int^{\infty}_{-\infty} dk \, 
    (-k^2)
    e^{ikz} \tilde \theta(z, t)
}

Plug all the junk into all the other junk to get 
\equations{
    \frac{\del \theta}{\del t}
    -
    D \frac{\del^2 \theta}{\del z^2}
    =
    0
    \Rightarrow 
    \frac{1}{2 \pi}
    \int^{\infty}_{-\infty} dk \, 
    e^{ikz} 
    \left(
    \frac{\del \tilde \theta}{\del t}
    +
    k^2 D \tilde \theta
    \right)
    =
    0
    \Rightarrow 
    \tilde \theta 
    =
    A(k) 
    \cdot 
    e^{-k^2 D t}
    \\
    A(k) 
    =
    \tilde \theta(k, t=0)
    =
    \int dz \, 
    e^{-ikz} \delta(z - z_0)
    =
    e^{-ikz_0}
    \\
    \tilde \theta 
    =
    e^{-ikz_0}
    e^{-k^2 D t}
}

Take the inverse fourier transform to get the thing back in z-space. You get some 
gaussian integral which returns an exponent. 
\equations{
    \theta 
    =
    - \alpha 
    \left[
    k^2 
    -
    \frac{i \beta}{\alpha} k 
    +
    \left(
    i \frac{\beta}{2 \alpha}
    \right)^2 
    \right]
    +
    \alpha
    \left(
    i \frac{\beta}{2 \alpha}
    \right)^2 
    ============
    \\
    \theta(z, t)
    =
    \frac{1}{\sqrt{4 \pi D t}}
    e^{- \frac{(z - z_0)^2}{4 D t}}
}
It is a gaussian in z-space that grows in time, which intuitively makes sense. 
You can normalize it and get a mean of 0 (symmetric). The variance can also be calculated 
\equations{
    \sigma_z^2 
    =
    \langle (z - z_0)^2 \rangle 
    =====
    2Dt
}

The variance increases linearly in time. This means the standard deviation grows with the 
square root of time, so diffusion is not actually that fast. 

\subsection{Microscopic Diffusion Theory}
Particles form a gaussian by performing a random walk and slowly going outwards.
Gaussians are binomial are coin flips are probability. My ass is not writing this down 
I know how probability works. 

The next lecture is explicitly doing this derivation so I guess I'm writing it. 

Consider a particle with a 1/2 chance of going right or left. We consider the $N_R$ as a 
step to the right, so our probability of going to th right is
\equations{
    p(N_R, N)
    =
    \frac{N!}{N_R! (N - N_R)!} 
    \left(
    \frac{1}{2}
    \right)^{N_R}
    \left(
    \frac{1}{2}
    \right)^{N-N_R}
    =
    \frac{N!}{N_R! (N - N_R)!} 
    \left(
    \frac{1}{2}
    \right)^{N}
}

With this we can figure out the mean and variance 
\equations{
    \langle N_R \rangle 
    =
    \sum_{N_R = 0}^{N}
    N_R p(N_R, N)
    =
    \frac{N}{2}
    \\
    \langle z \rangle 
    =
    (z \langle N_R \rangle - N) l_z 
    =
    0
    \\
    \langle N_R^2 \rangle 
    =
    \sum_{N_R = 0}^{N} 
    N_R^2 p(N_R, N)
    =
    \frac{N(N+1)}{4} 
    \\
    \langle z^2 \rangle 
    =
    \langle (z N_R - N)^2 l_z^2 \rangle 
    =
    4 l_z^2 \langle N_R^2 \rangle 
    -
    4 \langle N_R \rangle N l_z^2 
    +
    N^2 l_z^2 
    \\
    =
    l_z^2 N(N+1) 
    -
    2N^2 l_z^2 
    +
    N^2 l_z^2 
    =
    N l_z^2
    \\
    \sigma_z 
    =
    Nl_z^2 
    \hp 
    N 
    =
    \frac{t}{\tau_c}
    \Rightarrow 
    \sigma_z^2 
    =
    \frac{t}{\tau_c} l_z^2 
    \sim t
    \\
    \sigma_z 
    \sim 
    \sqrt{t}
}

Kinematics follows a very similar derivation 
\equations{
    z = vt 
    \Rightarrow 
    z_{rms}
    =
    \sqrt{2 D t}
    \sim 
    \sqrt{t}
}

The mean position is unchanged.

Diffusion is not a good method for transporting molecules over long distances. 
\equations{
    t
    =
    \frac{z^2}{2D}
    =
    \frac{10^4 cm^2}{2 * 10^6 \frac{cm^2}{s}}
    =
    5 \cdot 10^9 s 
    =
    150 \textrm{ years}
}

She wrote a graph about something or other and it looks like a straight line 
going up until $L$ and then a straight horizontal line. idk man it's graphing 
$\theta$ over $z$ but I don't know what $\theta$ is.

\section{Steady State Solution}
\equations{
    \theta_{ss}(z) 
    =
    \theta_0 \frac{z}{L}
    \Rightarrow 
    \frac{\del^2 \theta}{\del z^2} 
    =
    0
    =
    \frac{\del \theta}{\del t}
    \frac{1}{D}
    \Rightarrow 
    \theta(z, t)
    =
    \theta_{ss}(z)
    +
    \theta_p(z, t)
}

Use separation of variables to solve for the particular solution $\theta_p$
\equations{
    \theta_p(z, t)
    =
    Z(z) T(t)
    \Rightarrow 
    Z(z)
    \frac{\del T(t)}{\del t}
    =
    D T(t)
    \frac{\del^2 Z(z)}{\del z^2}
    \Rightarrow 
    \\
    \frac{1}{D T(t)}
    \frac{\del T(t)}{\del t}
    =
    \frac{1}{Z(z)}
    \frac{\del^2 Z(z)}{\del z^2}
}

With this you can find solutions 
\equations{
    \frac{\del T(t)}{\del t}
    =
    -D k^2 T(t)
    \hp
    \frac{\del^2 Z(z)}{\del z^2}
    =
    -k Z(z)
    \\
    \theta_p(z, t)
    =
    \begin{cases}
	e^{-k^2 D t} \cdot \cos(kz)
	\\
	\textrm{or}
	\\
	e^{-k^2 D t} \cdot \sin(kz)
    \end{cases}
}

The general solution can be written in the form 
\equations{
    \theta_p(z, t)
    =
    \frac{a_0}{z}
    +
    \sum_{n=1}^{\infty}
    a_n(t) \cos(k_n z)
    +
    \sum_{n=1}^{\infty}
    b_n(t) \sin(k_n z)
}

Sum over all $k$ that satisfy boundary conditions.
\equations{
    a_n(t)
    =
    a_n(t=0) e^{-k_n D \cdot t}
    \hp
    b_n(t)
    =
    b_n(t=0) e^{-k_n D \cdot t}
    \\
    \theta(z=0, t)
    \overset{!}{=} 
    0 
    \hp 
    \theta(z=L, t)
    \overset{!}{=} 
    \theta_0
    \\
    \theta_{ss}
    =
    \theta_{0}
    \frac{z}{L}
}
$\theta_{ss} + \theta_{p}$ fulfills the boundary conditions.

If $\theta(z=k, t) = 0$, then we can fill in a bunch of variables. 
\equations{
    \theta_p(z, t)
    =
    \sum_{n=1}^{\infty} 
    b_n(0)
    e^{- (\frac{n \pi}{L})^2 D t} \sin(\frac{n \pi}{L} z)
}

The cos disappears because you need $0$ at $0$ and $k = n \pi / L$ because you 
seen the sin term to be 0 at $L$. 
\subsection{Initial Conditions}
if $\theta(z, t=0) = 0$, then we can solve for more things. The $e$ term is 0 at $t=0$
so that goes away. 

You can do an inverse fourier transform to solve for $b_n(0)$ 
\equations{
    b_n(0)
    =
    \frac{2}{L}
    \int^{L}_{0} dz \, 
    \theta_p(z, t=0) \sin(\frac{n \pi}{L} z)
    =
    \frac{2}{L}
    \int^{L}_{0} dz \, 
    -\theta_0 \frac{z}{L} \sin(\frac{n \pi}{L} z)
}
substitute $u = n \pi z / L$
\equations{
    b_n(0)
    =
    - \frac{2 \theta_0}{(n \pi)^2} 
    \int^{n \pi}_{0} du \, 
    u \sin(u)
    =
    2 \theta_0 \frac{(-1)^{n}}{2 \pi}
}

Plug the stuff into the thing to get
\equations{
    \theta(z, t)
    =
    \theta \frac{z}{L}
    +
    2 \theta_0 
    \sum_{n=1}^{\infty}
    \frac{(-1)^{n}}{n \pi} 
    e^{- (\frac{n \pi}{L})^2 D t} 
    \cdot 
    \sin(\frac{n \pi}{L} z)
}

After a long time, we consider only the most slowly decaying term (smallest n). 
\equations{
    \theta(z, t \to \infty)
    \approx \theta_0 \frac{z}{L}
    -
    \frac{2 \theta_0}{\pi}
    e^{- (\frac{\pi}{L})^2 D t}
    \cdot 
    \sin(\frac{\pi z}{L})
}

Everything is a damped harmonic oscillator

\section{FINAL INFORMATION}
UP TO LECTURE 25 THANK GOD

There will be office hours both this week and next week (yay!). 

Wednesday December 17th at 8am at Noyes 217.

It's a 2 hour exam. Study the entire course.

\begin{itemize}
    \item
	heat engines 
    \item 
	carnot cycle 
    \item 
	entropy and its relation to heat capacity
    \item 
	really understand heat capacity/specific heat
    \item
	microstates and its relation to entropy 
    \item
	How to calculate average energies 
    \item
	limits of $t \to 0$ and $t \to \infty$ to leading order terms.
    \item
	Quantum statistics (Fermi Energies and stuff)
    \item
	Know every ensemble 
\end{itemize}

Go over the lectures and review some things. We talked about heat capacity 
for insulators and conductors. 

You get the same sheet and there's also a formula sheet online.

\section{Extra Stuff lmao}
Ashcroft and Mermin

\chapter{FINAL PREP}
Know derivations for stuff, not formulas. Most formulas are on the pre-made equation sheet. 

Van der Walls derivation for phase transitions 

MEAN FIELD THEORY IS LIKE EVERYTHING.

















\end{document}
